<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="模型压缩总结2, ght">
    <meta name="description" content="总结近期的文章">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>模型压缩总结2 | ght</title>
    <link rel="icon" type="image/png" href="/ght.png">
    
    <style>
        body{
            background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/css/post.css">



    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">ght</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">ght</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/ghtll/ghtll.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/ghtll/ghtll.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">模型压缩总结2</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="container content">

    
    <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/">
                                <span class="chip bg-color">模型压缩</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" class="post-category">
                                文献阅读
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-07-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-07-30
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="1-CBQ"><a href="#1-CBQ" class="headerlink" title="1 CBQ"></a>1 CBQ</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>CBQ: Cross-Block Quantization for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年4月</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化、低秩近似</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目前的训练后量化面临的问题：</p>
<ul>
<li>处理单个块忽略了块之间的依赖</li>
</ul>
<p>CBQ提出：</p>
<ul>
<li>跨块重构的PTQ方法</li>
<li>使用同源重构方案，建立跨多块的依赖性，最小化累积误差</li>
<li>提出从粗道喜的预处理策略（CFP）：用于抑制权重中和激活中的异常值</li>
<li>提出自适用LoRARounding技术：用于精确的量化权重</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>量化和反量化处理表示：</p>
<p><img src="/images/2024714/1.png" alt=""></p>
<p>$\Delta w$表示权重的量化舍入矩阵。</p>
<h3 id="Cross-block-reconstruction"><a href="#Cross-block-reconstruction" class="headerlink" title="Cross-block reconstruction"></a>Cross-block reconstruction</h3><p>同时优化K个transfomers块：</p>
<p><img src="/images/2024714/2.png" alt=""></p>
<h5 id="Cross-block-dependency"><a href="#Cross-block-dependency" class="headerlink" title="Cross-block dependency"></a>Cross-block dependency</h5><p>逐块重建可以有效地节省计算内存，但它只考虑每个块内的局部信息，忽略了不同块之间的依赖性。文章引入了使用滑动窗口方法的跨块依赖（CBD）方案。该方案能够同时优化窗口内的多个块。此外，两个相邻的滑动窗具有<strong>重叠</strong>的块，确保窗口之间的块也相互连接。CBD 方案的优化表述为：</p>
<p><img src="/images/2024714/3.png" alt=""></p>
<p>尽管CBD技术有助于降低重建难度，但需要注意的是，它不能完全消除累积的误差，并且每个窗口内的优化仍然是局部的。为了解决这个限制并在确保稳定性的同时进一步增强重建过程，文章引入了一种额外的<strong>同源重建</strong>方案，如下图所示。在该方案中，将量化模型的前一个块的输出输入到全精度模型的当前块中。这会生成与量化模型的输出同源的附加输出。</p>
<p><img src="/images/2024714/4.png" alt=""></p>
<p>优化目标变为：</p>
<p><img src="/images/2024714/5.png" alt=""></p>
<p>对于距离度量，结合 L2 和 KullbackLeibler 散度 (KLD) 损失来测量重建误差。 它倾向于抑制特征空间中的异常值并增强优化过程的鲁棒性。通过合并这两个术语，捕获了空间距离和分布差异，从而实现了更全面、更稳健的优化过程。那么距离度量可以表示为：</p>
<p><img src="/images/2024714/6.png" alt=""></p>
<h3 id="Coarse-to-fine-pre-processing"><a href="#Coarse-to-fine-pre-processing" class="headerlink" title="Coarse-to-fine pre-processing"></a>Coarse-to-fine pre-processing</h3><p>先粗粒度，再细粒度：对于权重异常值直接截断，<strong>对于激活异常值，每个channel进行缩放。</strong></p>
<p><img src="/images/2024714/7.png" alt=""></p>
<h3 id="LoRA-Rounding-for-weight-quantization"><a href="#LoRA-Rounding-for-weight-quantization" class="headerlink" title="LoRA-Rounding for weight quantization"></a>LoRA-Rounding for weight quantization</h3><p>当将模型压缩为低位宽时，权重量化引起的性能下降是不可忽略的。权重的量化误差来自于舍入误差和裁剪误差，但之前针对LLM的PTQ方法只关注后者的优化而没有考虑考虑舍入误差。 文章通过可学习矩阵 V 和修正 sigmoid 函数获得权重舍入矩阵 ΔW：</p>
<p><img src="/images/2024714/8.png" alt=""></p>
<p>考虑到对于大模型V太大，这里文章提出了lora的方法即：</p>
<p><img src="/images/2024714/9.png" alt=""></p>
<p>并带有正则化损失：</p>
<p><img src="/images/2024714/10.png" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>所有实验在单卡GPU实现：</p>
<p><img src="/images/2024714/11.png" alt=""></p>
<h1 id="2-（图像编码不要用2）"><a href="#2-（图像编码不要用2）" class="headerlink" title="2 （图像编码不要用2）"></a>2 （图像编码不要用2）</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Efficient Low-Dimensional Compression of Overparameterized Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年3月</td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/soominkwon/comp-deep-nets">https://github.com/soominkwon/comp-deep-nets</a></td>
</tr>
<tr>
<td>压缩技术</td>
<td>微调动力学，深度矩阵分解</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>作者观察到，对于许多深度模型，权重矩阵的<strong>更新</strong>发生在低维不变子空间内。对于深度线性模型，文章证明了它们的主要组件在一个小子空间内增量地拟合，并利用这些见解提出了一种深度线性网络的压缩算法，其中涉及减少中间层的宽度。文章观察到压缩网络比原始网络<strong>收敛得更快</strong>，<strong>始终产生更小的恢复错误</strong>。</p>
<h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Implicit Regularization in Deep Matrix Factorization</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2019</td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/roosephu/deep_matrix_factorization">https://github.com/roosephu/deep_matrix_factorization</a></td>
</tr>
<tr>
<td>压缩技术</td>
<td>微调动力学，深度矩阵分解</td>
</tr>
</tbody>
</table>
</div>
<h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>RPTQ: Reorder-based Post-training Quantization for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2023年5</td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/hahnyuan/RPTQ4LLM">https://github.com/hahnyuan/RPTQ4LLM</a></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h2><p>量化大语言模型的挑战来自跨channel的不同范围，文章引入RPTQ量化方法，基于重新排序的方法，重新排列各个channel并按照簇进行量化。</p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>概括图：</p>
<p><img src="/images/2024714/41.png" alt=""></p>
<h3 id="Channel的重新排序和聚类"><a href="#Channel的重新排序和聚类" class="headerlink" title="Channel的重新排序和聚类"></a>Channel的重新排序和聚类</h3><p>采用k-mean聚类然后讲统一集群中的channel聚类在一起，每个簇使用相同的缩放因子S和零点Z。</p>
<h3 id="Avoid-Explicit-Reordering-and-Misalignment"><a href="#Avoid-Explicit-Reordering-and-Misalignment" class="headerlink" title="Avoid Explicit Reordering and Misalignment"></a>Avoid Explicit Reordering and Misalignment</h3><p>重排序操作融合到层规范操作中：</p>
<p><img src="/images/2024714/42.png" alt=""></p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/43.png" alt=""></p>
<h1 id="5"><a href="#5" class="headerlink" title="5"></a>5</h1><h2 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>OneBit: Towards Extremely Low-bit Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年4</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h2><p>通过矩阵分解将大语言模型量化到1位。</p>
<h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><h3 id="1-bit-Linear-Layer-Architecture"><a href="#1-bit-Linear-Layer-Architecture" class="headerlink" title="1-bit Linear Layer Architecture"></a>1-bit Linear Layer Architecture</h3><p>文章(BitNet: Scaling 766 1-bit transformers for large language models)从零开始训练1位模型，在W1A16模型中，线性层被设计为：</p>
<p><img src="/images/2024714/51.png" alt=""></p>
<p>受到启发，使用$Sign$函数对权重进行量化，但是$\mathbf{W}_{\pm1}$缺少浮点数，减少了精度，因此文章引入两个FP16的两个向量：</p>
<p><img src="/images/2024714/52.png" alt=""></p>
<h3 id="Sign-Value-Independent-Decomposition"><a href="#Sign-Value-Independent-Decomposition" class="headerlink" title="Sign-Value-Independent Decomposition"></a>Sign-Value-Independent Decomposition</h3><p>对$\mathbf{W}$进行秩为1的矩阵分解：</p>
<p><img src="/images/2024714/53.png" alt=""></p>
<p>由于$\mathbf{W}$被提取了符号，因此对$\mathbf{W}$的绝对值进行分解可以采用SVD分解和非负矩阵分解NMF。SVID将线性层重新表述为（这样做比上式占用内存更小）：</p>
<p><img src="/images/2024714/54.png" alt=""></p>
<p>文章提出，SVID更近似与原始权重矩阵：</p>
<p><img src="/images/2024714/55.png" alt=""></p>
<h3 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h3><p>采用量化感知知识蒸馏进行训练：</p>
<p><img src="/images/2024714/56.png" alt=""></p>
<p>隐藏层的误差定义为：</p>
<p><img src="/images/2024714/57.png" alt=""></p>
<p>最终的损失函数为：</p>
<p><img src="/images/2024714/58.png" alt=""></p>
<h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/59.png" alt=""></p>
<h1 id="6"><a href="#6" class="headerlink" title="6"></a>6</h1><h2 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>LCQ: Low-Rank Codebook based Quantization for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年4</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-4"><a href="#摘要-4" class="headerlink" title="摘要"></a>摘要</h2><p>目前的权重量化大多采用rank-1的码本进行量化，当压缩比例较高时，精度低，文章提出了一种基于低秩码本的量化方法。</p>
<h2 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h2><p>有校准数据集，每个校准数据$\mathbf{X}_i\in \R^{L\times D} $的sequence length为$L$,feature dimension 为$D$：</p>
<p><img src="/images/2024714/61.png" alt=""></p>
<p>在一些工作比如AWQ、OminiQuant中，采用分组量化，具体而言。对于一个线性层有$N_W$个权重，将所有权重分成$N_V$个子集，每个子集有$N_W/N_V$个权重。对于每一个子集，将其分成$N_G$个组。指定每个子集的权重为：$\mathbf{W}\in \R^{N_{G}\times G}$,其中$G$表示组大小并且$N_G=\frac{N_W}{N_V\times G}$表示每个子集的权重组数量。需要为每组权重维护一个包含量化值的量化码本。</p>
<p>对于$b$比特量化，每组权重的所有量化值个数为:$N_Q=2^b$。定义所有组的量化码本为：$\mathbf{C}\in \R^{N_G\times N_Q}$,第$i$行是第$i$组权重对应的码本。量化函数以逐元素的方式对权重进行操作，将每个权重量化为码本中最接近的量化值。令$\mathbf{Z}\in[0,1, \dots,N_Q]^{N_G \times G}$表示权重的量化索引，即$Z_{i,j}$表示$W_{i,j}$的量化索引：</p>
<p><img src="/images/2024714/62.png" alt=""></p>
<p>量化函数被定义为：</p>
<p><img src="/images/2024714/63.png" alt=""></p>
<p><img src="/images/2024714/64.png" alt=""></p>
<h3 id="Low-Rank-Codebook"><a href="#Low-Rank-Codebook" class="headerlink" title="Low-Rank Codebook"></a>Low-Rank Codebook</h3><p>在AWQ、GPTQ中，量化值的间隔是相等的，因此不需要显式存储量化码本$\mathbf{C}$,具体来说$\mathbf{C}$可以被两个向量表示，一个缩放向量$\mathbf{S}_1\in \R^{1\times N_G} $（根据$\mathbf{W}$自适应计算）和一个固定点集向量$\mathbf{V}_1\in \R^{1\times N_Q} $（在-1到1之间均匀分布的向量）。</p>
<p><img src="/images/2024714/65.png" alt=""></p>
<p>然后通过计算$\mathbf{S}_1$和$\mathbf{V}_1$的外积得到码本。然后为了由于权重的不对称分布，引入一个量化偏移$\mathbf{B}\in \R^{N_G \times 1} $，每组权重有他们各自的偏移。最终的量化码本被表示为：</p>
<p><img src="/images/2024714/66.png" alt=""></p>
<p>因此存储为rank-1。为了提升表示能力，这里使用low-rank表示，引入两个矩阵：</p>
<p><img src="/images/2024714/67.png" alt=""></p>
<p><img src="/images/2024714/68.png" alt=""></p>
<p>量化码本被表示为：</p>
<p><img src="/images/2024714/69.png" alt=""></p>
<p><img src="/images/2024714/610.png" alt=""></p>
<h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>使用Transformer块的输出重建误差作为目标函数：</p>
<p><img src="/images/2024714/611.png" alt=""></p>
<p><img src="/images/2024714/612.png" alt=""></p>
<h3 id="Gradient-based-Optimization"><a href="#Gradient-based-Optimization" class="headerlink" title="Gradient-based Optimization"></a>Gradient-based Optimization</h3><h5 id="Gradient-Approximation-for-Quantization-Function"><a href="#Gradient-Approximation-for-Quantization-Function" class="headerlink" title="Gradient Approximation for Quantization Function"></a>Gradient Approximation for Quantization Function</h5><p>$argmin(·)$函数无法反向传播，根据Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation这篇文章，</p>
<p>将量化函数重写为多个片段相加的形式，每个片段对应两个相邻量化值之间的间隔：</p>
<p><img src="/images/2024714/613.png" alt=""></p>
<p>训练过程：这里训练过程中W不变。</p>
<p><img src="/images/2024714/616.png" alt=""></p>
<h5 id="Reparameterization-of-Quantization-Parameters"><a href="#Reparameterization-of-Quantization-Parameters" class="headerlink" title="Reparameterization of Quantization Parameters"></a>Reparameterization of Quantization Parameters</h5><p>在梯度下降中，直接学习S很难，主要原因是这些值在不同层之间会有很大的变化，直接学习S导致某些权重梯度更新过多。</p>
<p>根据OmniQuant文章，提出了重新参数化策略：</p>
<p><img src="/images/2024714/614.png" alt=""></p>
<h5 id="Initialization-of-Quantization-Parameters"><a href="#Initialization-of-Quantization-Parameters" class="headerlink" title="Initialization of Quantization Parameters"></a>Initialization of Quantization Parameters</h5><p>使用AWQ方法来初始化S、B、V。</p>
<h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>由于rank高，存储高了，这里对S、b、v进行了二次量化：</p>
<p><img src="/images/2024714/615.png" alt=""></p>
<h2 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h2><p>RTX A6000 GPU card of 48GB</p>
<p><img src="/images/2024714/617.png" alt=""></p>
<h1 id="7（一般）"><a href="#7（一般）" class="headerlink" title="7（一般）"></a>7（一般）</h1><h2 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>QQQ: Quality Quattuor-Bit Quantization for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年7</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-5"><a href="#摘要-5" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出四位权重和8位激活的量化方法</p>
<h2 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h2><p>采用smoothquant那种方法，只对异常通道进行缩放。采用GPTQ方法对权重进行量化</p>
<p>但是文章设计了W4A8加速内核：</p>
<p><img src="/images/2024714/71.png" alt=""></p>
<h1 id="8"><a href="#8" class="headerlink" title="8"></a>8</h1><h2 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>LeanQuant: Accurate Large Language Model Quantization with Loss-Error-Aware Grid</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024年7</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-6"><a href="#摘要-6" class="headerlink" title="摘要"></a>摘要</h2><p>以GPTQ为基础，个根据GPTQ的最优解来寻找量化网格。</p>
<h2 id="方法-5"><a href="#方法-5" class="headerlink" title="方法"></a>方法</h2><h3 id="Revisiting-the-Loss-Error"><a href="#Revisiting-the-Loss-Error" class="headerlink" title="Revisiting the Loss Error"></a>Revisiting the Loss Error</h3><p>文章发现以GPTQ方法的最优误差正比于海森矩阵的逆的倒数和量化的均方误差：</p>
<p><img src="/images/2024714/81.png" alt=""></p>
<h3 id="Loss-Error-Aware-Network-Quantization"><a href="#Loss-Error-Aware-Network-Quantization" class="headerlink" title="Loss-Error-Aware Network Quantization"></a>Loss-Error-Aware Network Quantization</h3><p>一般来说均匀量化网格是均匀间隔的，无法保持异常权重的量化精度，文章提出了非均匀的损失误差量化，根据以下目标来学习b位量化的网格线集合：</p>
<p><img src="/images/2024714/82.png" alt=""></p>
<p>文章的方法是采用k-means方法来学习网格线集合。</p>
<h2 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/83.png" alt=""></p>
<h1 id="9"><a href="#9" class="headerlink" title="9"></a>9</h1><h2 id="简介-8"><a href="#简介-8" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>DB-LLM: Accurate Dual-Binarization for Efficient LLMs</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
<td>量化</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-7"><a href="#摘要-7" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出新的二值化方法，在微观层面，同时考虑了2位宽的精度优势和二值化的效率优势，将二位量化权重拆分为两个独立的二进制，然后提出偏差感知蒸馏方法。</p>
<h2 id="方法-6"><a href="#方法-6" class="headerlink" title="方法"></a>方法</h2><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>对于b位均匀量化，量化和反量化可以表示为：</p>
<p><img src="/images/2024714/91.png" alt=""></p>
<p>权重二值化可以表示为：</p>
<p><img src="/images/2024714/92.png" alt=""></p>
<h3 id="Flexible-Dual-Binarization"><a href="#Flexible-Dual-Binarization" class="headerlink" title="Flexible Dual Binarization"></a>Flexible Dual Binarization</h3><p>研究人员发现语言模型的权重呈对称高斯分布，并且一小部分显著权重对量化性能至关重要。文章对多低位视角的优化进行了深入研究（图4）。二值化表示能力较差，其余两个水平向 0 收敛（图 3 ），忽略了显着权重并归因于最高损失值。2 位量化自然地克服了表示瓶颈（表达跨度超过图 3 中二值化的两倍）。最小损失点显着降低，但损失面依然陡峭，给优化带来了难度。</p>
<p><img src="/images/2024714/93.png" alt="图三：第一个输出投影矩阵 (LLaMA-1-7B) 的分布。彩色级别表示网格搜索的最佳解决方案，最大限度地减少二值化、2 位量化和 FDB 的代理量化误差（输出的 MSE 损失）。受权值分布正态性的影响，二值化由于不存在代表0的级别，将两个级别压缩得更接近0，从而阻碍了大量数值较高的重要权值的精确表示，其表达跨度还不到2位的一半量化"></p>
<p><img src="/images/2024714/94.png" alt="图四：基于二值化 (a)、2 位量化 (b) 和我们的 FDB (c) 的单个量化线性层的损失情况。对于(a)、(b)和(c)，我们扰动单层的训练参数并计算MSE损失，将量化层的输出与全精度模型的输出进行比较。 (d) 通过将三个表面并置在一个坐标框架内来突出它们之间的差异。"></p>
<p>为了结合二值化固有的效率和2位量化的灵活表示能力，提出了FDB，首先从高位llm中继承高性能获得初始化，然后微调增强表示能力。将2位llm拆分为两个独立的1位：</p>
<p><img src="/images/2024714/95.png" alt=""></p>
<p>为了实现等式4中量化级别之间的等距步长，将二值化调整为{0，1}。$\alpha_1$和$\alpha_2$初始值表示为：</p>
<p><img src="/images/2024714/96.png" alt=""></p>
<p>微调阶段，量化参数$\alpha_1$和$\alpha_2$会被优化，从而导致非等距量化水平。目标是比较图5中值和水平中心之间的大小（<strong>这里公式6为什么不和$\alpha_1 /2$比大小呢？解释如下</strong>）：</p>
<p><img src="/images/2024714/910.png" alt=""></p>
<p><img src="/images/2024714/97.png" alt=""></p>
<p><img src="/images/2024714/99.png" alt="图五"></p>
<p>$\mathbf{H}(.)$表示单位阶跃函数，复制定义为0，正值定义为1。FDP前向过程表示为：</p>
<p><img src="/images/2024714/98.png" alt=""></p>
<h3 id="Discussion-on-compression-and-acceleration"><a href="#Discussion-on-compression-and-acceleration" class="headerlink" title="Discussion on compression and acceleration."></a>Discussion on compression and acceleration.</h3><p>这样做会产生大量的稀疏性（约百分之60）潜在增加了计算速度。</p>
<h3 id="Discussion-on-flexibility"><a href="#Discussion-on-flexibility" class="headerlink" title="Discussion on flexibility."></a>Discussion on flexibility.</h3><p>从图四可以看到，FDB具有更平坦的优化表面。</p>
<h3 id="Deviation-aware-Distillation"><a href="#Deviation-aware-Distillation" class="headerlink" title="Deviation-aware Distillation"></a>Deviation-aware Distillation</h3><p>·文章观察到全精度llm的同一侧偏好服从下图的长尾分布，并表现出增加的失真。文章探究失真原因，对失效预测进行探讨，利用信息熵衡量相应的不确定性：</p>
<p><img src="/images/2024714/911.png" alt=""></p>
<p>如下图所示，师生模型的熵于任务损失（交叉熵是一致的），文章假设处理模糊样本时，量化模型有效性会下降，导致倾向于更保守的预测。文章提出偏差感知蒸馏：</p>
<p><img src="/images/2024714/912.png" alt=""></p>
<p>利用一对熵即师生熵作为难度指标来优先考虑不确定样本：</p>
<p><img src="/images/2024714/913.png" alt=""></p>
<p>总损失为：</p>
<p><img src="/images/2024714/914.png" alt=""></p>
<h2 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h2><p>8 NVIDIA A800 GPUs with 80 GB memory. Compressing a 7B model approximately requires 20 GPU hours.</p>
<p><img src="/images/2024714/915.png" alt=""></p>
<h1 id="10"><a href="#10" class="headerlink" title="10"></a>10</h1><h2 id="简介-9"><a href="#简介-9" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>BitNet: Scaling 1-bit Transformers for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2023</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-8"><a href="#摘要-8" class="headerlink" title="摘要"></a>摘要</h2><p>提出BitNet，可扩展和稳定的1为transformer架构模型，目的时从0开始训练1位权重</p>
<h2 id="方法-7"><a href="#方法-7" class="headerlink" title="方法"></a>方法</h2><p>BitNet架构如下图：使用BitLinear代替传统的矩阵乘法，采用二值化模型权重。保留其他组件的高精度</p>
<p><img src="/images/2024714/101.png" alt=""></p>
<h3 id="BitLinear"><a href="#BitLinear" class="headerlink" title="BitLinear"></a>BitLinear</h3><p>首先使用Sign函数将权重二值化为$±1$,在二值化之前将权重集中为零均值，二值化之后采用比例因子$\beta$来减少实值和二值化后的误差：</p>
<p><img src="/images/2024714/102.png" alt=""></p>
<p>文章进一步将激活量化到b位精度，使用absmax量化方法，缩放激活到$<a href="Q_b=2^{b-1}">-Q_b,Q_b</a>$，通过与$Q_b$相乘并除以矩阵的绝对最大值:</p>
<p><img src="/images/2024714/103.png" alt=""></p>
<p>对于非线性函数之前的激活，例如（ReLU）,通过减去输入最小值将其缩放到$[0,Q_b]$以便所有值都是非负的：</p>
<p><img src="/images/2024714/104.png" alt=""></p>
<p>文章将激活量化为8位，为了稳定性和效率，在训练期间执行每张量量化，在推理期间执行每token量化，矩阵乘法为：</p>
<p><img src="/images/2024714/105.png" alt=""></p>
<p>假设$W$和$x$互相独立，则输出的估计方差为：</p>
<p><img src="/images/2024714/106.png" alt=""></p>
<p>对于全精度计算，使用标准初始化方法（例如 Kaiming 初始化或 Xavier 初始化），输出 Var(y) 的方差为 1，这对训练稳定性有很大好处。为了保留量化后的方差，我们在激活量化之前引入了 LayerNorm [BKH16] 函数。这样，输出 y 的方差估计为 Var(y) ≈ E[LN(ex)2] = 1，其与全精度对应项 Var(y) 具有相同的幅度，因此BitLinear公式为：</p>
<p><img src="/images/2024714/107.png" alt=""></p>
<h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>直通估计器。为了训练我们的 1 位模型，我们使用直通估计器 (STE)[BLC13] 来近似反向传播期间的梯度。此方法在向后传递过程中绕过不可微函数，例如 Sign（等式 2）和 Clip（等式 5）函数。 STE 允许梯度流过网络，而不受这些不可微函数的影响，从而可以训练我们的量化模型。</p>
<ol>
<li><strong>低精度量化</strong>：在训练过程中，权重（weights）和激活（activations）被量化为低精度格式。低精度通常指的是16位浮点数（例如FP16）或者更低的精度，如8位整数（INT8）。这样做可以减少内存占用和加速计算，因为低精度数值需要的存储空间和计算资源更少。</li>
<li><strong>高精度存储</strong>：然而，为了确保训练的稳定性和准确性，梯度（gradients）和优化器状态（optimizer states）仍然以高精度格式存储。高精度通常指的是32位浮点数（FP32），这种格式提供了更高的数值精度，有助于减少训练过程中的数值误差。</li>
<li><strong>潜在权重</strong>：根据之前的研究工作[LSL+21]，对于可学习的参数，我们维护一个高精度的潜在权重（latent weight）。这个潜在权重用于累积参数更新，以确保参数更新的精度。</li>
<li><strong>前向传播中的二值化</strong>：在模型的前向传播过程中，潜在权重会被即时二值化（binarized），即转换为二进制形式（通常是1位，即-1或+1）。这种二值化可以进一步减少计算复杂度和内存需求。</li>
<li><strong>推理过程</strong>：值得注意的是，这些二值化的权重并不用于模型的推理过程。推理时，模型通常使用全精度的权重，以确保输出的准确性。</li>
</ol>
<h2 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/108.png" alt=""></p>
<h1 id="11"><a href="#11" class="headerlink" title="11"></a>11</h1><h2 id="简介-10"><a href="#简介-10" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Compressible Dynamics in Deep Overparameterized Low-Rank Learning &amp; Adaptation</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="12"><a href="#12" class="headerlink" title="12"></a>12</h1><h2 id="简介-11"><a href="#简介-11" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>DATA-AWARE LOW-RANK COMPRESSION FOR LARGE NLP MODELS</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td>2024</td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>压缩技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="13"><a href="#13" class="headerlink" title="13"></a>13</h1><h2 id="简介-12"><a href="#简介-12" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Extreme Compression of Large Language Models via Additive Quantization</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="14"><a href="#14" class="headerlink" title="14"></a>14</h1><h2 id="简介-13"><a href="#简介-13" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Few-Shot Diffusion Models Escape the Curse of Dimensionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="15"><a href="#15" class="headerlink" title="15"></a>15</h1><h2 id="简介-14"><a href="#简介-14" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Few-Shot Diffusion Models Escape the Curse of Dimensionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="16"><a href="#16" class="headerlink" title="16"></a>16</h1><h2 id="简介-15"><a href="#简介-15" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="17"><a href="#17" class="headerlink" title="17"></a>17</h1><h2 id="简介-16"><a href="#简介-16" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Literature survey on low rank approximation of matrices∗</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="18"><a href="#18" class="headerlink" title="18"></a>18</h1><h2 id="简介-17"><a href="#简介-17" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-9"><a href="#摘要-9" class="headerlink" title="摘要"></a>摘要</h2><p>训练后量化在低位量化中失效，文章提出了一种无数据蒸馏方法，利用预训练模型产生的输出，保留了原始输出的分布。文章量化了权重激活和KV缓存。</p>
<h2 id="方法-8"><a href="#方法-8" class="headerlink" title="方法"></a>方法</h2><p>概述：</p>
<p><img src="/images/2024714/181.png" alt="图1"></p>
<p>量化感知训练存在的挑战：</p>
<ul>
<li>选择合适的微调数据很重要，微调数据太窄，或者与原始的预训练分布不同，可能会损害模型性能，</li>
<li>LLM训练规模和复杂性，很难准确复制其原始的训练设置</li>
</ul>
<h3 id="Data-free-Distillation"><a href="#Data-free-Distillation" class="headerlink" title="Data-free Distillation"></a>Data-free Distillation</h3><ol>
<li><strong>方法概述</strong>：<ul>
<li>该方法通过从原始预训练模型中生成下一个标记（token）来合成数据。</li>
<li>如图1(a)所示，首先从词汇表中随机选择一个起始标记 <code>&lt;start&gt;</code>，然后让预训练模型生成下一个标记 <code>&lt;out1&gt;</code>。</li>
<li>生成的标记 <code>&lt;out1&gt;</code> 被附加到起始标记 <code>&lt;start&gt;</code> 上，用于生成新的输出 <code>&lt;out2&gt;</code>。</li>
<li>这个迭代过程重复进行，直到达到句子结束标记或达到最大生成长度。</li>
</ul>
</li>
<li><strong>采样策略</strong>：<ul>
<li>最直接的方法是选择排名第一的候选标记作为下一个标记，但这种方法生成的句子缺乏多样性，并且可能会循环重复某些标记。</li>
<li>为了解决这个问题，采用了从预训练模型的 SoftMax 输出中随机采样下一个标记的方法，这种方法基于概率分布，生成的句子更加多样化，并且显著提高了微调学生模型的准确性。</li>
</ul>
</li>
<li><strong>关键发现</strong>：<ul>
<li>研究发现，最初的几个标记对预测趋势起着至关重要的作用，因此这些标记需要有较高的置信度。</li>
<li>在生成过程中，采用了一种混合采样策略：对于前3到5个标记，确定性地选择排名第一的预测，而对于剩余的标记，则随机采样。</li>
</ul>
</li>
</ol>
<h3 id="Quantization-Aware-Training"><a href="#Quantization-Aware-Training" class="headerlink" title="Quantization-Aware Training"></a>Quantization-Aware Training</h3><h5 id="Preliminaries-1"><a href="#Preliminaries-1" class="headerlink" title="Preliminaries"></a>Preliminaries</h5><p>对于最小最大量化，公式可以表述为：</p>
<p><img src="/images/2024714/182.png" alt=""></p>
<p>对于对称量化：</p>
<p><img src="/images/2024714/183.png" alt=""></p>
<p>对于非对称量化：</p>
<p><img src="/images/2024714/184.png" alt=""></p>
<p>现在的工作一般采用剪切的量化：</p>
<p><img src="/images/2024714/185.png" alt=""></p>
<h5 id="Quantization-for-Large-Language-Models"><a href="#Quantization-for-Large-Language-Models" class="headerlink" title="Quantization for Large Language Models"></a>Quantization for Large Language Models</h5><p><img src="/images/2024714/186.png" alt="图2"></p>
<p>上图为量化的模型，由于权重和激活中存在显著的异常值，他们对量化精度有影响，在量化过程中修剪这些异常值会导致精度下降，因此文章选择保留这些异常值，而且文章发现激活权重大多是对称分布的，因此选择MinMax量化：</p>
<p><img src="/images/2024714/187.png" alt=""></p>
<p>为了确保高效的量化，文章采用per-token激活量化和per-channel权重量化。同时对KV缓存采用激活量化的类似方法</p>
<p>文章使用交叉熵的蒸馏训练量化网络：</p>
<p><img src="/images/2024714/188.png" alt=""></p>
<p><em><u>QAT的关键思想是在模型训练过程中引入量化的操作，让模型“意识”到量化过程，并通过反向传播优化模型参数，以适应量化带来的影响。具体来说，QAT遵循以下步骤：</u></em></p>
<ol>
<li><em><u>模拟量化：在模型的前向传播过程中，将权重和激活值通过量化和反量化的过程，模拟量化在实际部署中的效果。这意味着，权重和激活值先被量化到低位宽的整数表示，然后再被反量化回浮点数，以供后续的计算使用。</u></em></li>
<li><em><u>梯度近似：由于量化操作（如取整）是不可微分的，为了在反向传播过程中计算梯度，QAT采用了梯度近似的技术。常见的方法包括直接通过量化操作传递梯度（即假设量化操作的梯度为1）或使用“直通估计”（Straight Through Estimator, STE）。</u></em></li>
<li><em><u>优化参数：通过模拟量化的前向传播和梯度近似的反向传播，模型参数在训练过程中得到优化，使模型适应量化后的表示。</u></em></li>
</ol>
<p><em><u>QAT优点：</u></em></p>
<ol>
<li><em><u>减少量化损失：由于QAT在训练过程中考虑了量化的影响，它可以显著减少量化对模型精度的负面影响，相比于PTQ，通常能够获得更好的性能。</u></em></li>
<li><em><u>提高模型兼容性：QAT使模型适应了量化后的权重和激活值的分布，从而提高了模型在特定硬件上的兼容性和运行效率。</u></em></li>
<li><em><u>灵活性和适应性：QAT允许开发者根据目标平台的特定需求，调整量化方案（如量化位宽、量化策略等），优化模型的性能。</u></em></li>
</ol>
<h2 id="实验-7"><a href="#实验-7" class="headerlink" title="实验"></a>实验</h2><p>All of our experiments are conducted using a single 8-gpu training node.</p>
<p><img src="/images/2024714/189.png" alt=""></p>
<h1 id="19"><a href="#19" class="headerlink" title="19"></a>19</h1><h2 id="简介-18"><a href="#简介-18" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="20"><a href="#20" class="headerlink" title="20"></a>20</h1><h2 id="简介-19"><a href="#简介-19" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization">https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization</a></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-10"><a href="#摘要-10" class="headerlink" title="摘要"></a>摘要</h2><p>一般的非均匀量化忽略了复杂的投影过程，这在硬件部署中会产生不可忽略的时间和开销，文章提出了非均匀到均匀量化N2UQ方法。通过学习灵活的不等距输入阈值来拟合底层分布，同时i将这些实值输入量化为等距输出水平。为了训练具有可学习输入阈值的量化网络，文章提出了G-STE。此外文章考虑保持熵的正则化，进一步减少权值量化中的信息损失。</p>
<p>(a) 先前的非均匀量化函数以等距级别输出权重和激活，这需要将浮点级别映射到二进制数字的后处理，以获得量化的加速效果[1,13,50] 。 (b) 所提出的 N2UQ 学习输入阈值以提供更大的灵活性，同时输出统一的量化值，从而实现硬件友好的线性映射和高效的按位运算。棘手的梯度计算输入阈值通过提出的广义直通估计器（G-STE）来解决</p>
<p><img src="/images/2024714/201.png" alt="图1"></p>
<h2 id="方法-9"><a href="#方法-9" class="headerlink" title="方法"></a>方法</h2><h3 id="Preliminaries-2"><a href="#Preliminaries-2" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>均匀量化实现矩阵乘法加速：</p>
<p><img src="/images/2024714/202.png" alt=""></p>
<h3 id="Nonuniform-to-Uniform-Quantization"><a href="#Nonuniform-to-Uniform-Quantization" class="headerlink" title="Nonuniform-to-Uniform Quantization"></a>Nonuniform-to-Uniform Quantization</h3><ul>
<li>量化网络加速的前提是量化后的权重和激活可以用二进制表示</li>
</ul>
<p><img src="/images/2024714/203.png" alt=""></p>
<ol>
<li><strong>均匀量化与非均匀量化</strong>：<ul>
<li><strong>均匀量化</strong>：当激活$ a_q$和权重$ w_q$ 被均匀量化时，它们可以很容易地通过线性映射转换为二进制表示。这意味着每个量化级别之间的间隔是相等的。</li>
<li><strong>非均匀量化</strong>：对于非均匀量化的 $a_q$和 $w_q$，情况则不同。非均匀量化意味着量化级别之间的间隔不相等。</li>
</ul>
</li>
<li><strong>非均匀量化的挑战</strong>：<ul>
<li>非均匀量化的输出实际上是 $2^n$ 个不等距的浮点值（其中 $n$ 是量化位数），如图1所示。</li>
<li>将这些非均匀量化的值转换为$ n$ 位二进制数字通常需要额外的操作或者使用查找表（LUTs）。</li>
</ul>
</li>
</ol>
<ul>
<li>量化器的特性：</li>
</ul>
<ol>
<li><strong>量化函数</strong>：量化函数$ x_q = F_Q(x_r)$描述了将输入$ x_r$(原始信号）转换为输出 $x_q$（量化后的信号）的过程。</li>
<li><strong>均匀量化级别</strong>：均匀量化级别指的是量化后的输出 $x_q$的各个级别之间的间隔是相等的。</li>
<li><strong>量化器设计的灵活性</strong>：这段话指出，通过适当的量化器设计，可以实现输出 $x_q$的均匀量化级别，而不必要求输入$ x_r$ 的各个范围也是均匀的。换句话说，即使输出 $x_q$ 的量化级别是均匀的，输入 $x_r$的量化范围也可以是非均匀的。</li>
</ol>
<p>总结来说，这段话强调了量化器设计的一个关键点：即使输出是均匀量化的，输入的量化范围也可以是非均匀的。这表明量化器的设计具有一定的灵活性，可以根据具体需求来调整输入和输出的量化特性。</p>
<h5 id="Forward-Pass-Threshold-Learning-Quantization"><a href="#Forward-Pass-Threshold-Learning-Quantization" class="headerlink" title="Forward Pass: Threshold Learning Quantization"></a>Forward Pass: Threshold Learning Quantization</h5><p>基于上述观察，文章开发了用于激活量化的非均匀到均匀量化器，其前向传递函数为：</p>
<p><img src="/images/2024714/204.png" alt=""></p>
<h5 id="Backward-Pass-Generalized-Straight-Through-Estimator-G-STE"><a href="#Backward-Pass-Generalized-Straight-Through-Estimator-G-STE" class="headerlink" title="Backward Pass: Generalized Straight-Through Estimator (G-STE)"></a>Backward Pass: Generalized Straight-Through Estimator (G-STE)</h5><p>反向传播的困难：</p>
<ol>
<li>上述公式对于输入$x_r$的倒数都是0</li>
<li>关于阈值参数的梯度计算是难以处理的</li>
</ol>
<p>对于问题1，以前的量化工作采用直通估计器STE来近似量化函数的反向梯度：</p>
<p><img src="/images/2024714/205.png" alt=""></p>
<p>这个简单的近似函数非常适合均匀量化器。然而，STE 在量化器的输入和输出间隔中隐式强制执行等轴纵横比，因为它将量化函数视为向后传递中的恒等函数。这阻碍了量化器设计在固定输出电平的同时允许可学习的输入阈值。STE在均匀量化中效果良好，但它隐含地强制了输入和输出区间在量化函数中具有相同的轴向比例。因为在反向传播时，STE将量化函数视为恒等函数，这阻碍了量化器设计中允许学习输入阈值同时固定输出水平的能力。因此文章提出了广义直通估计器（原文推导）</p>
<p><strong>引理1</strong>.在二值化中，前向确定性二值化函数的梯度近似的直通估计（STE）可以从随机二值化函数的期望中导出[19]。</p>
<p>在随机二值化中，实值变量根据与-1和1的距离随机二值化：</p>
<p><img src="/images/2024714/206.png" alt=""></p>
<p>这里$\tilde{x}_{i,l}^{b}$表示随机的二元变量，为了更新$W_{ij,l}$，通过随机二值化函数计算预期梯度：</p>
<p><img src="/images/2024714/207.png" alt=""></p>
<p><em><u>推导</u></em>：</p>
<p><img src="/images/2024714/209.png" alt=""></p>
<p>这里$\mathbb{E}$,$\mathbb{E}_{\tilde{x}_{i,l}^{b}}$和$\mathbb{E}_{/\tilde{x}_{i,l}^{b}}$分别表示对整个网络，仅随机二元量化和除随机二元变量之外的期望，有：</p>
<p><img src="/images/2024714/208.png" alt=""></p>
<p>这里得到了二值化函数的估计，在阈值附件相同地传递梯度，在实值输入距离阈值太远时忽略梯度。而且，采用确定性二值化函数可以在等式1中设置概率阈值p=0.5来获得：</p>
<p><img src="/images/2024714/210.png" alt=""></p>
<p>为此，我们证明 STE 将随机二值化的期望编码为前向确定性二值化函数的后向近似。</p>
<p><strong>引理 2</strong>. 量化函数$x^q$ 可以被视为具有不同阈值的二值化函数$x_b$的求和,如下图所示：</p>
<p><img src="/images/2024714/211.png" alt=""></p>
<p><img src="/images/2024714/212.png" alt=""></p>
<p>从第一个量化段开始，将初始点表示为$s$，它的长度表示为$a_1$。遵循随机二值化的概念，在$[s,s+a_1]$范围内，真实值变量可以随机量化为0/1，概率与其到$s/(s+a_1)$的距离称正比（这里将$s=-1,a_1=2$带入，正好为公式4）：</p>
<p><img src="/images/2024714/213.png" alt=""></p>
<p>类似于公式6，该量化段的推导可以根据等式6的期望来计算：</p>
<p><img src="/images/2024714/214.png" alt="公式9"></p>
<p>这样，阈值参数$a_1$对网络的影响很好的被编码在后向逼近公式9中，在前向传播中，不需要随机种子，而是采用确定性量化，将概率阈值设为0.5：</p>
<p><img src="/images/2024714/215.png" alt=""></p>
<p><strong>定理1</strong>:Generalized straight-through estimator:</p>
<p><img src="/images/2024714/216.png" alt=""></p>
<p>非均匀到均匀量化器的后向梯度近似函数:</p>
<p><img src="/images/2024714/217.png" alt=""></p>
<p>并且：</p>
<p><img src="/images/2024714/218.png" alt=""></p>
<h5 id="Entropy-Preserving-Weight-Regularization"><a href="#Entropy-Preserving-Weight-Regularization" class="headerlink" title="Entropy Preserving Weight Regularization"></a>Entropy Preserving Weight Regularization</h5><p>文章提出了权重正则化，鼓励量化权重具有更多的信息承载能力。一个重要的观察是，真实网络中的权重数量级小，但是量化后的权重通常会在$[-1,1]$范围内扩展，这种幅度上的不匹配将导致量化权重崩溃到接近于零的几个量化级别。如下图所示，导致信息损失：</p>
<p><img src="/images/2024714/219.png" alt=""></p>
<p>从信息论的角度来看，当权重包含更多熵时，可以保留更多信息，文章因此在量化器之前对真实权重进行正则化，来获得量化权重的最大熵：</p>
<p><img src="/images/2024714/220.png" alt=""></p>
<p>这里$p_i$时被量化到第$i$个量化级别的真实权重的比例，N是总的量化级别的数量。基于朗格朗日乘数：</p>
<p><img src="/images/2024714/221.png" alt=""></p>
<p>当实值权重被量化到多个量化级别的比例相等时，量化后的权重中的信息熵达到最大值。</p>
<p><img src="/images/2024714/222.png" alt=""></p>
<h2 id="实验-8"><a href="#实验-8" class="headerlink" title="实验"></a>实验</h2><p>卷积神经网络上的实验</p>
<h1 id="21"><a href="#21" class="headerlink" title="21"></a>21</h1><h2 id="简介-20"><a href="#简介-20" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>OMNIQUANT: OMNIDIRECTIONALLY CALIBRATED QUANTIZATION FOR LARGE LANGUAGE MODELS</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-11"><a href="#摘要-11" class="headerlink" title="摘要"></a>摘要</h2><p>训练后量化在低bit量化性能低，文章提出了OmniQuant，包括两个组件：</p>
<ul>
<li>可学习权重裁剪（LWC）:通过优化裁剪阈值来调节权重极值</li>
<li>可学习等效变换（LET）：将量化挑战从激活转移到权重来解决激活异常</li>
</ul>
<p>OmniQuant在逐块误差最小化的可微框架中运行，可以有效优化仅权重量化和权重激活量化的量化过程。</p>
<h2 id="方法-10"><a href="#方法-10" class="headerlink" title="方法"></a>方法</h2><p>大语言模型量化的两个困难：</p>
<ul>
<li>异常channel的存在，导致激活很难量化，考虑到激活是平坦且均匀的，可以将激活量化难度转移到权重量化熵；</li>
<li>权重的量化误差：可以以全精度保留这些权重</li>
</ul>
<p>文章提出可微分量化技术，学习量化参数。</p>
<h3 id="BLOCK-WISE-QUANTIZATION-ERROR-MINIMIZATION"><a href="#BLOCK-WISE-QUANTIZATION-ERROR-MINIMIZATION" class="headerlink" title="BLOCK-WISE QUANTIZATION ERROR MINIMIZATION"></a>BLOCK-WISE QUANTIZATION ERROR MINIMIZATION</h3><p>之前基于梯度的PTQ量化方法误差应用到数十亿模型中，文章提出了逐块量化误差最小化的设计，目标为：</p>
<p><img src="/images/2024714/2101.png" alt=""></p>
<p>这里$\mathcal{F}$表示transformer块的映射函数，$\Theta_{1,2}$分别是LWC和LET中的量化参数，逐块量化一个变压器的参数然后进入下一个。</p>
<h3 id="LEARNABLE-WEIGHT-CLIPPING"><a href="#LEARNABLE-WEIGHT-CLIPPING" class="headerlink" title="LEARNABLE WEIGHT CLIPPING"></a>LEARNABLE WEIGHT CLIPPING</h3><p>传统方法中，剪切阈值是直接学习的，而在LWC中，剪切强度被视为一个可学习的参数，通过训练过程来优化。这意味着LWC方法可能会更加灵活，能够根据数据和任务的特性来自适应地调整剪切强度，以获得更好的性能：</p>
<p><img src="/images/2024714/2102.png" alt=""></p>
<p>其中$\gamma \in[0,1]$和$\beta \in[0,1]$属于可学习系数。文章通过sigmoid函数实例化$\gamma$和$\beta $。且$\Theta_1 ={\gamma,\beta}$</p>
<p><img src="/images/2024714/2103.png" alt=""></p>
<h3 id="LEARNABLE-EQUIVALENT-TRANSFORMATION"><a href="#LEARNABLE-EQUIVALENT-TRANSFORMATION" class="headerlink" title="LEARNABLE EQUIVALENT TRANSFORMATION"></a>LEARNABLE EQUIVALENT TRANSFORMATION</h3><p>一个线性层可以表示为：</p>
<p><img src="/images/2024714/2104.png" alt=""></p>
<p>最后，对变换后的激活和权重进行量化，如下所示</p>
<p><img src="/images/2024714/2105.png" alt=""></p>
<p>$Q_a$表示普通的MinMax量化，$Q_w$是具有LEC的MinMax量化。文章在 LLM 的所有线性层中都采用了这种等效变换，除了 FFN 的第二个线性层，如下图 所示。这可能是因为非线性层之后特征的高度稀疏性导致应用可学习的等效变换时梯度不稳定。</p>
<p><img src="/images/2024714/2106.png" alt=""></p>
<p>除了线性层之外，注意力操作也占计算的很大一部分。此外，LLM 的自回归模式需要为每个令牌存储键值 (KV) 缓存，这会导致长序列需要大量内存。因此，文章还在权重激活量化设置中将 Q/K/V 矩阵量化为低位。具体来说，自注意力亲和力矩阵的可学习等效变换可以写为：</p>
<p><img src="/images/2024714/2107.png" alt=""></p>
<p>$\Theta_2 ={\delta,s,s_a}$</p>
<h2 id="实验-9"><a href="#实验-9" class="headerlink" title="实验"></a>实验</h2><p>Single A100-40G GPU，1-16h</p>
<p><img src="/images/2024714/2108.png" alt=""></p>
<h1 id="22"><a href="#22" class="headerlink" title="22"></a>22</h1><h2 id="简介-21"><a href="#简介-21" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE MODELS</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="23"><a href="#23" class="headerlink" title="23"></a>23</h1><h2 id="简介-22"><a href="#简介-22" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Cornell-RelaxML/QuIP">https://github.com/Cornell-RelaxML/QuIP</a></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="摘要-12"><a href="#摘要-12" class="headerlink" title="摘要"></a>摘要</h1><p>文章引入一种新的量化与非相干处理（QuIP）方法其原理是，量化过程可以从非相干的权重和Hessian矩阵中获益。具体来说，这意味着权重的大小是均匀的，并且需要精确舍入的方向与坐标轴不对齐。</p>
<p>“非相干性”（incoherence）通常指的是两个或多个信号或向量在相位、方向或频率上的随机分布，它们之间没有固定的相位关系或对齐。在QuIP（Quantization with Incoherence Processing）的上下文中，权重的非相干性和海森矩阵的非相干性可以这样理解：</p>
<ol>
<li><strong>权重的非相干性</strong>：<ul>
<li>在深度学习中，权重是神经网络中的参数，它们决定了输入特征如何影响输出。</li>
<li>权重的非相干性意味着这些权重在不同的方向上没有固定的相位关系，它们的分布是随机的或无序的。物理上，这可以类比于一个多粒子系统，其中每个粒子的运动方向是随机的，没有一个统一的模式。</li>
</ul>
</li>
<li><strong>海森矩阵的非相干性</strong>：<ul>
<li>海森矩阵是二阶导数矩阵，用于描述函数的曲率，即权重参数的局部变化如何影响损失函数。</li>
<li>海森矩阵的非相干性意味着矩阵中的元素（即二阶导数）在不同的方向上没有固定的模式或对齐。物理上，这可以类比于一个多维空间中的力场，其中力的方向在空间中是随机分布的，没有一个统一的方向。</li>
</ul>
</li>
</ol>
<p>QuIP包含两个步骤：</p>
<ol>
<li>自适应舍入过程：一个自适应舍入来最小化损失函数</li>
<li>高效的预处理和后处理：通过随机正交矩阵的乘法确保权重和hessian不相干性</li>
</ol>
<h2 id="方法-11"><a href="#方法-11" class="headerlink" title="方法"></a>方法</h2><p>问题定义：</p>
<p>遵循现有得到训练后量化方法，最小化代理目标来舍入权重：</p>
<p><img src="/images/2024714/2301.png" alt="公式1"></p>
<p>这个公式中$W$是权重矩阵，这样可是让量化在神经元之间并行运算。</p>
<h3 id="LDLQ-An-Optimal-Adaptive-Rounding-Method"><a href="#LDLQ-An-Optimal-Adaptive-Rounding-Method" class="headerlink" title="LDLQ: An Optimal Adaptive Rounding Method"></a>LDLQ: An Optimal Adaptive Rounding Method</h3><p>文章定义一系列自适应舍入策略来优化目标方程，针对$k=1,2,\dots,n$次迭代，对权重进行更新：</p>
<p><img src="/images/2024714/2302.png" alt=""></p>
<p>这里$W_k$表示第$k$列，$W_{1:(k-1)}$表示前$k-1$列，$\mathcal{Q}$表示最近舍入或者标准无偏舍入到整数（即$\mathbf{E}[\mathcal{Q}(z)=z]$），$a_k\in \R^{k-1}$是一些向量序列。</p>
<p>该设计方案一次对一列进行四舍五入，每一步中，都会添加一个校正项，最终可以表示为：</p>
<p><img src="/images/2024714/2303.png" alt=""></p>
<p>这里$U$是严格的上三角矩阵，它的列向量是$a_k$，$\mathcal{Q}$按元素进行操作。</p>
<p>令$\eta$表示$\mathcal{Q}$的量化误差：</p>
<p><img src="/images/2024714/2304.png" alt=""></p>
<p>可以推导出（把公式（2）带入即可）：</p>
<p><img src="/images/2024714/2305.png" alt=""></p>
<p>可以将目标公式重写为：</p>
<p><img src="/images/2024714/2306.png" alt=""></p>
<p><strong>The LDLQ Method</strong> 如何选择$U$？如果选择对$H$进行LDL分解：</p>
<p><img src="/images/2024714/2307.png" alt=""></p>
<p><img src="/images/2024714/2308.png" alt=""></p>
<h3 id="Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure"><a href="#Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure" class="headerlink" title="Deriving the Optimality of the LDLQ Adaptive Rounding Procedure"></a>Deriving the Optimality of the LDLQ Adaptive Rounding Procedure</h3><p>文章推理了LDLQ的最优性，考虑代理损失的最差和平均情况，令$\mathcal{A}$表示舍入方法，令$\mathcal{A}(W,H)$表示量化的结果，则最坏和平均代理损失定义为：</p>
<p><img src="/images/2024714/2309.png" alt=""></p>
<p><strong>定理1</strong>LDLQ在舍入方法中是最差和平均情况下最优的：</p>
<p><img src="/images/2024714/2310.png" alt=""></p>
<p>对于非QuIP框架的，设置$U=0$则比较$tr(D)$和$tr(H)$实验发现：</p>
<p><img src="/images/2024714/2311.png" alt=""></p>
<h3 id="Incoherence-Optimality-with-a-Spectral-Bound"><a href="#Incoherence-Optimality-with-a-Spectral-Bound" class="headerlink" title="Incoherence: Optimality with a Spectral Bound"></a>Incoherence: Optimality with a Spectral Bound</h3><p>文章观察到H为低秩，是否可以使用H的频谱来限制LQLQ的行为，从而限制$tr(D)$</p>
<p><img src="/images/2024714/2312.png" alt=""></p>
<p><img src="/images/2024714/2313.png" alt=""></p>
<p><strong>定理2</strong></p>
<p><img src="/images/2024714/2314.png" alt=""></p>
<p>文章通过不相干处理来获得$tr(D)$的界限，该界限取决于H频谱。根据下列公式可以发现，H的秩k越小，LDLQ损失越小：</p>
<p><img src="/images/2024714/2315.png" alt=""></p>
<h2 id="Quantization-With-Incoherence-Processing-Incoherence-Processing-Step"><a href="#Quantization-With-Incoherence-Processing-Incoherence-Processing-Step" class="headerlink" title="Quantization With Incoherence Processing: Incoherence Processing Step"></a>Quantization With Incoherence Processing: Incoherence Processing Step</h2><p>经过上述分析，不相干有利于降低精度，文章接下来预处理权重和hession矩阵，确保不相干属性。使对称矩阵不相干的一种直接方法是通过均匀随机正交矩阵将其共轭。令$U\in \R^{m×m}$,$V\in \R^{n×n}$通过随机正交乘法确保权重和海森矩阵不相干：</p>
<p><img src="/images/2024714/2316.png" alt=""></p>
<p>注意这种变化没有改变目标函数的形式：</p>
<p><img src="/images/2024714/2317.png" alt=""></p>
<h3 id="Incoherence-via-Efficient-Orthogonal-Multiplication"><a href="#Incoherence-via-Efficient-Orthogonal-Multiplication" class="headerlink" title="Incoherence via Efficient Orthogonal Multiplication"></a>Incoherence via Efficient Orthogonal Multiplication</h3><p>上述对于存储不会引入开销，但是如果对于推理则会产生额外的开销。文章将 $n$ 分解为两个近似相等的因子 $p$和 $q$，其中 $p$ 和 $q$都近似等于 $n$的平方根。然后设置 $U$ 为 $U_L$ 和 $U_R$的克罗内克积（Kronecker product），其中 $U_L$是从 $p \times p$正交矩阵中均匀采样的，$U_R$是从 $q \times q$正交矩阵中均匀采样的。通过将向量$x$重塑为一个$p \times q$的矩阵，可以完成$ x$与矩阵 $U$的乘法。左侧乘以 $U_L$，在右侧乘以 $U_R$的转置$ U^T_R$，然后再重塑回来。</p>
<p><img src="/images/2024714/2318.png" alt=""></p>
<h2 id="Additional-Heuristics"><a href="#Additional-Heuristics" class="headerlink" title="Additional Heuristics"></a>Additional Heuristics</h2><p><img src="/images/2024714/2319.png" alt=""></p>
<h2 id="实验-10"><a href="#实验-10" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2320.png" alt=""></p>
<h1 id="24"><a href="#24" class="headerlink" title="24"></a>24</h1><h2 id="简介-23"><a href="#简介-23" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>R2 Loss: Range Restriction Loss for Model Compression and Quantization</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-13"><a href="#摘要-13" class="headerlink" title="摘要"></a>摘要</h2><p>为了提高量化精度，文章专注于权重的异常值。文章提出范围限制损失，通过在预训练期间从权重中删除异常值来构建低位量化。在训练期间提出了三种不同的损失作为辅助损失。文章指出：$L_{\infty}$损失和$Margin \ R^2$损失对于对称量化非常有效，而$Soft-Min-Max \ R^2$损失显示出更好的模型压缩性能 。</p>
<h2 id="方法-12"><a href="#方法-12" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/2024714/2405.png" alt=""></p>
<p>文章引入范围限制损失作为辅助损失，以减少每一层权重的范围，以获得更好的预训练模型来进一步的压缩或者量化。与传统的$L_1$或$L_2$正则化不同，Range Restriction Loss专门针对权重中的异常值（outliers），通过惩罚这些异常值来限制它们的范围。文章从$L_{\infty}$损失扩展到margin 损失，最后引入soft-min-max损失将$R^2$损失。</p>
<p>$L_{\infty}\ R^2 \ \text{loss}$：该方法通过在训练过程中添加$L_{\infty}(W)$作为模型中每一层辅助损失，以迭代方式惩罚离群值：</p>
<p><img src="/images/2024714/2401.png" alt=""></p>
<p>$\text{Margin} \ R^2 \ \text{loss}$：这是上面的扩展，文章对允许的权重范围定义了一个余量，任何超出此界限的权重都会受到处罚，文章还对边距的宽度进行处罚，来确保整体权重分布范围较小：</p>
<p><img src="/images/2024714/2402.png" alt=""></p>
<p>$\text{Soft-min-max} \ R^2 \ \text{loss}$：这是不对称的损失，消除对全权重大小的约束并严格执行权重范围：</p>
<p><img src="/images/2024714/2403.png" alt=""></p>
<p>其中温度$\alpha$是可学习参数。这种损失不仅会平滑地惩罚离群值，而且还会惩罚接近离群值的权重。</p>
<p>各种损失的可视化如下：</p>
<p><img src="/images/2024714/2404.png" alt=""></p>
<h2 id="实验-11"><a href="#实验-11" class="headerlink" title="实验"></a>实验</h2><p>卷积神经网络</p>
<p>eight GPUs</p>
<p><img src="/images/2024714/2406.png" alt=""></p>
<h1 id="25"><a href="#25" class="headerlink" title="25"></a>25</h1><h2 id="简介-24"><a href="#简介-24" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h1 id="26"><a href="#26" class="headerlink" title="26"></a>26</h1><h2 id="简介-25"><a href="#简介-25" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/smoothquant">https://github.com/mit-han-lab/smoothquant</a></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-14"><a href="#摘要-14" class="headerlink" title="摘要"></a>摘要</h2><p>现有的模型量化无法同时保持精度和硬件效率，文章提出了SmoothQuant，这是一种无需训练、保持精度的训练后量化方法，SmoothQuant通过使用数字等效转化将量化难度从激活转移到权重来离线平滑激活异常值。</p>
<h2 id="方法-13"><a href="#方法-13" class="headerlink" title="方法"></a>方法</h2><p>量化将高精度值映射到离散级别。文章研究整数均匀量化，量化过程可以表示为（这里是per-tensor量化）：</p>
<p><img src="/images/2024714/2601.png" alt=""></p>
<p>通过校准样本的激活来离线计算$\Delta$即静态量化，使用激活的运行时统计来获得$\Delta$即动态量化，并且量化具有不同的粒度级别：</p>
<p><img src="/images/2024714/2602.png" alt=""></p>
<h3 id="Review-of-Quantization-Difficulty"><a href="#Review-of-Quantization-Difficulty" class="headerlink" title="Review of Quantization Difficulty"></a>Review of Quantization Difficulty</h3><ol>
<li>激活比权重更难量化：激活存在异常值</li>
<li>异常值使激活量化变得困难</li>
<li>异常值持续存在于固定渠道中：异常值出现在一小部分channel中，会持续出现在所有token中，但是跨token的给定通道的幅度之间方差很小。由于异常值存在，而在每个channel的方差很小，如果对激活执行怕per-channel量化，则量化误差很小，下表为模拟结果：</li>
</ol>
<p><img src="/images/2024714/2604.png" alt=""></p>
<p><img src="/images/2024714/2603.png" alt=""></p>
<p>但是per-channel量化无法很好的映射到硬件加速的GEMM内核。在这些内核中，只能沿着矩阵乘法的外部维度（激活的token维度，权重的输出通道）进行缩放。</p>
<p><img src="/images/2024714/2605.png" alt=""></p>
<p>可以用下式表示：</p>
<p><img src="/images/2024714/2606.png" alt=""></p>
<h3 id="SmoothQuant"><a href="#SmoothQuant" class="headerlink" title="SmoothQuant"></a>SmoothQuant</h3><p>对激活进行缩放，对权重进行反缩放：</p>
<p><img src="/images/2024714/2607.png" alt=""></p>
<p>选择缩放因子，平滑权重量化和激活量化：</p>
<p><img src="/images/2024714/2608.png" alt=""></p>
<p>应用位置：</p>
<p><img src="/images/2024714/2609.png" alt=""></p>
<h2 id="实验-12"><a href="#实验-12" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2610.png" alt=""></p>
<h1 id="27"><a href="#27" class="headerlink" title="27"></a>27</h1><h2 id="简介-26"><a href="#简介-26" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Vahe1994/SpQR">https://github.com/Vahe1994/SpQR</a></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-15"><a href="#摘要-15" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出系数量化表示（SpQR）：识别和隔离导致特别大的量化误差的离群权重，并高精度存储他们，同时将其他权重压缩为3-4位。</p>
<h2 id="方法-14"><a href="#方法-14" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/2024714/2701.png" alt=""></p>
<p>识别异常值，对非常异常值采用小分组量化，对小分组量化的系数采用二次量化。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/images/2024714/2702.png" alt=""></p>
<h1 id="28"><a href="#28" class="headerlink" title="28"></a>28</h1><h2 id="简介-27"><a href="#简介-27" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要-16"><a href="#摘要-16" class="headerlink" title="摘要"></a>摘要</h2><p>将权重变为三元组{-1，0，1}，从头开始训练语言模型</p>
<h2 id="方法-15"><a href="#方法-15" class="headerlink" title="方法"></a>方法</h2><p>文章基于BitNet方法，相对于BitNet修改有：</p>
<h3 id="Quantization-Function"><a href="#Quantization-Function" class="headerlink" title="Quantization Function"></a>Quantization Function</h3><p>为了将权重限制在-1,0,1。文章采用$absmean$量化方法，首先按权重矩阵的平均绝对值缩放，然后将每个值舍入为{-1，0，1}中最接近的整数：</p>
<p><img src="/images/2024714/2801.png" alt=""></p>
<p>激活量化函数遵循Bitnet中的相同实现，文章没有将非线性函数之前的激活缩放到范围$[0,Q_b]$，而是每个激活都缩放为$[-Q_b,Q_b]$，以消除零点量化</p>
<h2 id="实验-13"><a href="#实验-13" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2802.png" alt=""></p>
<h1 id="29"><a href="#29" class="headerlink" title="29"></a>29</h1><h2 id="简介-28"><a href="#简介-28" class="headerlink" title="简介"></a>简介</h2><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation</th>
</tr>
</thead>
<tbody>
<tr>
<td>期刊</td>
<td></td>
</tr>
<tr>
<td>发表时间</td>
<td></td>
</tr>
<tr>
<td>代码</td>
<td></td>
</tr>
<tr>
<td>技术</td>
</tr>
</tbody>
</table>
</div>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ghtll</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://ghtll.github.io/2024/07/14/2024714/">https://ghtll.github.io/2024/07/14/2024714/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">ghtll</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/">
                                    <span class="chip bg-color">模型压缩</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2024/07/14/2024714/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/2.jpg" class="responsive-img" alt="模型压缩总结2">
                        
                        <span class="card-title">模型压缩总结2</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            总结近期的文章
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-07-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" class="post-category">
                                    文献阅读
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/">
                        <span class="chip bg-color">模型压缩</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/04/10/quip/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/3.jpg" class="responsive-img" alt="quip">
                        
                        <span class="card-title">quip</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            不相干量化
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" class="post-category">
                                    文献阅读
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E9%87%8F%E5%8C%96/">
                        <span class="chip bg-color">量化</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: ght<br />'
            + '文章作者: gaohaotian<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->


<!-- 代码块收缩 -->



    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2024</span>
            
            <a href="/about" target="_blank">gaohaotian</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2023";
                        var startMonth = "9";
                        var startDate = "17";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ghtll" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:260878610@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=260878610" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 260878610" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/sakura.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>

</html>
