<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CoFi</title>
      <link href="/2023/10/27/CoFi/"/>
      <url>/2023/10/27/CoFi/</url>
      
        <content type="html"><![CDATA[<h1 id="CoFi"><a href="#CoFi" class="headerlink" title="CoFi"></a>CoFi</h1><p><strong>Structured Pruning Learns Compact and Accurate Models</strong></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>同时剪枝粗粒度（例如层）和细粒度（例如头部和隐藏单元）的模块，这些模块使用不同粒度的掩码来控制每个参数的剪枝决策。还设计了一种分层蒸馏策略，以在优化过程中从未剪枝到已剪枝的模型中传递知识。<br>$$<br>I_W&#x3D;\frac{1}{|D|}\sum^{|D|}_{i&#x3D;1}(\frac{\partial}{\partial W}L(d_i;w))^2<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩总结</title>
      <link href="/2023/10/16/summery1016/"/>
      <url>/2023/10/16/summery1016/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Losparse"><a href="#1-Losparse" class="headerlink" title="1 Losparse"></a>1 Losparse</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>LoSparse: Structured Compression of Large LanguageModels based on Low-Rank and Sparse Approximation ∗</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2023.7</td></tr><tr><td>代码</td><td><a href="https://github.com/yxli2123/LoSparse">yxli2123&#x2F;LoSparse (github.com)</a></td></tr><tr><td>压缩技术</td><td>低秩近似、结构剪枝</td></tr></tbody></table><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>将权重近似为(将这种分解应用于模型的每个权重矩阵)：<br>$$<br>W&#x3D;UV+S<br>$$<br><img src="/../images/summery1016_imgs/1.png"></p><p>重要性评分公式方法：</p><p><img src="/../images/summery1016_imgs/2.png" alt="image-20231016142137668"></p><p><img src="/../images/summery1016_imgs/43.png"></p><h2 id="i"><a href="#i" class="headerlink" title="i"></a>i</h2><p>剪枝采用迭代剪枝，在迭代过程中S的重要性评估不稳定，原文采用平滑+大batchsize。由于样本造成的不稳定，这里可以想一个办法。要不让样本更稳定，要不更换一个重要性评估方法。</p><p>初始化$U^0$和$V^0$会丢失很多知识，在文中，矩阵分解占比仅有1-5％，其余都是S的占比。如果在不增加UV的占比情况下提高他的知识，是否可以去除更多的S？考虑加权分解。先加权分解UV，在迭代剪枝。最后微调？</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>评估任务：</strong>natural language understanding (NLU)、question answering (QA)、natural language generation (NLG)</p><p><strong>压缩模型：</strong>DeBERTaV3-base、, BERT-base、BART-large</p><p><strong>Baselines:</strong> Full fine-tuning 、Movement pruning、Iterative pruning (ITP)</p><h3 id="Natural-Language-Understanding"><a href="#Natural-Language-Understanding" class="headerlink" title="Natural Language Understanding"></a>Natural Language Understanding</h3><p>在通用语言理解评估（GLUE）基准上修剪DeBERTaV3-base模型：</p><p><img src="/../images/summery1016_imgs/3.png"></p><p>修剪bert：</p><p><img src="/../images/summery1016_imgs/4.png"></p><h3 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h3><p>在SQuADv1.1上压缩了DeBERTaV3-base和BERT-base:</p><p><img src="/../images/summery1016_imgs/5.png"></p><h3 id="Natural-Language-Generation"><a href="#Natural-Language-Generation" class="headerlink" title="Natural Language Generation"></a>Natural Language Generation</h3><p>在XSum和CNN&#x2F;DailyMail数据集上压缩BART-large：</p><p><img src="/../images/summery1016_imgs/6.png"></p><h1 id="2-LoRAPrune"><a href="#2-LoRAPrune" class="headerlink" title="2 LoRAPrune"></a>2 LoRAPrune</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>PRUNING MEETS LOW-RANK PARAMETER-EFFICIENT FINE-TUNING</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.3</td></tr><tr><td>代码</td><td>无</td></tr><tr><td>压缩技术</td><td>剪枝 SOTA</td></tr></tbody></table><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/10.png"></p><p>插入lora，用lora的梯度代替权重的梯度。</p><p>使用一阶泰勒展开式来近似的重要性过于复杂，因此改为使用BA的梯度：</p><p><img src="/../images/summery1016_imgs/7.png"></p><p><img src="/../images/summery1016_imgs/8.png"></p><p>BA梯度仍然复杂，因此继续近似：</p><p><img src="/../images/summery1016_imgs/9.png"></p><p><img src="/../images/summery1016_imgs/44.png"></p><h2 id="i-1"><a href="#i-1" class="headerlink" title="i"></a>i</h2><p>用lora代替w进行计算。前百分之10与后百分之30只涉及参数更新。修剪过程采用“修剪-微调-修剪方法”。</p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><strong>任务</strong>：VTB-1k(19 个图像分类数据集)、FGVC(CUB-200-2011、NABirds、Oxford Flowers、Stanford Cars 和 Stanford Dogs)、GLUE(CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE 等任务)</p><p>对于CV任务采用ViT-B&#x2F;16模型，对于NLP任务采用 BERT-base模型（RTX3090）</p><p>对比：幅值修剪（MaP）、带LoRA的幅值修剪（MaP-LoRA）、运动修剪（MvP）、随机修剪（RaP）、参数高效稀疏性（PST）</p><p>结果（<strong>微调和剪枝过程结合在一起是高效</strong>）：</p><p><img src="/../images/summery1016_imgs/11.png"></p><p><img src="/../images/summery1016_imgs/12.png"></p><h1 id="3-PruneOFA"><a href="#3-PruneOFA" class="headerlink" title="3 PruneOFA"></a>3 PruneOFA</h1><h2 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Prune Once for All: Sparse Pre-Trained Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2021.11</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>非结构化剪枝、蒸馏</td></tr></tbody></table><h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><p>采用渐进幅度修剪：</p><p><img src="/../images/summery1016_imgs/13.png"></p><p>流程：</p><p><img src="/../images/summery1016_imgs/14.png"></p><p>提出模式锁的方法，它可以防止在训练模型时改变模型中发现的零</p><h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><p>模型：BERT-Base, BERT-Large and DistilBERT</p><p><img src="/../images/summery1016_imgs/15.png"></p><p><img src="/../images/summery1016_imgs/16.png"></p><p><img src="/../images/summery1016_imgs/17.png"></p><h1 id="4-oBert"><a href="#4-oBert" class="headerlink" title="4 oBert"></a>4 oBert</h1><h2 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2022.10</td></tr><tr><td>代码</td><td><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">sparseml&#x2F;research&#x2F;optimal_BERT_surgeon_oBERT at main · neuralmagic&#x2F;sparseml (github.com)</a></td></tr><tr><td>压缩技术</td><td>非结构化剪枝</td></tr></tbody></table><h2 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h2><p>采用二阶Hassian矩阵近似重要性进行剪枝：</p><p><img src="/../images/summery1016_imgs/18.png"></p><p>采用经验fisher矩阵近似hassian矩阵：</p><p><img src="/../images/summery1016_imgs/19.png"></p><h2 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h2><p>比较方法：Movement Pruning (MvP)、Lottery Ticket (LT-BERT)</p><p><img src="/../images/summery1016_imgs/20.png"></p><p><img src="/../images/summery1016_imgs/21.png"></p><h1 id="5-FLOP"><a href="#5-FLOP" class="headerlink" title="5 FLOP"></a>5 FLOP</h1><h2 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Structured Pruning of Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2021.5</td></tr><tr><td>代码</td><td><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">sparseml&#x2F;research&#x2F;optimal_BERT_surgeon_oBERT at main · neuralmagic&#x2F;sparseml (github.com)</a></td></tr><tr><td>压缩技术</td><td>低秩近似、结构化剪枝</td></tr></tbody></table><h3 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h3><p>端到端的训练方法，优化目标：</p><p><img src="/../images/summery1016_imgs/22.png"></p><p>重新参数化技巧：</p><p><img src="/../images/summery1016_imgs/23.png"></p><p>利用低秩因子分解的权重矩阵参数化方法：</p><p><img src="/../images/summery1016_imgs/24.png"></p><p>增强拉格朗日方法：</p><p><img src="/../images/summery1016_imgs/25.png"></p><p>优化的最终目标：</p><p><img src="/../images/summery1016_imgs/26.png"></p><h2 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/27.png"></p><h1 id="6-FWSVD"><a href="#6-FWSVD" class="headerlink" title="6 FWSVD"></a>6 FWSVD</h1><h2 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>LANGUAGE MODEL COMPRESSION WITH WEIGHTED LOW-RANK FACTORIZATION</th></tr></thead><tbody><tr><td>期刊</td><td>ICLR</td></tr><tr><td>发表时间</td><td>2022.6</td></tr><tr><td>代码</td><td><a href="https://github.com/RahulSChand/Weighted-low-rank-factorization-Pytorch">https://github.com/RahulSChand/Weighted-low-rank-factorization-Pytorch</a></td></tr><tr><td>压缩技术</td><td>加权分解</td></tr></tbody></table><h3 id="方法-5"><a href="#方法-5" class="headerlink" title="方法"></a>方法</h3><p>矩阵分解：</p><p><img src="/../images/summery1016_imgs/28.png"></p><p>Fisher信息：</p><p><img src="/../images/summery1016_imgs/29.png"></p><p>费雪加权低秩近似：</p><p><img src="/../images/summery1016_imgs/30.png"></p><h2 id="i-2"><a href="#i-2" class="headerlink" title="i"></a>i</h2><p>微调lora（保留增量梯度）-&gt;计算重要性-&gt;加权分解SVD-&gt;W&#x3D;BA+S剪枝（剪枝S，微调BA）</p><p>W每一行相同的权重，否则没有闭环解？如何改进。</p><p><strong>单词嵌入层(非负矩阵分解，考虑人脸识别和数字分解)</strong></p><h2 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h2><p>压缩路径：</p><p><img src="/../images/summery1016_imgs/31.png"></p><p><img src="/../images/summery1016_imgs/32.png"></p><h1 id="7-LLM-Pruner"><a href="#7-LLM-Pruner" class="headerlink" title="7 LLM-Pruner"></a>7 LLM-Pruner</h1><h2 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>LLM-Pruner: On the Structural Pruning of Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>NeurlPS</td></tr><tr><td>发表时间</td><td>2023.9</td></tr><tr><td>代码</td><td><a href="https://github.com/horseee/LLM-Pruner">https://github.com/horseee/LLM-Pruner</a></td></tr><tr><td>压缩技术</td><td>结构剪枝+LoRA微调</td></tr></tbody></table><h2 id="方法-6"><a href="#方法-6" class="headerlink" title="方法"></a>方法</h2><p>发现依赖结构-&gt;评估重要性(采用二阶导数)-&gt;微调恢复</p><h2 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h2><p>无baselines</p><p><img src="/../images/summery1016_imgs/33.png"></p><h1 id="8-SHEARED-LLAMA"><a href="#8-SHEARED-LLAMA" class="headerlink" title="8 SHEARED LLAMA"></a>8 SHEARED LLAMA</h1><h2 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.10</td></tr><tr><td>代码</td><td><a href="https://github.com/horseee/LLM-Pruner">https://github.com/horseee/LLM-Pruner</a></td></tr><tr><td>压缩技术</td><td>结构剪枝+动态批量加载</td></tr></tbody></table><h2 id="方法-7"><a href="#方法-7" class="headerlink" title="方法"></a>方法</h2><p>微调对于结构剪枝至关重要</p><p>端到端剪枝</p><p><img src="/../images/summery1016_imgs/34.png"></p><p>根据训练数据比例动态加载训练数据</p><h2 id="实验-7"><a href="#实验-7" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/35.png"></p><h1 id="9-ZipLM"><a href="#9-ZipLM" class="headerlink" title="9 ZipLM"></a>9 ZipLM</h1><h2 id="简介-8"><a href="#简介-8" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>ZipLM: Hardware-Aware Structured Pruning of Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.2</td></tr><tr><td>代码</td><td><a href="https://github.com/horseee/LLM-Pruner">https://github.com/horseee/LLM-Pruner</a></td></tr><tr><td>压缩技术</td><td>结构剪枝+知识蒸馏</td></tr></tbody></table><h2 id="方法-8"><a href="#方法-8" class="headerlink" title="方法"></a>方法</h2><p>一次剪枝一个结构来解决相关性问题。</p><p>权重更新，二阶导数近似：</p><p><img src="/../images/summery1016_imgs/36.png"></p><p>蒸馏：</p><p><img src="/../images/summery1016_imgs/37.png"></p><h2 id="实验-8"><a href="#实验-8" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/38.png"></p><h1 id="10-Matrix-Decomposition"><a href="#10-Matrix-Decomposition" class="headerlink" title="10 Matrix Decomposition"></a>10 Matrix Decomposition</h1><h2 id="简介-9"><a href="#简介-9" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Compressing Pre-trained Language Models by Matrix Decomposition</th></tr></thead><tbody><tr><td>期刊</td><td>AACl</td></tr><tr><td>发表时间</td><td>2021</td></tr><tr><td>代码</td><td><a href="https://github.com/kene111/matrix-decomposition">https://github.com/kene111/matrix-decomposition</a></td></tr><tr><td>压缩技术</td><td>低秩分解+蒸馏</td></tr></tbody></table><h2 id="方法-9"><a href="#方法-9" class="headerlink" title="方法"></a>方法</h2><ol><li><p>通过SVD分解</p></li><li><p>知识蒸馏，训练损失由三部分构成：<br>$$<br>L&#x3D;\alpha L_{CE}+(1-\alpha)L_{KD}+L_{FD}<br>$$<br><img src="/../images/summery1016_imgs/39.png"></p></li></ol><p><img src="/../images/summery1016_imgs/40.png"></p><p><img src="/../images/summery1016_imgs/41.png"></p><p><strong>基础模型(微调模型)用于分解和作为教师模型</strong></p><h2 id="实验-9"><a href="#实验-9" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/42.png"></p><h1 id="11-Movement-Pruning"><a href="#11-Movement-Pruning" class="headerlink" title="11 Movement Pruning"></a>11 Movement Pruning</h1><h2 id="简介-10"><a href="#简介-10" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Movement Pruning:Adaptive Sparsity by Fine-Tuning</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2020</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>移动剪枝</td></tr></tbody></table><h2 id="方法-10"><a href="#方法-10" class="headerlink" title="方法"></a>方法</h2><p>考虑到权重值在迁移学习中不是从头开始的，传统的基于幅度的剪枝可能无法充分适应新任务的需求，因为他们的修剪决策是基于原始模型的权重值。公式推导：</p><p><img src="/../images/summery1016_imgs/45.png"></p><p><img src="/../images/summery1016_imgs/46.png"></p><p><img src="/../images/summery1016_imgs/112.png"></p><p><img src="/../images/summery1016_imgs/113.png"></p><p>从公式可以看出，当W远离0点是，重要性S变大。于是修剪那些逐渐靠近0的权重而幅度修剪的方法修剪离0近的值。</p><p><img src="/../images/summery1016_imgs/114.png"></p><h2 id="实验-10"><a href="#实验-10" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/115.png"></p><h1 id="12-ITP"><a href="#12-ITP" class="headerlink" title="12 ITP"></a>12 ITP</h1><h2 id="简介-11"><a href="#简介-11" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Importance Estimation for Neural Network Pruning</th></tr></thead><tbody><tr><td>期刊</td><td>CVPR</td></tr><tr><td>发表时间</td><td>2019</td></tr><tr><td>代码</td><td><a href="https://github.com/NVlabs/Taylor_pruning">https://github.com/NVlabs/Taylor_pruning</a><em>.</em></td></tr><tr><td>压缩技术</td><td>迭代剪枝、一阶二阶近似</td></tr></tbody></table><h2 id="方法-11"><a href="#方法-11" class="headerlink" title="方法"></a>方法</h2><p>计算精确的重要性对于大型网络来说是及其昂贵的，然后采用一阶或二阶近似的方法评估重要性。</p><p><img src="/../images/summery1016_imgs/121.png" alt="image-20231026144556225"></p><p><img src="/../images/summery1016_imgs/122.png"></p><p><img src="/../images/summery1016_imgs/123.png"></p><p>以一个训练过的网络作为输入，并在一个具有较小学习率的迭代微调过程中对其进行剪枝。在每个时期内，都要重复以下步骤：</p><ol><li>对于每个小批处理，我们计算参数梯度，并通过梯度下降来更新网络权值。我们还计算了每个神经元（或滤波器）的重要性</li><li>在预定义的小批之后，我们将每个神经元（或过滤器）的重要性分数取为小批的平均值，并去除N个重要性分数最小的神经元</li></ol><h2 id="实验-11"><a href="#实验-11" class="headerlink" title="实验"></a>实验</h2><p>在LSTM上做的实验。</p><h1 id="13-HMD"><a href="#13-HMD" class="headerlink" title="13  HMD"></a>13  HMD</h1><h2 id="简介-12"><a href="#简介-12" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Rank and run-time aware compression of NLP Applications</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2020</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>混合低秩压缩</td></tr></tbody></table><h2 id="方法-12"><a href="#方法-12" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/131.png"></p><h2 id="实验-12"><a href="#实验-12" class="headerlink" title="实验"></a>实验</h2><h1 id="14-SPDF"><a href="#14-SPDF" class="headerlink" title="14 SPDF"></a>14 SPDF</h1><h2 id="简介-13"><a href="#简介-13" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>剪枝之后恢复剪枝</td></tr></tbody></table><h2 id="方法-13"><a href="#方法-13" class="headerlink" title="方法"></a>方法</h2><p>首先，我们预先训练一个稀疏GPT模型，以减少计算训练的流量。然后，在微调阶段，我们强化GPT模型，允许零权值学习并增加建模能力，以更准确地学习下游任务。采用随机修剪，均匀稀疏的方法</p><p>三个假设：</p><ol><li>在llm的训练前阶段，可以使用高程度的权重稀疏度，同时通过密集的微调来保持下游的精度。</li><li>稀疏预训练模型的性能与下游任务中数据集的大小和难度相关。</li><li>当我们增加语言模型的规模时，更大的模型在训练前变得更容易接受更高水平的稀疏性。</li></ol><p><img src="/../images/summery1016_imgs/141.png"></p><h2 id="实验-13"><a href="#实验-13" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/142.png"></p><h1 id="15-A-Fast-Post-Training-Pruning-Framework-forTransformers"><a href="#15-A-Fast-Post-Training-Pruning-Framework-forTransformers" class="headerlink" title="15 A Fast Post-Training Pruning Framework forTransformers"></a>15 A Fast Post-Training Pruning Framework forTransformers</h1><h2 id="简介-14"><a href="#简介-14" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>A Fast Post-Training Pruning Framework for Transformers</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/WoosukKwon/retraining-free-pruning">https://github.com/WoosukKwon/retraining-free-pruning</a></td></tr><tr><td>压缩技术</td><td>训练后采用20K数据三分钟剪枝</td></tr></tbody></table><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><ol><li>剪枝是降低变压器模型巨大推理成本的一种有效方法。然而，之前关于修剪变压器的工作需要重新训练模型</li><li>虽然结构化剪枝方法可以实现高压缩率和加速，但它们通常很难在实践中使用。其中一个原因是在修剪期间或修剪后额外训练的计算成本很高，另一个原因是剪枝管道的高度复杂性，其中每个剪枝阶段通常需要重写训练代码，并引入额外的超参数来进行调优。</li><li></li></ol><h2 id="方法-14"><a href="#方法-14" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/151.png"></p><p>定义问题：</p><p><img src="/../images/summery1016_imgs/152.png"></p><p>二阶近似：</p><p><img src="/../images/summery1016_imgs/153.png"></p><p>海森近似：</p><p><img src="/../images/summery1016_imgs/154.png"></p><p><strong>掩码搜索：</strong></p><p><img src="/../images/summery1016_imgs/155.png"></p><p><img src="/../images/summery1016_imgs/156.png"></p><p><strong>掩码重排</strong>：</p><p>虽然它简化了问题，但仅使用对角线假设可能无法找到最佳的解决方案，因为它没有考虑到不同掩模变量之间的相互作用。例如，如果在一个图层中有两个注意头发挥着相似的作用，那么只修剪其中一个可能不会影响模型的准确性。然而，当它们两者都被修剪时，模型的精度可能会显著降低。这种交互作用被费雪信息矩阵的非对角元素捕获，在前一阶段被忽略。因此，我们可以通过使用对Fisher矩阵的块对角近似来更好地考虑剪枝问题中的相互作用，其中一个块对应于一个MHA层或一个FFN层，如下图所示。</p><p><img src="/../images/summery1016_imgs/157.png"></p><p>用贪婪算法近似地解决这个问题。在将掩模初始化即热启动）后，为每一轮选择一个具有最高Fisher信息的修剪头（或过滤器），并与当前掩模中的一个未修剪头（或过滤器）交换：</p><p><img src="/../images/summery1016_imgs/158.png"></p><p><strong>掩码微调：</strong></p><p>非零变量被调优到任何真实值，这样修剪后的模型就可以恢复其精度。通过线性最小二乘法进行分层重建。我们调整掩模变量以使最小化层重构误差，。从第一层到最后一层，我们用修剪模型中剩余的头&#x2F;滤波器重建原始模型的输出激活。正式形式如下：</p><p><img src="/../images/summery1016_imgs/159.png"></p><h2 id="实验-14"><a href="#实验-14" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/1510.png"></p><h1 id="16-随机森林预测剪枝架构"><a href="#16-随机森林预测剪枝架构" class="headerlink" title="16 随机森林预测剪枝架构"></a>16 随机森林预测剪枝架构</h1><h2 id="简介-15"><a href="#简介-15" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>PRUNING LARGE LANGUAGE MODELS VIA ACCURACY PREDICTOR</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>建立预测器预测剪枝体系结构</td></tr></tbody></table><h2 id="笔记-1"><a href="#笔记-1" class="headerlink" title="笔记"></a>笔记</h2><ol><li>目前，llm压缩的一些工作主要集中在模型量化上</li><li>注意层的重要性远高于MLP：</li></ol><p><img src="/../images/summery1016_imgs/163.png"></p><h2 id="方法-15"><a href="#方法-15" class="headerlink" title="方法"></a>方法</h2><p>构建架构-精度对：</p><p><img src="/../images/summery1016_imgs/161.png"></p><p>权重重要性评估：</p><p><img src="/../images/summery1016_imgs/162.png"></p><p>建立随机森林模型。</p><p>QLoRA微调恢复。</p><h2 id="实验-15"><a href="#实验-15" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/164.png"></p><h1 id="17-Wanda"><a href="#17-Wanda" class="headerlink" title="17 Wanda"></a>17 Wanda</h1><h2 id="简介-16"><a href="#简介-16" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td><a href="https://github.com/locuslab/wanda">locuslab&#x2F;wanda: A simple and effective LLM pruning approach. (github.com)</a></td></tr><tr><td>压缩技术</td><td>一次剪枝，不需要再训练或重量更新</td></tr></tbody></table><h2 id="笔记-2"><a href="#笔记-2" class="headerlink" title="笔记"></a>笔记</h2><ol><li>由于最近在llm中出现的大幅度特征的观察，我们的方法在每个输出的基础上，将最小幅度的权重乘以相应的输入激活。</li><li>到目前为止，许多显著的进展都集中在模型量化上，这是一个将参数被量化为更低的位级表示的过程</li><li>幅度修剪（Han et al.，2015），一种成熟的修剪方法，即使具有相对较低的稀疏性水平，在llm上也会显著失败</li><li>一旦llm达到一定规模（实际中约6B参数），一小组隐藏状态特征就会比其余特征大得多。这些离群值特征表现出几个有趣的特征。首先，它们有非常大的大小，大约是典型的隐藏状态值的100倍。其次，它们通常是稀疏的，并且存在于某些特定的特征维度中。最后，这些离群值特征对于llm的预测能力是至关重要的：在推理时消除这些特征将导致语言建模性能的显著下降</li><li>与SparseGPT不同，我们的方法不需要对修剪过的网络进行权值更新，这表明llm具有有效的精确的稀疏子网络，而不是它们仅仅存在于原始权值的邻域中</li><li>选择正确的比较组对于剪枝大型语言模型（LLMs）非常重要，即使在传统的Magnitude剪枝方法中也是如此</li></ol><h2 id="方法-16"><a href="#方法-16" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/171.png"></p><p><strong>我们在每个输出的基础上（图中的每一行）上比较和删除权重，其中权重重要性分数在每个输出神经元中进行局部比较</strong></p><p>对比SparseGPT：</p><p><img src="/../images/summery1016_imgs/172.png"></p><p>令$\lambda&#x3D;0$：</p><p><img src="/../images/summery1016_imgs/173.png"></p><h2 id="实验-16"><a href="#实验-16" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/174.png"></p><p>校准数据量：</p><p><img src="/../images/summery1016_imgs/175.png"></p><p>不同分组结果：</p><p><img src="/../images/summery1016_imgs/176.png"></p><h1 id="18-SparseBERT"><a href="#18-SparseBERT" class="headerlink" title="18 SparseBERT"></a>18 SparseBERT</h1><h2 id="简介-17"><a href="#简介-17" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Rethinking Network Pruning— under the Pre-train and Fine-tune Paradigm</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2021</td></tr><tr><td>代码</td><td><a href="https://github.com/dongkuanx27/SparseBERT">https://github.com/dongkuanx27/SparseBERT</a></td></tr><tr><td>压缩技术</td><td>SparseBERT将在微调阶段执行。它在修剪的同时保留了通用的和特定于任务的语言知识、幅度剪枝</td></tr></tbody></table><h2 id="笔记-3"><a href="#笔记-3" class="headerlink" title="笔记"></a>笔记</h2><ol><li>通过研究知识在训练前、微调和修剪过程中如何传递和丢失来填补这一空白，并提出一个知识感知的稀疏剪枝过程</li><li>最近的研究结果表明，自我注意和前馈层是过度参数化的，是最多计算消耗的部分</li></ol><h2 id="方法-17"><a href="#方法-17" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/181.png"></p><p><img src="/../images/summery1016_imgs/182.png"></p><p>SparseBERT使用没有微调的预先训练的BERT作为初始化模型，修剪自注意和前馈层的线性变换.</p><p>为了在剪枝过程中学习特定于任务的任务知识，同时保留通用知识，我们应用了知识蒸馏：采用特定任务的微调BERT作为教师网络，采用预先训练的BERT作为学生。我们将下游任务数据输入师生框架，以训练学生再现教师的行为。</p><p>蒸馏损失：</p><p><img src="/../images/summery1016_imgs/183.png"></p><p>蒸馏与剪枝并行：</p><p><img src="/../images/summery1016_imgs/184.png"></p><h2 id="实验-17"><a href="#实验-17" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/185.png"></p><h1 id="19-Compresso"><a href="#19-Compresso" class="headerlink" title="19 Compresso"></a>19 Compresso</h1><h2 id="简介-18"><a href="#简介-18" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th><strong>Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models.</strong></th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td>(<a href="https://github.com/microsoft/Moonlit/tree/main/Compresso">https://github.com/microsoft/Moonlit/tree/main/Compresso</a>)</td></tr><tr><td>压缩技术</td><td>使用指令调优数据集、提示词、端到端剪枝</td></tr></tbody></table><h2 id="笔记-4"><a href="#笔记-4" class="headerlink" title="笔记"></a>笔记</h2><ol><li>LLM培训由于其庞大的模型规模，资源非常密集。其次，llm的训练数据集是广泛的，而且由于法律限制，往往不可用。直接使用开源数据集可能会导致分布外的问题，因为剪枝数据的分布与预训练前的数据分布有很大的不同</li><li>大多数最先进的修剪方法都涉及到一个训练过程来更新梯度，并利用它们来估计权重的重要性，然而，由于两个主要原因，这些方法不能直接应用于llm。首先，它们是需要下游训练数据集。因此，修剪后的模型不能保留在不同任务之间的泛化能力。其次，llm的修剪过程需要大量的训练资源。</li><li>尽管它的速度很快，但一次性修剪也有其局限性。首先，它在很大程度上依赖于预先预定义的权重重要性度量来修剪决策，因此在所有层之间采用均匀稀疏比，而不考虑每个层的不同冗余。其次，与基于训练的剪枝相比，剩余模型参数的误差恢复很有限，这可能会影响最终的性能。我们的压缩机解决了所有这些限制。</li><li>使用指令对llm进行微调已被证明可以提高性能和对不可见任务的泛化</li></ol><h2 id="方法-18"><a href="#方法-18" class="headerlink" title="方法"></a>方法</h2><p>为了解决基于训练的剪枝中高训练成本和数据收集的挑战，我们将低秩适应（LoRA）纳入L0正则化，并使用指令调优数据集作为训练数据的替代方案。具体来说，我们利用可学习的二进制掩码来决定是否保留或修剪每个子模块（即，头、FFN中间维度和隐藏维度）。</p><p>然后，在指令调整过程中，采用L0正则化方法优化掩模值，同时通过LoRA更新模型参数。此外，与一次性LLM修剪方法相比，通常采用跨所有层的均匀稀疏比，压缩机自动学习改进的层级稀疏比。</p><p><img src="/../images/summery1016_imgs/191.png"></p><p>数据集：建议使用指令调优数据集作为修剪数据。尽管它们的分布不同于训练前的数据集，但它们已经证明了在微调预训练和收敛的llm以符合人类意图方面的成功</p><p>高效的基于训练的结构化修剪：</p><p>基本思想是： (i)我们引入一组二进制掩模Z∈{0,1}来指示是删除（Z &#x3D; 0）还是保留（Z &#x3D; 1）每个掩模子模块，从而表示剩余的模型大小；（ii）我们冻结原始LLM，利用LoRA向LLM的秩分解矩阵注入LLM可训练的每一层。这大大减少了可训练参数的数量和所需的GPU内存；（iii）我们使用增强的L0正则化联合优化这些掩模值和LoRA模块（方法。这确保了修剪后的模型大小满足给定的约束条件。</p><p>剪枝形式（采用Cofi）：</p><p><img src="/../images/summery1016_imgs/192.png"></p><p>引入Lora：</p><p><img src="/../images/summery1016_imgs/193.png"></p><p>定义稀疏性函数(无需手动选择剪枝比例)：</p><p><img src="/../images/summery1016_imgs/194.png"></p><p>$L_0$重参数化：</p><p><img src="/../images/summery1016_imgs/195.png"></p><p>惩罚项：</p><p><img src="/../images/summery1016_imgs/196.png"></p><p>训练目标是下一个令牌预测损失和$L_{0{reg}}$损失的组合。</p><p>剪枝过程中，我们将提示符放在输入文本之前。根据指令调优的实践（Taori et al.，2023），我们不计算提示部分的下一代令牌生成损失。协作提示在两个阶段的修剪和推理阶段都被使用:</p><p><img src="/../images/summery1016_imgs/197.png"></p><h2 id="实验-18"><a href="#实验-18" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/198.png" alt="image-20231106160054281"></p><h1 id="20-Cofi"><a href="#20-Cofi" class="headerlink" title="20 Cofi"></a>20 Cofi</h1><h2 id="简介-19"><a href="#简介-19" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Structured Pruning Learns Compact and Accurate Models</th></tr></thead><tbody><tr><td>期刊</td><td>ACL</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/princeton-nlp/CoFiPruning">https://github.com/princeton-nlp/CoFiPruning</a></td></tr><tr><td>压缩技术</td><td>粗粒度和细粒度剪枝、分层精馏策略</td></tr></tbody></table><h2 id="笔记-5"><a href="#笔记-5" class="headerlink" title="笔记"></a>笔记</h2><ol><li>蒸馏方法需要大量的未标记数据，而且训练成本昂贵</li><li>经验证据表明，50%的层可以下降，而没有很大的精度下降，导致2×的加速。</li><li>FFN修剪的其他主要部分-前馈层（FFNs）-也被认为是过度参数化的</li><li>加速率是我们在整个论文中使用的一个主要度量方法，因为压缩率并不一定反映了推理延迟的实际改进</li><li>我们将这项工作的范围框架为针对特定任务的剪枝。我们希望未来的研究能够继续这一工作，因为与一般蒸馏相比，从大型预训练模型进行的修剪可能会导致更少的计算，并导致更灵活的模型结构</li></ol><h2 id="方法-19"><a href="#方法-19" class="headerlink" title="方法"></a>方法</h2><p><img src="/../images/summery1016_imgs/201.png"></p><p>目标稀疏：</p><p><img src="/../images/summery1016_imgs/202.png"></p><p>隐层蒸馏损失：</p><p><img src="/../images/summery1016_imgs/203.png"></p><p>动态映射关系：</p><p><img src="/../images/summery1016_imgs/204.png"></p><p>蒸馏损失：</p><p><img src="/../images/summery1016_imgs/205.png"></p><h2 id="实验-19"><a href="#实验-19" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/206.png"></p><h1 id="21-BIP"><a href="#21-BIP" class="headerlink" title="21 BIP"></a>21 BIP</h1><h2 id="简介-20"><a href="#简介-20" class="headerlink" title="简介"></a>简介</h2><table><thead><tr><th>名称</th><th>Advancing Model Pruning via Bi-level Optimization</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/OPTML-Group/BiP">https://github.com/OPTML-Group/BiP</a></td></tr><tr><td>压缩技术</td><td>剪枝-训练范式并行处理，双层优化</td></tr></tbody></table><h2 id="笔记-6"><a href="#笔记-6" class="headerlink" title="笔记"></a>笔记</h2><ol><li>正如彩票假说（LTH）所示，修剪也有提高其泛化能力的潜力</li><li>获得较高的剪枝模型精度（如IMP）和较高的计算效率（如一次性剪枝）</li><li>剪枝-再训练学习范式涵盖了两种任务：❶剪枝决定了模型权值的稀疏模式，以及❷训练保留非零权值来恢复模型的精度</li><li>底层和上层优化的数据批量选择：我们在实现（θ-step）和（m-step）时，采用不同的数据批量（具有相同的批量大小）。这是BLO公式的优点之一，它可以灵活地定制底层和上层问题</li><li>在m上的离散优化：我们遵循“凸松弛+硬阈值”机制。具体地说，我们将二进制掩蔽变量放宽为连续掩蔽分数m∈[0,1]。然后我们得到基于松弛m的后传递损失梯度</li></ol><h2 id="方法-20"><a href="#方法-20" class="headerlink" title="方法"></a>方法</h2><p>将剪枝任务（即❶）和模型再训练任务（即❷）解释为两个优化级别，其中前者被表述为上层优化问题，并依赖于低层次再训练任务的优化。因此，我们将模型剪枝问题转换为以下BLO问题（❷嵌套在❶中）：</p><p><img src="/../images/summery1016_imgs/207.png"></p><p>BLO可以灵活地在上层和下层的优化级别上分别使用不匹配的修剪和再训练目标。这种灵活性允许我们在(1)中规范底层训练目标函数，并在两个级别上定制已实现的优化方法。更具体地说，我们可以使用一个数据批处理（称为B2）来更新上层修剪掩码m，而不是不同于用于获取底层解决方案θ∗(m)的数据批处理（称为B1）。由此产生的BLO过程可以模拟元学习的想法来改进模型泛化[98]，其中低级问题使用B1对θ进行微调，而上层问题使用B2验证稀疏感知精细模型（m⊙θ∗(m)）的泛化。</p><p>在梯度下降的情况下，公式中的目标函数的梯度产生：</p><p><img src="/../images/summery1016_imgs/212.png"></p><p>IG：</p><p><img src="/../images/summery1016_imgs/213.png" alt="image-20231106190027013"></p><p>由于矩阵反演和二阶偏导数的存在，精确的IG公式(3)仍然难以计算。为了简化它，我们施加了无黑森假设，∇2θℓ&#x3D;0，它一般是温和的</p><p>近似：</p><p><img src="/../images/summery1016_imgs/214.png"></p><p>使用一阶近似：</p><p><img src="/../images/summery1016_imgs/215.png"></p><p>两个步骤：</p><ul><li><p>用于模型再训练的较低级别的SGD：</p><p><img src="/../images/summery1016_imgs/216.png" alt="image-20231106190355644"></p></li><li><p>用于修剪的上层SPGD：</p><p><img src="/../images/summery1016_imgs/217.png"></p></li></ul><p><img src="/../images/summery1016_imgs/218.png"></p><h2 id="实验-20"><a href="#实验-20" class="headerlink" title="实验"></a>实验</h2><p><img src="/../images/summery1016_imgs/219.png"></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PruneOnceforAll</title>
      <link href="/2023/10/09/PruneOnceforAll/"/>
      <url>/2023/10/09/PruneOnceforAll/</url>
      
        <content type="html"><![CDATA[<h1 id="Prune-Once-for-All"><a href="#Prune-Once-for-All" class="headerlink" title="Prune Once for All"></a>Prune Once for All</h1><p>Prune Once for All: Sparse Pre-Trained Language Models：</p><p>通过整合权重剪枝和模型蒸馏来训练稀疏的预训练Transformer语言模型。这些稀疏的预训练模型可以用于迁移学习，适用于各种任务，同时保持它们的稀疏模式。</p><h1 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h1><p>就准确性而言，BERT是在训练前阶段还是在迁移学习阶段被修剪并不重要。这表明，LM可以在训练前修剪一次，然后微调到任何下游任务，而不需要特定于任务的调整。</p><p>在本文中，提出了一种新的方法，为(Prune OFA），利用<strong>权重剪枝</strong>和<strong>模型蒸馏</strong>来产生预先训练的基于变压器的语言模型与高稀疏比。将方法应用于BERT-Base、BERT-Large和DistilBERT ，为这些模型架构生成稀疏的预训练模型。然后，展示了如何对这些稀疏模型进行微调，以生成四个特定任务的稀疏模型。文章还表明，可以使用量化感知训练进一步压缩模型，以在压缩-精度比方面实现最先进的结果。</p><p>本文方法不需要对每个任务的特殊剪枝超参数进行调整，因为本文对所有任务的模型进行一次修剪。</p><h1 id="Weight-pruning"><a href="#Weight-pruning" class="headerlink" title="Weight pruning"></a>Weight pruning</h1><p>在本文中，主要关注非结构化的权值剪枝。2018年提出了一种渐进幅度修剪（GMP）的方法，在训练过程中逐步以低幅度修剪权重。在训练过程中，每一个f步修剪最小幅度的权值，直到达到时间步$t$的时间稀疏比$s_t$，定义为：</p><p><img src="/../images/PruneOFA_imgs/1.png"></p><p>其中$s_i$和$s_f$是初始和最终的稀疏度比，$t_s$和$t_e$是剪枝的开始和结束的时间步长。</p><p>在最近的一篇论文中，提出了一种基于IMP（迭代幅度剪枝）和学习率退卷（LRR）的剪枝算法。IMP包括两个步骤：修剪模型的一部分，并继续对其进行微调，以从诱导的剪枝错误中恢复。重复这两个步骤，直到达到期望的稀疏度比。在LRR中，学习速率调度器被恢复到它在微调步骤开始时的剪枝步骤之前的状态。文中建议将学习速率重绕原理合并到GMP中，即每f步在时间$t_s$时重绕到其状态。在测试之后，调度程序继续执行其原始设置，直到训练结束。</p><h1 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h1><p>在本文中，作者提出了利用模型蒸馏的方法来进行剪枝过程。作者关注的方法是，教师和学生共享相同的架构，但他们的稀疏比不同。在这种情况下，教师是一个在目标任务上进行训练的密集模型，而学生是一个具有固定稀疏性或正在进行修剪的模型。在训练前和微调阶段，可以在蒸馏阶段应用于语言模型。在训练前阶段，教师是一个预先训练过的语言模型，而在微调阶段，教师是一个针对目标任务进行微调的语言模型。</p><h1 id="Prune-Once-for-All-1"><a href="#Prune-Once-for-All-1" class="headerlink" title="Prune Once for All"></a>Prune Once for All</h1><p>该方法由两个步骤组成，即教师模型准备和学生模型剪枝。我们训练的稀疏预训练模型是我们用于迁移学习的模型，同时保持其稀疏模式。我们将这个方法称为“一次剪枝，用于所有”，因为我们展示了如何仅对预训练模型进行一次剪枝，然后对稀疏的预训练模型进行多个语言任务的微调。</p><p><img src="/../images/PruneOFA_imgs/2.png" alt="Prune OFA method"></p><p><strong>Teacher preparation</strong></p><p>Prune OFA的第一步是获得一个在预训练任务中针对目标$L_{PT}$经过优化的模型，如图所示。相同的数据集将用于下一步对学生模型进行剪枝。这个模型将在学生模型剪枝步骤中初始化学生模型和教师模型。</p><p><strong>Student pruning</strong></p><p>学生模型是从在教师准备步骤中准备的教师模型中初始化的。然后，在来自教师准备步骤的预训练任务和知识蒸馏目标$L_{kd}$的线性组合上对学生模型进行微调：</p><p><img src="/../images/PruneOFA_imgs/3.png"></p><p>同时使用$GMP + LRR$方法进行修剪。该过程的输出模型是一个稀疏预训练的LM，可以在不需要额外修剪的情况下用于迁移学习，从而为特定的下游任务生成稀疏模型。</p><p>在学生模型的微调过程中，使用了两个不同的目标来进行训练，这两个目标是从教师准备步骤中获得的：</p><ol><li>第一个目标是来自教师准备步骤的预训练任务，也就是在准备教师模型时使用的任务。这个任务的知识和经验被用来指导学生模型的微调。</li><li>第二个目标是知识蒸馏目标，通常表示为$L_{kd}$。知识蒸馏是一种训练方法，其中学生模型试图模拟教师模型的输出，以获取其知识和泛化能力。这个目标也被用于微调学生模型。</li></ol><p>这两个目标的线性组合意味着它们以某种权重的加权方式结合在一起，用来微调学生模型。这个组合可以帮助学生模型在微调过程中综合利用来自不同目标的信息，以提高性能。</p><p><strong>Pattern-lock</strong></p><p>作者希望在微调过程中保持由Prune OFA创建的稀疏预训练模型的稀疏性模式。提出了一种称为模式锁的方法，它可以防止在训练模型时改变模型中发现的零。</p><p>方法：在训练之前，Pattern-lock方法会为每个稀疏层$l$初始化一个掩码$M^l$，该掩码代表了该层的稀疏模式，其权重为$W^l$:</p><p><img src="/../images/PruneOFA_imgs/4.png"></p><p>然后，在训练时，损失$L$梯度$w.r.t$修改为:</p><p><img src="/../images/PruneOFA_imgs/5.png"></p><p>确保最初为0的权重将保持在0到整个微调.</p><h1 id="Experimental"><a href="#Experimental" class="headerlink" title="Experimental"></a>Experimental</h1><p>作者通过在三种不同大小的架构上应用Prune OFA来展示他们的方法；BERT-Base，BERT-Barge和DistilBERT。由于没有用于训练BERT-Base、BERT-Large和DistilBERT的原始处理训练数据，作者运行一个额外的步骤，使用作者准备的处理训练数据来微调预先训练过的模型。接下来，作者执行学生修剪步骤来获得他们的稀疏预训练模型。将BERT-Base和DistilBERT修剪到{85%，90%}，将BERT-Large修剪到90%。修剪应用于变压器编码器中的所有线性层，包括池器层如果存在的话。</p><p><strong>idea</strong></p><p>给预训练模型进行剪枝，然后针对特定任务进行微调。如果微调的时候继续剪枝？</p><p>如果给预训练模型进行低秩近似剪枝，然后微调？</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>oBert</title>
      <link href="/2023/10/08/oBert/"/>
      <url>/2023/10/08/oBert/</url>
      
        <content type="html"><![CDATA[<h1 id="obert"><a href="#obert" class="headerlink" title="obert"></a>obert</h1><p><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">github.com</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>直接看结果：</p><p><img src="/../images/obert_imgs/1.png"></p><p>贡献：</p><ul><li>调研了lottery-ticket, movement pruning,magnitude and second-order pruning.</li><li>介绍了一种通用的二阶剪枝方法，称为最优BERT外科医生（oBERT），支持非结构化和块剪枝，是第一种既高精度又可扩展到BERT模型维数的二阶方法</li><li><strong>二阶剪枝方法需要逆黑森的近似，这对于LLM参数计数的存储和计算是昂贵的→未来如何近似逆海森矩阵？</strong></li></ul><h1 id="The-Optimal-BERT-Surgeon-oBERT"><a href="#The-Optimal-BERT-Surgeon-oBERT" class="headerlink" title="The Optimal BERT Surgeon (oBERT)"></a>The Optimal BERT Surgeon (oBERT)</h1><h2 id="Generalized-Second-Order-Block-Pruning"><a href="#Generalized-Second-Order-Block-Pruning" class="headerlink" title="Generalized Second-Order Block Pruning"></a>Generalized Second-Order Block Pruning</h2><p>令$W_M&#x3D;M\odot W^*$其中$W^*\in R^d$是一个密集模型的权重，$d$是全部权重，$M\in {0,1}^d$表示掩码，即剪枝，于是使用泰勒展开式得到：</p><p><img src="/../images/obert_imgs/2.png"></p><p>考虑到$W^*$优化良好，于是假定$\nabla L(W^*)\approx0$,通过修剪权值子集所引起的损失的变化可以表示为:</p><p><img src="/../images/obert_imgs/3.png"></p><p><strong>→→→→如果不近似呢?如何推导？</strong></p><p>其中：</p><p><img src="/../images/obert_imgs/4.png"><br>$$<br>\delta W :&#x3D;W_M-W^*<br>$$<br>在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵：</p><p><img src="/../images/obert_imgs/5.png"></p><p>$m$是用于近似黑森的梯度外积的数量</p><p><strong>推导：</strong></p><p>对于一个将输入向量$in\in n_{in}$映射到输出向量$o\in n_0$的网络：</p><p><img src="/../images/obert_imgs/6.png"></p><p>与训练集对应的均方误差定义为($P$是样本数，$t^{[k]}$是期望输出，$o^{[k]}$是实际输出)：</p><p><img src="/../images/obert_imgs/7.png"></p><p>关于$W$的一阶导数是($\frac{\partial E}{\partial W}&#x3D;\frac{\partial E}{\partial o}  \frac{\partial o}{\partial W}$)：</p><p><img src="/../images/obert_imgs/8.png"></p><p>海森矩阵是：</p><p><img src="/../images/obert_imgs/9.png"></p><p>考虑一个完全训练到$W*$的局部最小误差的网络,可以忽略$t^{[k]}-o^{[k]}$：</p><p><img src="/../images/obert_imgs/10.png"></p><p>如果输出网络只有一个输出，我们可以将导数的n维数据向量$X^{[k]}$定义为：</p><p><img src="/../images/obert_imgs/11.png"></p><p>于是海森矩阵可以写成:</p><p><img src="/../images/obert_imgs/12.png"></p><p>如果网络是多元输出，则$X\in n×n_o$:</p><p><img src="/../images/obert_imgs/13.png"></p><p>于是海森矩阵可以写成：</p><p><img src="/../images/obert_imgs/14.png"></p><p>以上表明，$H$是与梯度变量$X$相关的样本协方差矩阵。对于单个输出情况，可以通过依次添加连续的“分量”计算完整的海森矩阵:</p><p><img src="/../images/obert_imgs/15.png"></p><p>然后可以得到在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵。</p><p>回到剪枝问题，识别一个给定形状的权重Q块，通过零掩蔽去除将导致最小的损失增加。这将导致以下约束优化问题：</p><p><img src="/../images/obert_imgs/16.png"></p><p>一次剪枝一组权重Q，对于这个组中的权重，如果对其进行剪枝，则必须保证增量和原始值相同，如果不对其剪枝，则不用管它。</p><p>由拉格朗日乘数法得到权重更新：</p><p><img src="/../images/obert_imgs/17.png"></p><p>Q权重的重要性：</p><p><img src="/../images/obert_imgs/18.png"></p><h2 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h2><p>由于$\hat F^{-1}(W)$太难计算，文中使用了近似的方法。</p><h3 id="修剪最优的权重集"><a href="#修剪最优的权重集" class="headerlink" title="修剪最优的权重集"></a>修剪最优的权重集</h3><p>在实践中，评估每组Q的显著性得分$\rho_Q$，并修剪得分最低的$\frac{s×d}{|Q|}$组权重，其中$s\in (0,1]$表示稀疏度，$d$是全部权重。</p><h3 id="经验逆fisher矩阵计算"><a href="#经验逆fisher矩阵计算" class="headerlink" title="经验逆fisher矩阵计算"></a>经验逆fisher矩阵计算</h3><p>采用Woodbury&#x2F;Sherman-Morrison(WSM) inversion formula：<br>$$<br>(A+uv^T)^{-1}&#x3D;A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}<br>$$<br>得到：</p><p><img src="/../images/obert_imgs/19.png"></p><p><img src="/../images/obert_imgs/20.png"></p><h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>$N_B&#x3D;\frac{d}{B}$表示总的块数，第一步计算：</p><p><img src="/../images/obert_imgs/21.png"></p><p>第二步计算($\in R^{N_B}$)：</p><p><img src="/../images/obert_imgs/22.png"></p><p>第三步计算：</p><p><img src="/../images/obert_imgs/23.png"></p><h1 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h1><p>更新每组权重得分$\rho_Q$：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">scores<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">*</span> finv<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>更新$W^*$</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">obs_updates<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    <span class="token punctuation">.</span>mul<span class="token punctuation">(</span>        <span class="token punctuation">(</span>param<span class="token punctuation">.</span>data <span class="token operator">*</span> <span class="token punctuation">(</span>mask_diffs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>计算逆经验$Fisher$矩阵</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> g<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Updates empirical Fisher inverse with a new gradient    :param g: a collected gradient    """</span>    <span class="token comment"># if 'd / B' is not integer, pad with zeros for batch calculations</span>    <span class="token keyword">if</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">:</span>        g <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>            <span class="token punctuation">[</span>g<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B <span class="token operator">-</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>g<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>    <span class="token comment"># prepare grad for batch calculations</span>    g <span class="token operator">=</span> g<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks<span class="token punctuation">,</span> self<span class="token punctuation">.</span>B<span class="token punctuation">)</span>    <span class="token comment"># batched f_inv x g: (batch, B, B) x (batch, B) -> (batch, B)</span>    finv_g <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bij,bj->bi"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>f_inv<span class="token punctuation">,</span> g<span class="token punctuation">)</span>    <span class="token comment"># scalar denominator for each batch: (batch)</span>    alpha <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>m <span class="token operator">+</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bi,bi->b"</span><span class="token punctuation">,</span> g<span class="token punctuation">,</span> finv_g<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    finv_g <span class="token operator">/=</span> alpha    <span class="token comment"># update f_inv with new outer product: (batch, B) x (batch, B) -> (batch, B, B)</span>    self<span class="token punctuation">.</span>f_inv<span class="token punctuation">.</span>baddbmm_<span class="token punctuation">(</span>finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 二阶近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵导数</title>
      <link href="/2023/10/08/Hessian/"/>
      <url>/2023/10/08/Hessian/</url>
      
        <content type="html"><![CDATA[<h1 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h1><ul><li>一元函数：$f:R \rightarrow R$</li><li>多元函数：$f:R^n \rightarrow R$</li><li>向量函数：$f:R^n \rightarrow R^m$</li></ul><h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><p>导数针对一元函数$f:R \rightarrow R$，$f(x) \approx  f(x_0)+f^1(x_0)(x-x_0)$</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度针对多元函数$f:R^n \rightarrow R$，梯度是一个向量：$\nabla f&#x3D;\begin{bmatrix} \frac{\partial f}{\partial x_1} \ \frac{\partial f}{\partial x_2} \ \frac{\partial f}{\partial x_3}\end{bmatrix}$,也可以写作：函数相对于$\vec {x}$的梯度算子为$\nabla_x$。<br>$$<br>f{(\vec x)}\approx f(\vec{x_0})+\nabla f(\vec{x_0})\cdot (\vec x- \vec{x_0})<br>$$</p><h2 id="Jacobian-矩阵"><a href="#Jacobian-矩阵" class="headerlink" title="$Jacobian$矩阵"></a>$Jacobian$矩阵</h2><p>针对向量函数：$f:R^n \rightarrow R^m$</p><p><img src="/../images/hessian_imgs/1.png"></p><p>矩阵分量：<br>$$<br>\textbf{J}_{ij}&#x3D;\frac{\partial f_i}{\partial x_j}<br>$$<br>其他常用的符号：<br>$$<br>Df、\textbf{Df}、\textbf{J}_f(x_1,…,x_n),\frac{\partial(f_1,…,f_m)}{\partial (x_1,…,x_n)}<br>$$<br>近似：<br>$$<br>f{(\vec x)}\approx f(\vec{x_k})+J (\vec{x_k})\cdot (\vec x- \vec{x_k})<br>$$</p><h1 id="Hessian-矩阵"><a href="#Hessian-矩阵" class="headerlink" title="$Hessian$矩阵"></a>$Hessian$矩阵</h1><p>使用于：$f:R^n \rightarrow R$，是函数的二阶矩阵：</p><p><img src="/../images/hessian_imgs/2.png"></p><p>这是一个$n×n$的方阵，可以写成：<br>$$<br>\textbf{H}_{ij}&#x3D;\frac{\partial ^2f}{\partial x_i \partial x_j}<br>$$<br>近似：</p><p><img src="/../images/hessian_imgs/3.png"></p><p><img src="/../images/hessian_imgs/4.png"></p><p><img src="/../images/hessian_imgs/5.png"></p><h1 id="Fisher-矩阵"><a href="#Fisher-矩阵" class="headerlink" title="$Fisher$矩阵"></a>$Fisher$矩阵</h1><p>$Fisher \ information$:假设观察到的数据$X_1,X2,…,X_n$服从一个概率分布$f(X;\theta)$,$\theta$是目标参数，那么似然函数$likelihood$：</p><p><img src="/../images/hessian_imgs/6.png"></p><p>为了解开方程，需要$\log(likelihood)$的一阶导数为0，其一阶导数$Score\ function$：</p><p><img src="/../images/hessian_imgs/7.png"></p><p>那么$Fisher\ information$用$I(\theta)$表示，定义即$Score\ function$的二阶矩：</p><p><img src="/../images/hessian_imgs/8.png"></p><p>现证明$E[S(X;\theta)]&#x3D;0$：</p><p><img src="/../images/hessian_imgs/9.png"></p><p>从而得到：</p><p><img src="/../images/hessian_imgs/10.png"></p><p>于是得到$Fisher \ information$的第一条数学意义：用来估计$Maximum\ Likelihood \ Estimate$方程的方差。即收集到的数据越多，象征着得到的信息越多。</p><p>对于$\theta$有多大把握，可以围绕估计值的期望，根据模型评分的协方差定义一个不确定性度量：</p><p><img src="/../images/hessian_imgs/11.png"></p><p>上面评分函数的协方差即$Fisher $信息的定义，一般$\theta $ 是一个向量，即$Fisher $信息是以矩阵形式存在，称为$Fisher$信息矩阵$FIM$：</p><p><img src="/../images/hessian_imgs/12.png"></p><p>一般情况下似然函数是复杂的，很难计算期望值，因此可以使用经验分布来近似$F$中的期望值。它由训练数据$X&#x3D;{X_1,X_2,…,X_N}$给出，即：</p><p><img src="/../images/hessian_imgs/13.png"></p><h1 id="Fisher-和-Hessian"><a href="#Fisher-和-Hessian" class="headerlink" title="$Fisher$和$Hessian$"></a>$Fisher$和$Hessian$</h1><p>对数似然的负期望$Hessian$，等于$Fisher$信息矩阵。</p><p>对数似然的$Hessian$为：</p><p><img src="/../images/hessian_imgs/14.png"></p><p>期望：</p><p><img src="/../images/hessian_imgs/15.png"></p><p>因此：</p><p><img src="/../images/hessian_imgs/16.png"></p><p>费舍尔信息矩阵被定义为评分函数的协方差，它是一个曲率矩阵，可以理解为对数似然函数的黑森负期望。因此，F的直接应用，是在二阶优化方法中替换H </p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/question/26561604">(4 封私信) 费雪信息 (Fisher information) 的直观意义是什么？ - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/228099600">费舍尔信息矩阵及自然梯度法 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bilibili.com/video/BV17B4y1y7wX/?spm_id_from=333.788.recommend_more_video.-1&vd_source=50e29aacf23dfe7179edd1b5d8ece200">【TRPO系列讲解】（二）Hessian矩阵、Fisher信息矩阵、KL散度_哔哩哔哩_bilibili</a></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线代 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoraPrune</title>
      <link href="/2023/09/26/LoraPrune/"/>
      <url>/2023/09/26/LoraPrune/</url>
      
        <content type="html"><![CDATA[<h1 id="文章-LoRAPrune"><a href="#文章-LoRAPrune" class="headerlink" title="文章: $LoRAPrune$"></a>文章: $LoRAPrune$</h1><p>文章链接：<br>代码链接：</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>神经网络剪枝可以压缩模型，目前大多数方法依赖于计算参数的梯度，但是计算参数梯度开销很大，文章提出$LoRAPrune$方法，首先设计一个$PEFT$感知的剪枝准则，它利用低等级自适应（$LoRA$）的值和梯度，而不是预先训练的参数的梯度来进行重要性估计。然后，提出了一个迭代剪枝方法，以去除冗余参数，同时最大化$PEFT$的优势。</p><p>在各种任务上的实验结果表明，该方法取得了最先进的结果。例如，在$VTAB-1k$基准中，$LoRAPrune$仅利用了0.76%的可训练参数，显著优于幅度和运动剪枝方法，分别高出5.7%和4.3%。此外，该方法实现了与$PEFT$方法相当的性能，突出了其在提供高质量结果方面的有效性，同时受益于剪枝的优势。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>神经网络剪枝是一种流行的模型压缩技术，通过去除冗余参数，可以显著降低这些大型模型的规模和复杂性。大多数最先进的评估参数重要性的方法都需要参数的梯度。</p><p>莫尔查诺夫引入了一种技术，该技术使用泰勒展开来近似由剪枝引起的损失波动，并使用一阶项来评估参数的重要性。同样，Yu 开发了一种基于梯度显著性评分的方法来评估参数的重要性，Zhang 提出了灵敏度平滑作为计算参数重要性的方法。此外，剪枝过程经常被纳入作为迭代剪枝-再训练循环的一部分，以恢复模型的精度。但是微调和计算梯度代价是昂贵的。</p><p>参数高效调优方法：LoRA插入一组可训练的并行或串行的低秩矩阵。插入的低秩矩阵中的参数数仅为模型参数的1%左右。在下游任务的微调期间，原始参数被冻结（即不更新，不计算梯度），只有插入的低秩矩阵被更新以近似参数更新。由于LoRA只更新了少量的参数，与全参数微调方法相比，它的优化难度和计算需求显著降低。然而，PEFT通常需要冻结的预训练参数，而不计算它们的梯度，依赖于预训练参数的梯度的剪枝方法不能直接应用于这些大语言模型。</p><p>一个想法：是否可以利用LoRA的低秩矩阵的梯度来评估预训练参数的重要性。因此提出LoRAPrune：只使用LoRA的梯度。与下图中描述的梯度引导剪枝方法相比，LoRAPrune利用LoRA的梯度作为预先训练的参数梯度的近似值。因此，$LoRAPrune$实现了对lpm中的冻结参数进行修剪的目标。</p><p><img src="/../images/1.png" alt="两种剪枝方案"></p><p>上图将LoRAPrune（左）与现有的梯度引导剪枝方法（右）进行比较： (a) LoRAPrune通过在整个过程中只计算低秩矩阵，可以对大规模模型进行高效的调优和剪枝。(b)传统的剪枝方法需要从大尺度参数中获得梯度。颜色（红色）表示可训练参数，颜色（蓝色）表示冻结参数，颜色（黄色）表示渐变。</p><p><strong>本文贡献：</strong></p><ul><li>利用从模型的低秩分解中导出的梯度来近似估算预训练参数的重要性。通过这样做，它避免了直接计算所有参数的梯度的需求。</li><li>提出一种将PEFT与剪枝相结合的方法。与其他PEFT方法相比，LoRAPrune能够部署具有相似数量训练参数的轻量级预训练模型。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p><strong>Parameter-efficient tuning</strong>：参数高效微调</p><p><strong>Neural network pruning</strong>：神经网络剪枝。如何确定参数的重要性仍然需要进行研究</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h3 id="初步"><a href="#初步" class="headerlink" title="初步"></a>初步</h3><p>首先重新讨论了具有结构重参数化的参数高效自适应方法。为了有效地微调神经网络的参数，目标模块（如全连接层）可以以并行或顺序的方式插入一个LoRA到预先训练好的参数中。在训练过程中，预先训练的参数被冻结，不计算其梯度，而插入的LoRA是可训练的。</p><p><strong>Parallel low-rank adaption</strong>：给定两个低秩矩阵$A\in R^{r×k}$和$B\in R^{d×r}$，通过并行低秩自适应微调的目标模块的正向过程可以写为：</p><p><img src="/../images/2.png"></p><p>$W_0,z\in R^{n×k},x\in R^{n×d}$表示原始目标模块的权重、输出和输入。自适应后，新的权重$W$可以重新参数化为$W &#x3D; W_0 + BA$。</p><p><strong>Sequential low-rank adaption</strong>:目标模块在顺序低秩自适应中的前向过程可以写为:</p><p><img src="/../images/3.png"></p><p>自适应后，新的权重$W$可以重新参数化为$W &#x3D; (BA+E)W_0$。</p><h3 id="低秩梯度判据"><a href="#低秩梯度判据" class="headerlink" title="低秩梯度判据"></a>低秩梯度判据</h3><p>一个参数$w_{ij}∈W_0$的重要性可以通过去除它所引起的损失来量化。对于输入x和相应的标签y，$w_ij$的诱导误差可以被测量为有无参数的预测误差的平方差：</p><p><img src="/../images/4.png"></p><p>为每个参数计算成本很高。使用一阶泰勒展开式来近似重要性：</p><p><img src="/../images/5.png"></p><p>在LPM中获取$W_0$的梯度是困难的，因为它需要大量的计算能力和存储空间。在本文中，作者讨论了如何通过在下游任务自适应中插入可学习矩阵$A$和$B$来修剪预先训练好的参数$w0$。如上所述，$A$和$B$可以以平行或顺序的方式插入到预先训练好的模型中。因此，文章分别讨论了这两种情况下相应的剪枝方法。</p><p><strong>Pruning for parallel adapter</strong>：在并行的情况下，如果删除元素$w_{ij}∈W$，可以设置元素$（BA）_{ij} &#x3D;−w_{ij}$。等式（3）中每个参数的重要性可以重新表述如下：</p><p><img src="/../images/6.png"></p><p>利用$（BA）_{ij} &#x3D;−w_{ij}$的一阶泰勒展开式来近似等式(5)，参数$w_{ij}$的估计重要性可以表示为:<br><img src="/../images/7.png"></p><p>接下来只保存并使用两个低秩矩阵A和B的梯度来近似：</p><p><img src="/../images/8.png"></p><p><img src="/../images/9.png"></p><p><strong>在(8)中，是否可以改变系数？</strong></p><p>下所示，LoRA梯度准则只需要计算A和B的梯度，与预训练的总权值W0的梯度相比，节省了内存和计算量。</p><p><img src="/../images/10.png"></p><h3 id="LORA剪枝"><a href="#LORA剪枝" class="headerlink" title="LORA剪枝"></a>LORA剪枝</h3><p>使用移动平均值来评估参数的重要性。具体来说，第t次迭代时的参数重要性计算如下（它允许在模型训练的早期阶段就开始剪枝操作，而不必等待整个模型完全收敛，从而节省了时间和计算资源）：</p><p><img src="/../images/11.png"></p><p>插入一个二进制掩模$\beta∈{0,1}^{d×k}$作为参数，然后使用<strong>修剪-微调-修剪</strong>方法进行修剪。考虑到每个参数的重要性，在每次剪枝迭代中通过设置相应的掩码为1，并将其余参数设置为0来保留Top-k个重要参数。形式上，每一个修剪层的正向过程可以写成:</p><p><img src="/../images/12.png"></p><p>算法如下：</p><p><img src="/../images/13.png"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在3090GPU上运行实验，采用修剪-微调-修剪操作。证明了将微调和剪枝过程相结合具有一定的时间效率，且不会影响plm的性能。</p><h1 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h1>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2023/09/20/SVD/"/>
      <url>/2023/09/20/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h1><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>奇异值分解$(Value Decomposition$，简称$SVD)$是一种在线性代数和矩阵分析中非常重要的数学技术，它可以将一个矩阵分解为三个矩阵的乘积，具体来说，将一个矩阵$A$分解为以下形式：<br>$$<br>A &#x3D; UΣV^T<br>$$<br>$SVD$的关键性质和应用包括：</p><ol><li>数据降维：$SVD$可用于将高维数据降维到低维，通过保留最重要的奇异值和对应的奇异向量，可以实现数据压缩和特征选择。</li><li>矩阵逆：$SVD$可用于计算矩阵的伪逆，对于非方阵或奇异矩阵尤其有用。</li><li>奇异值阈值截断：通过保留前k个最大的奇异值和相应的奇异向量，可以实现矩阵的低秩近似，用于图像压缩、推荐系统等。</li><li>主成分分析$(PCA)$：$SVD$可以用于$PCA$，通过对数据协方差矩阵进行$SVD$分解，可以找到数据的主成分。</li><li>推荐系统：$SVD$在协同过滤中有广泛应用，用于推荐用户可能感兴趣的物品。</li><li>图像压缩：$SVD$可用于图像压缩和去噪，通过保留奇异值较大的部分，可以减小图像尺寸并去除一些噪声。</li></ol><h3 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2 矩阵"></a>2 矩阵</h3><p>矩阵的意义：<a href="https://www.cnblogs.com/marsggbo/p/10144060.html">【转载】理解矩阵（三） - marsggbo - 博客园 (cnblogs.com)</a></p><p>以$Ma&#x3D;b$为例介绍矩阵$M$的含义：</p><ul><li>从变换的角度来说，矩阵$M$可以理解为对向量$ a$做变换得到了 $b$。</li><li>坐标系的角度来说，$M$可以理解成是一个坐标系（常用的坐标是笛卡尔坐标系，即 $I$），向量$a$就是在$M$这个坐标系下的坐标，$a$对应到$I$坐标系下的坐标是向量 $b$。</li></ul><h3 id="2-特征值分解"><a href="#2-特征值分解" class="headerlink" title="2 特征值分解"></a>2 特征值分解</h3><p><a href="https://blog.csdn.net/zpalyq110/article/details/86751064">奇异值分解(SVD)原理及实例分析_Freeman_zxp的博客-CSDN博客</a></p><h3 id="2-1-特征值和特征向量"><a href="#2-1-特征值和特征向量" class="headerlink" title="2.1 特征值和特征向量"></a>2.1 特征值和特征向量</h3><h5 id="特征值（Eigenvalues）："><a href="#特征值（Eigenvalues）：" class="headerlink" title="特征值（Eigenvalues）："></a>特征值（Eigenvalues）：</h5><ol><li><p>特征值是一个矩阵的标量性质，通常用λ表示。</p></li><li><p>特征值告诉我们矩阵在某个方向上的缩放因子或拉伸因子。</p></li><li><p>特征值的数目等于矩阵的维度。</p></li><li><p>特征值可以是实数或复数。</p></li></ol><p><strong>对于2：</strong>考虑一个二维平面上的线性变换，由一个矩阵A表示。我们有一个单位向量v（长度为1），它表示一个在该平面上的方向。当我们将这个向量v乘以矩阵A时，我们得到另一个向量Av。如果Av与v的方向相同（可能只是相反方向），那么这意味着矩阵A并没有改变该方向，只是对向量进行了缩放或拉伸。特征值就是用来表示这个缩放或拉伸的因子。具体来说，如果λ是矩阵A的一个特征值，而v是对应的特征向量，那么当我们将向量v乘以矩阵A时，结果是λv。这意味着矩阵A对向量v的作用只是将它缩放为原来的长度的λ倍。如果λ大于1，那么矩阵A在v的方向上对向量进行了拉伸；如果0 &lt; λ &lt; 1，那么矩阵A在v的方向上对向量进行了压缩；如果λ为负数，那么矩阵A对向量进行了反转，并改变了它的方向。所以，特征值λ告诉我们在特定方向v上矩阵A的作用是如何改变向量的大小或方向的。</p><h5 id="特征向量（Eigenvectors）："><a href="#特征向量（Eigenvectors）：" class="headerlink" title="特征向量（Eigenvectors）："></a>特征向量（Eigenvectors）：</h5><ol><li><p>特征向量是与特征值相关联的向量，通常用v表示。</p></li><li><p>特征向量表示在矩阵变换下不改变方向的向量。</p></li><li><p>特征向量描述了矩阵的变换性质，即它们定义了矩阵的主要方向。</p></li><li><p>特征向量通常标准化为单位向量。</p></li></ol><h3 id="2-2-特征值分解"><a href="#2-2-特征值分解" class="headerlink" title="2.2 特征值分解"></a>2.2 特征值分解</h3><p>特征值分解的实质是求解给定矩阵的特征值和 特征向盘，提取出矩阵最重要的特征。<br>既然我们知道一个矩阵是可以通过特征值和特征向量来表示，那假设存在一个$n×n$的满秩对称矩阵$A$，我们便可以通过特征值将$A$分解。首先求出$A$的$n$个特征值：$\lambda_1,\lambda_2,…,\lambda_n$,以及对应的特征向量(标准化处理后的):$x_1,x_2,…,x_n$。于是：<br>$$<br>Ax_1&#x3D;\lambda_1x_1 \\Ax_2&#x3D;\lambda_2x_2 \\……\\Ax_n&#x3D;\lambda_nx_n<br>$$<br>令$U&#x3D;[x_1,x_2,…,x_n]$,$\Lambda&#x3D;\begin{bmatrix} \lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_n \end{bmatrix}$化简公式为：$AU&#x3D;U\Lambda$。$U$是正交阵，有$U^T&#x3D;U^{-1}$。最终：<br>$$<br>A&#x3D;U\Lambda U^{-1}&#x3D;U\Lambda U^T<br>$$</p><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3 奇异值分解"></a>3 奇异值分解</h3><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p>对于满秩对称矩阵，可以简单的通过计算特征值进行分解，对于$m×n$的一般矩阵，需要使用奇异值分解$SVD$。已知对于任意矩阵都满足$A^TA,AA^T$，为对称矩阵，因此可以对$A^TA,AA^T$进行分解。</p><p>定义矩阵$A$的$SVD$为：<br>$$<br>A&#x3D;U\Sigma V^T<br>$$<br>其中$U$是$m×m$的矩阵，$\Sigma$是$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值。$V$是$m×m$的矩阵,$U$和$V$都是酉矩阵，满足$U^TU&#x3D;I,V^TV&#x3D;I$。</p><p><strong>计算：</strong></p><p>首先得到$n×n$的方阵$A^TA$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(A^TA)v_i&#x3D;\lambda_iv_i<br>$$<br>这样我们就可以得到矩阵 $A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将  $A^TA$ 的所有特征向量张成一个$n×n$的矩阵$V$,即$SVD$公式中的矩阵$V$,一般我们将$V$中的每个特征向量叫做A的右奇异向量。</p><p>然后得到$m×m$的方阵$AA^T$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(AA^T)u_i&#x3D;\lambda_iu_i<br>$$<br>这样我们就可以得到矩阵 $AA^T$的$n$个特征值和对应的$n$个特征向量$u$了。将  $AA^T$ 的所有特征向量张成一个$m×m$的矩阵$U$,即$SVD$公式中的矩阵$U$,一般我们将$U$中的每个特征向量叫做A的左奇异向量。</p><p>对于$\Sigma$,除了对角线上是奇异值其他位置都是0,因此只需要求出每个奇异值$\sigma$即可。注意到：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow AV&#x3D;U\Sigma V^TV\Rightarrow AV&#x3D;U\Sigma\Rightarrow Av_i&#x3D;\sigma_iu_i\Rightarrow \sigma_i&#x3D;Av_i&#x2F;u_i<br>$$<br>可以求出奇异值矩阵$\Sigma$。</p><p>证明$A^TA$的特征向量组成的就是$SVD$中的$V$矩阵：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow A^T&#x3D;V\Sigma U^T\Rightarrow A^TA&#x3D;V\Sigma U^TU\Sigma V^T&#x3D;V\Sigma^2V^T\ (U^TU&#x3D;I)<br>$$<br>进一步可以看到特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：<br>$$<br>\sigma_i&#x3D;\sqrt{\lambda_i}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoSparse</title>
      <link href="/2023/09/19/LoSparse/"/>
      <url>/2023/09/19/LoSparse/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1950343972&auto=1&height=66"></iframe><h1 id="文章题目：-LoSparse"><a href="#文章题目：-LoSparse" class="headerlink" title="文章题目：$LoSparse$"></a>文章题目：$LoSparse$</h1><p><strong>文章链接：</strong><a href="https://arxiv.org/pdf/2306.11222.pdf">https://arxiv.org/pdf/2306.11222.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/yxli2123/LoSparse">https://github.com/yxli2123/LoSparse</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>$transform$模型需要大量的计算资源，出于模型压缩的目的，作者提出了$LoSparse(Low-Rank and Sparse approximation)$模型，它通过一个<strong>低秩矩阵</strong>和一个<strong>稀疏矩阵</strong>的和来逼近一个权重矩阵。该方法结合了低秩近似和剪枝的优点，同时避免了它们的限制。剪枝增强了低秩近似的多样性，低秩近似可以防止剪枝损失太多的表达神经元</p><ul><li>低秩近似：压缩了神经元中的一致和表达丰富的部分</li><li>剪枝：去除了神经元中的不一致和非表达丰富的部分</li></ul><p>低秩可以压缩权重中相关性大的部分，但是完全忽略了相关性较小的部分（这些相关性小的部分在神经元之间可能是独特的，具有表达能力，并且对于模型的性能来说可能非常关键），结合剪枝，可以继续减少相关性小的那部分但是不至于全部去除导致网络多样性下降。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>目前模型压缩常用的是剪枝,分为结构性剪枝和非结构性剪枝。结构性剪枝一种方式为ITP，即迭代剪枝，可以同时训练和剪枝，本文用到的剪枝方法就是迭代剪枝。</p><p>为什么只用剪枝不行：</p><p><img src="/../images/image-20230918230903444.png" alt="神经元重要性得分直方图"></p><p>(b)为理想情况，大多数神经元是冗余的，对于网络来说不重要，只有小部分神经元是重要的，但是现实情况为(a)，很大一部分神经元是可表达的，因此大量剪枝可能导致重要的神经元会被减去。</p><p>低秩近似可以提取相关性大的权重的公共基。但是$transformer$模型的秩很高，它们包含许多参数，这使得简单地应用低秩近似来压缩这些矩阵可能会损害模型的性能。这是因为这种情况下忽略了神经元的多样性。</p><p>因此提出了低秩近似和稀疏近似的结合：低秩近似防止了修剪过度去除表达神经元，而稀疏近似增强了低秩近似的多样性。</p><p>同时该方法与知识蒸馏是正交关系，很容易再集成知识蒸馏手段，提高模型性能。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>介绍了transform模型和权重敏感性评分公式：</p><p><img src="/../images/image-20230918232543247.png"></p><p>为了减少随机抽样导致的可变性，本文使用平滑公式:</p><p><img src="/../images/image-20230918232355166.png"></p><p>结构性剪枝神经元的重要性评分公式：</p><p><img src="/../images/image-20230918232819344.png"></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>通过一个低秩矩阵和一个稀疏矩阵的和来近似一个权矩阵：</p><p><img src="/../images/image-20230918233005983.png" alt="LoSparse 在单个线性投影矩阵的示意图"></p><h3 id="低秩矩阵和稀疏矩阵的近似"><a href="#低秩矩阵和稀疏矩阵的近似" class="headerlink" title="低秩矩阵和稀疏矩阵的近似"></a>低秩矩阵和稀疏矩阵的近似</h3><p>给定一个权重矩阵$W\in R^{d_1×d_2}$,通常采用结构化剪枝稀疏矩阵$S\in R^{d_1×d_2}$来近似$W$以进行压缩。然而稀疏矩阵近似导致性能不佳，尤其是当压缩比率较高时。因此，本文引入了一个低秩矩阵来改进近似。具体来说，权重矩阵可以表示为：</p><p><img src="/../images/image-20230918233726382.png"></p><p>其中$U\in R^{d_1×d_2}$和$R\in R^{d_1×d_2}$的乘积表示秩为$r$的低秩矩阵。</p><p><strong>为什么需要低秩矩阵？</strong></p><p><img src="/../images/image-20230918234337880.png" alt="语言模型的奇异值"></p><p>首先，<strong>它可以有效地逼近神经元的相干部分</strong>，我们可以看到语言模型中权重矩阵的频谱在开始时迅速下降。<strong>频谱下降反映模型中特征之间的相关性。较大的特征值通常对应于具有更高相关性的特征。因此，快速下降的频谱表示模型中存在一些高度相关的特征，即一致部分</strong>，因此可以通过低秩来压缩公共部分。</p><p>其次<strong>低秩矩阵和稀疏矩阵的解耦使得剪枝变得容易</strong>。<strong>频谱的趋于平稳表示模型的权重矩阵中的特征或模式已经相对稳定地学习和表示。这也可能表明在训练过程中，神经元之间的相关性逐渐减弱，即神经元对不同的特征或模式变得更加独立</strong>而低秩矩阵无法捕获这些信息，但是低秩矩阵能够将相干部分与神经元的非相干部分解耦。因此可以添加一个矩阵$S$来近似剩余的不连贯部分，然后修剪非表达不连贯的部分。</p><p><img src="/../images/image-20230918230903444.png" alt="线性投影的神经元的重要性得分分布情况"></p><p>上图表示，大多数不连贯的部分在解耦之后具有较低的重要性分数，因此可用剪枝删除冗余参数。LoSparse算法成功地分离了神经元中不连贯的部分，并简化了非表达成分的修剪。(和ideal的图逼近)。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>给定一个预训练的权重矩阵$W^{(0)}$，首先基于$W^{(0)}$的奇异值分解（SVD）初始化秩 $r$的低秩矩阵。具体来说，本文选择：</p><p><img src="/../images/image-20230919000424683.png"></p><p>$u_1,u_2,…,u_r\in R^{d_1}$是左奇异向量，$v_1,v_2,…,v_r\in R^{d_1}$是左奇异向量，关于上面的$r$个奇异值$\sigma_1 \geq \sigma_2 \geq …\geq\sigma_3$在$W^{(0)}$的SVD中。</p><p>因此初始化$S^{(0)}$为：</p><p><img src="/../images/image-20230919001321431.png"></p><p>因此，原始的前向传递（$Y&#x3D;XW$）可替换为更高效的形式:</p><p><img src="/../images/image-20230919001529751.png"></p><p>作者将上面这种分解应用到了每一个权重矩阵并且将$S&#x3D;{S_m}_{m&#x3D;1}^{M}$表示为所有稀疏矩阵的集合。初始化$S$之后，对所有$S$进行迭代剪枝。</p><p><strong>具体方法：</strong></p><p>在第t次迭代时，首先采取随机梯度下降步骤来更新$U^{(t)}$,$V^{(t)}$和$S^{(t)}$。对于$S^{(t)}$，</p><p><img src="/../images/1695089652536.jpg"></p><p>这是损失函数 $L$ 对于变量 $S^{(t)}$的梯度。梯度是一个向量，它包含了损失函数相对于每个分量（或维度）的偏导数。</p><p>然后在公式(4)的基础上对$S^{(t)}$进行重要性评分，$\widetilde{S}^{(t)}$剪枝公式如下：</p><p><img src="/../images/1111.png"></p><p>$\tau( {\widetilde{S}^{(t)}},\Gamma({S}^{(t)}))$的第$i$列定义如下：</p><p><img src="/../images/1695090856116.jpg"></p><p>在每个迭代中，选择性地保留了权重矩阵 $S^{(t)}$中具有高重要性分数的一部分神经元，而剔除了贡献较低的神经元。这个策略通过逐渐减小 $p_t$ 的值来控制保留的神经元数量，以达到对模型进行剪枝$（pruning）$或精简的目的,$p_t$减小公式如下：</p><p><img src="/../images/1695091453574.jpg"></p><p>具体算法如下：</p><p><img src="/../images/1695091607438.jpg" alt="算法"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在自然语言理解、问答任务、自然语言生成任务与其他模型做了对比，显示出模型的优越性。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h5 id="稀疏逼近的有效性"><a href="#稀疏逼近的有效性" class="headerlink" title="稀疏逼近的有效性"></a>稀疏逼近的有效性</h5><p>作者将LoSparse与两种变体进行比较:(1)丢弃稀疏矩阵$S$，只微调低秩矩阵$UV$$(Low-rank \ I)$；(2)遵循(8)的初始化，但逐渐将初始化的$S$修剪为零$(Low-rank  \ II)$。</p><p><img src="/../images/1695092292904.jpg" alt="不同任务比较结果"></p><p>从图中可见，$(Low-rank  \ II)$的性能要比$(Low-rank \ I)$好得多。也就是说，修剪掉所有的稀疏矩阵比微调一个由奇异值阈值得到的低秩矩阵更有效，因此$LoSeparse$#可以增强低秩近似。</p><p><strong>解释：</strong>这是因为$Low-rank  \ I$的初始化与预训练的权重不同，因此它可能会从预训练的权重中丢失太多的知识。因此，下游任务的性能会严重下降。另一方面，$LoSeparse$弥补了低秩初始化与预训练权值之间的差距，从而保留了存储在预训练权值中的原始知识。这表明，尽管单独的低秩近似更有效和简洁，但我们应该利用稀疏近似来指导其训练过程。</p><h5 id="稀疏分配"><a href="#稀疏分配" class="headerlink" title="稀疏分配"></a>稀疏分配</h5><p>主要探究低秩近似和稀疏近似如何互相分配，主要方法是给定一个固定的剩余比例，改变低秩矩阵的比例，并相应改变稀疏矩阵的比例。不同分配条件下的结果如下图，可以看到低秩近似和稀疏近似对NLU任务的性能贡献几乎相等，因为当改变分配时性能保持稳定。</p><p><img src="/../images/786cac11a7d27a4dadbb82d0082e95a.png" alt="关于稀疏分配的结果"></p><h3 id="结合知识蒸馏"><a href="#结合知识蒸馏" class="headerlink" title="结合知识蒸馏"></a>结合知识蒸馏</h3><p>本文选择了一个针对特定任务进行微调的$DeBertaV3-base$模型作为教师模型，一个压缩的$DeBertaV3-base$模型作为学生模型。结果发现知识蒸馏可以进一步提高$LoSeparse$的性能,</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>文章提出了一种结合低秩近似和结构化稀疏近似的$transfomer$模型压缩方法$LoSparse$。在自然语言理解、问题回答和自然语言生成方面的实验表明，作者的方法明显优于以前的压缩方法。此外，其在自然语言生成任务和极高稀疏度的设置中特别有效。实验证明了$Losparse$是通用的，与其他流行的压缩方法是互补的。实验表明，$LoSparse$算法可以提高$CoFi$算法和传统的知识蒸馏迭代剪枝算法的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
