<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>oBert</title>
      <link href="/2023/10/08/oBert/"/>
      <url>/2023/10/08/oBert/</url>
      
        <content type="html"><![CDATA[<h1 id="obert"><a href="#obert" class="headerlink" title="obert"></a>obert</h1><p><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">github.com</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>直接看结果：</p><p><img src="/../images/obert_imgs/1.png"></p><p>贡献：</p><ul><li>调研了lottery-ticket, movement pruning,magnitude and second-order pruning.</li><li>介绍了一种通用的二阶剪枝方法，称为最优BERT外科医生（oBERT），支持非结构化和块剪枝，是第一种既高精度又可扩展到BERT模型维数的二阶方法</li><li><strong>二阶剪枝方法需要逆黑森的近似，这对于LLM参数计数的存储和计算是昂贵的→未来如何近似逆海森矩阵？</strong></li></ul><h1 id="The-Optimal-BERT-Surgeon-oBERT"><a href="#The-Optimal-BERT-Surgeon-oBERT" class="headerlink" title="The Optimal BERT Surgeon (oBERT)"></a>The Optimal BERT Surgeon (oBERT)</h1><h2 id="Generalized-Second-Order-Block-Pruning"><a href="#Generalized-Second-Order-Block-Pruning" class="headerlink" title="Generalized Second-Order Block Pruning"></a>Generalized Second-Order Block Pruning</h2><p>令$W_M&#x3D;M\odot W^*$其中$W^*\in R^d$是一个密集模型的权重，$d$是全部权重，$M\in {0,1}^d$表示掩码，即剪枝，于是使用泰勒展开式得到：</p><p><img src="/../images/obert_imgs/2.png"></p><p>考虑到$W^*$优化良好，于是假定$\nabla L(W^*)\approx0$,通过修剪权值子集所引起的损失的变化可以表示为:</p><p><img src="/../images/obert_imgs/3.png"></p><p><strong>→→→→如果不近似呢?如何推导？</strong></p><p>其中：</p><p><img src="/../images/obert_imgs/4.png"><br>$$<br>\delta W :&#x3D;W_M-W^*<br>$$<br>在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵：</p><p><img src="/../images/obert_imgs/5.png"></p><p>$m$是用于近似黑森的梯度外积的数量</p><p><strong>推导：</strong></p><p>对于一个将输入向量$in\in n_{in}$映射到输出向量$o\in n_0$的网络：</p><p><img src="/../images/obert_imgs/6.png"></p><p>与训练集对应的均方误差定义为($P$是样本数，$t^{[k]}$是期望输出，$o^{[k]}$是实际输出)：</p><p><img src="/../images/obert_imgs/7.png"></p><p>关于$W$的一阶导数是($\frac{\partial E}{\partial W}&#x3D;\frac{\partial E}{\partial o}  \frac{\partial o}{\partial W}$)：</p><p><img src="/../images/obert_imgs/8.png"></p><p>海森矩阵是：</p><p><img src="/../images/obert_imgs/9.png"></p><p>考虑一个完全训练到$W*$的局部最小误差的网络,可以忽略$t^{[k]}-o^{[k]}$：</p><p><img src="/../images/obert_imgs/10.png"></p><p>如果输出网络只有一个输出，我们可以将导数的n维数据向量$X^{[k]}$定义为：</p><p><img src="/../images/obert_imgs/11.png"></p><p>于是海森矩阵可以写成:</p><p><img src="/../images/obert_imgs/12.png"></p><p>如果网络是多元输出，则$X\in n×n_o$:</p><p><img src="/../images/obert_imgs/13.png"></p><p>于是海森矩阵可以写成：</p><p><img src="/../images/obert_imgs/14.png"></p><p>以上表明，$H$是与梯度变量$X$相关的样本协方差矩阵。对于单个输出情况，可以通过依次添加连续的“分量”计算完整的海森矩阵:</p><p><img src="/../images/obert_imgs/15.png"></p><p>然后可以得到在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵。</p><p>回到剪枝问题，识别一个给定形状的权重Q块，通过零掩蔽去除将导致最小的损失增加。这将导致以下约束优化问题：</p><p><img src="/../images/obert_imgs/16.png"></p><p>一次剪枝一组权重Q，对于这个组中的权重，如果对其进行剪枝，则必须保证增量和原始值相同，如果不对其剪枝，则不用管它。</p><p>由拉格朗日乘数法得到权重更新：</p><p><img src="/../images/obert_imgs/17.png"></p><p>Q权重的重要性：</p><p><img src="/../images/obert_imgs/18.png"></p><h2 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h2><p>由于$\hat F^{-1}(W)$太难计算，文中使用了近似的方法。</p><h3 id="修剪最优的权重集"><a href="#修剪最优的权重集" class="headerlink" title="修剪最优的权重集"></a>修剪最优的权重集</h3><p>在实践中，评估每组Q的显著性得分$\rho_Q$，并修剪得分最低的$\frac{s×d}{|Q|}$组权重，其中$s\in (0,1]$表示稀疏度，$d$是全部权重。</p><h3 id="经验逆fisher矩阵计算"><a href="#经验逆fisher矩阵计算" class="headerlink" title="经验逆fisher矩阵计算"></a>经验逆fisher矩阵计算</h3><p>采用Woodbury&#x2F;Sherman-Morrison(WSM) inversion formula：<br>$$<br>(A+uv^T)^{-1}&#x3D;A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}<br>$$<br>得到：</p><p><img src="/../images/obert_imgs/19.png"></p><p><img src="/../images/obert_imgs/20.png"></p><h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>$N_B&#x3D;\frac{d}{B}$表示总的块数，第一步计算：</p><p><img src="/../images/obert_imgs/21.png"></p><p>第二步计算($\in R^{N_B}$)：</p><p><img src="/../images/obert_imgs/22.png"></p><p>第三步计算：</p><p><img src="/../images/obert_imgs/23.png"></p><h1 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h1><p>更新每组权重得分$\rho_Q$：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">scores<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">*</span> finv<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>更新$W^*$</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">obs_updates<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    <span class="token punctuation">.</span>mul<span class="token punctuation">(</span>        <span class="token punctuation">(</span>param<span class="token punctuation">.</span>data <span class="token operator">*</span> <span class="token punctuation">(</span>mask_diffs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>计算逆经验$Fisher$矩阵</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> g<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Updates empirical Fisher inverse with a new gradient    :param g: a collected gradient    """</span>    <span class="token comment"># if 'd / B' is not integer, pad with zeros for batch calculations</span>    <span class="token keyword">if</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">:</span>        g <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>            <span class="token punctuation">[</span>g<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B <span class="token operator">-</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>g<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>    <span class="token comment"># prepare grad for batch calculations</span>    g <span class="token operator">=</span> g<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks<span class="token punctuation">,</span> self<span class="token punctuation">.</span>B<span class="token punctuation">)</span>    <span class="token comment"># batched f_inv x g: (batch, B, B) x (batch, B) -> (batch, B)</span>    finv_g <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bij,bj->bi"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>f_inv<span class="token punctuation">,</span> g<span class="token punctuation">)</span>    <span class="token comment"># scalar denominator for each batch: (batch)</span>    alpha <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>m <span class="token operator">+</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bi,bi->b"</span><span class="token punctuation">,</span> g<span class="token punctuation">,</span> finv_g<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    finv_g <span class="token operator">/=</span> alpha    <span class="token comment"># update f_inv with new outer product: (batch, B) x (batch, B) -> (batch, B, B)</span>    self<span class="token punctuation">.</span>f_inv<span class="token punctuation">.</span>baddbmm_<span class="token punctuation">(</span>finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 二阶近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵导数</title>
      <link href="/2023/10/08/Hessian/"/>
      <url>/2023/10/08/Hessian/</url>
      
        <content type="html"><![CDATA[<h1 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h1><ul><li>一元函数：$f:R \rightarrow R$</li><li>多元函数：$f:R^n \rightarrow R$</li><li>向量函数：$f:R^n \rightarrow R^m$</li></ul><h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><p>导数针对一元函数$f:R \rightarrow R$，$f(x) \approx  f(x_0)+f^1(x_0)(x-x_0)$</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度针对多元函数$f:R^n \rightarrow R$，梯度是一个向量：$\nabla f&#x3D;\begin{bmatrix} \frac{\partial f}{\partial x_1} \ \frac{\partial f}{\partial x_2} \ \frac{\partial f}{\partial x_3}\end{bmatrix}$,也可以写作：函数相对于$\vec {x}$的梯度算子为$\nabla_x$。<br>$$<br>f{(\vec x)}\approx f(\vec{x_0})+\nabla f(\vec{x_0})\cdot (\vec x- \vec{x_0})<br>$$</p><h2 id="Jacobian-矩阵"><a href="#Jacobian-矩阵" class="headerlink" title="$Jacobian$矩阵"></a>$Jacobian$矩阵</h2><p>针对向量函数：$f:R^n \rightarrow R^m$</p><p><img src="/../images/hessian_imgs/1.png"></p><p>矩阵分量：<br>$$<br>\textbf{J}_{ij}&#x3D;\frac{\partial f_i}{\partial x_j}<br>$$<br>其他常用的符号：<br>$$<br>Df、\textbf{Df}、\textbf{J}_f(x_1,…,x_n),\frac{\partial(f_1,…,f_m)}{\partial (x_1,…,x_n)}<br>$$<br>近似：<br>$$<br>f{(\vec x)}\approx f(\vec{x_k})+J (\vec{x_k})\cdot (\vec x- \vec{x_k})<br>$$</p><h1 id="Hessian-矩阵"><a href="#Hessian-矩阵" class="headerlink" title="$Hessian$矩阵"></a>$Hessian$矩阵</h1><p>使用于：$f:R^n \rightarrow R$，是函数的二阶矩阵：</p><p><img src="/../images/hessian_imgs/2.png"></p><p>这是一个$n×n$的方阵，可以写成：<br>$$<br>\textbf{H}_{ij}&#x3D;\frac{\partial ^2f}{\partial x_i \partial x_j}<br>$$<br>近似：</p><p><img src="/../images/hessian_imgs/3.png"></p><p><img src="/../images/hessian_imgs/4.png"></p><p><img src="/../images/hessian_imgs/5.png"></p><h1 id="Fisher-矩阵"><a href="#Fisher-矩阵" class="headerlink" title="$Fisher$矩阵"></a>$Fisher$矩阵</h1><p>$Fisher \ information$:假设观察到的数据$X_1,X2,…,X_n$服从一个概率分布$f(X;\theta)$,$\theta$是目标参数，那么似然函数$likelihood$：</p><p><img src="/../images/hessian_imgs/6.png"></p><p>为了解开方程，需要$\log(likelihood)$的一阶导数为0，其一阶导数$Score\ function$：</p><p><img src="/../images/hessian_imgs/7.png"></p><p>那么$Fisher\ information$用$I(\theta)$表示，定义即$Score\ function$的二阶矩：</p><p><img src="/../images/hessian_imgs/8.png"></p><p>现证明$E[S(X;\theta)]&#x3D;0$：</p><p><img src="/../images/hessian_imgs/9.png"></p><p>从而得到：</p><p><img src="/../images/hessian_imgs/10.png"></p><p>于是得到$Fisher \ information$的第一条数学意义：用来估计$Maximum\ Likelihood \ Estimate$方程的方差。即收集到的数据越多，象征着得到的信息越多。</p><p>对于$\theta$有多大把握，可以围绕估计值的期望，根据模型评分的协方差定义一个不确定性度量：</p><p><img src="/../images/hessian_imgs/11.png"></p><p>上面评分函数的协方差即$Fisher $信息的定义，一般$\theta $ 是一个向量，即$Fisher $信息是以矩阵形式存在，称为$Fisher$信息矩阵$FIM$：</p><p><img src="/../images/hessian_imgs/12.png"></p><p>一般情况下似然函数是复杂的，很难计算期望值，因此可以使用经验分布来近似$F$中的期望值。它由训练数据$X&#x3D;{X_1,X_2,…,X_N}$给出，即：</p><p><img src="/../images/hessian_imgs/13.png"></p><h1 id="Fisher-和-Hessian"><a href="#Fisher-和-Hessian" class="headerlink" title="$Fisher$和$Hessian$"></a>$Fisher$和$Hessian$</h1><p>对数似然的负期望$Hessian$，等于$Fisher$信息矩阵。</p><p>对数似然的$Hessian$为：</p><p><img src="/../images/hessian_imgs/14.png"></p><p>期望：</p><p><img src="/../images/hessian_imgs/15.png"></p><p>因此：</p><p><img src="/../images/hessian_imgs/16.png"></p><p>费舍尔信息矩阵被定义为评分函数的协方差，它是一个曲率矩阵，可以理解为对数似然函数的黑森负期望。因此，F的直接应用，是在二阶优化方法中替换H </p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/question/26561604">(4 封私信) 费雪信息 (Fisher information) 的直观意义是什么？ - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/228099600">费舍尔信息矩阵及自然梯度法 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bilibili.com/video/BV17B4y1y7wX/?spm_id_from=333.788.recommend_more_video.-1&vd_source=50e29aacf23dfe7179edd1b5d8ece200">【TRPO系列讲解】（二）Hessian矩阵、Fisher信息矩阵、KL散度_哔哩哔哩_bilibili</a></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线代 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoraPrune</title>
      <link href="/2023/09/26/LoraPrune/"/>
      <url>/2023/09/26/LoraPrune/</url>
      
        <content type="html"><![CDATA[<h1 id="文章-LoRAPrune"><a href="#文章-LoRAPrune" class="headerlink" title="文章: $LoRAPrune$"></a>文章: $LoRAPrune$</h1><p>文章链接：<br>代码链接：</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>神经网络剪枝可以压缩模型，目前大多数方法依赖于计算参数的梯度，但是计算参数梯度开销很大，文章提出$LoRAPrune$方法，首先设计一个$PEFT$感知的剪枝准则，它利用低等级自适应（$LoRA$）的值和梯度，而不是预先训练的参数的梯度来进行重要性估计。然后，提出了一个迭代剪枝方法，以去除冗余参数，同时最大化$PEFT$的优势。</p><p>在各种任务上的实验结果表明，该方法取得了最先进的结果。例如，在$VTAB-1k$基准中，$LoRAPrune$仅利用了0.76%的可训练参数，显著优于幅度和运动剪枝方法，分别高出5.7%和4.3%。此外，该方法实现了与$PEFT$方法相当的性能，突出了其在提供高质量结果方面的有效性，同时受益于剪枝的优势。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>神经网络剪枝是一种流行的模型压缩技术，通过去除冗余参数，可以显著降低这些大型模型的规模和复杂性。大多数最先进的评估参数重要性的方法都需要参数的梯度。</p><p>莫尔查诺夫引入了一种技术，该技术使用泰勒展开来近似由剪枝引起的损失波动，并使用一阶项来评估参数的重要性。同样，Yu 开发了一种基于梯度显著性评分的方法来评估参数的重要性，Zhang 提出了灵敏度平滑作为计算参数重要性的方法。此外，剪枝过程经常被纳入作为迭代剪枝-再训练循环的一部分，以恢复模型的精度。但是微调和计算梯度代价是昂贵的。</p><p>参数高效调优方法：LoRA插入一组可训练的并行或串行的低秩矩阵。插入的低秩矩阵中的参数数仅为模型参数的1%左右。在下游任务的微调期间，原始参数被冻结（即不更新，不计算梯度），只有插入的低秩矩阵被更新以近似参数更新。由于LoRA只更新了少量的参数，与全参数微调方法相比，它的优化难度和计算需求显著降低。然而，PEFT通常需要冻结的预训练参数，而不计算它们的梯度，依赖于预训练参数的梯度的剪枝方法不能直接应用于这些大语言模型。</p><p>一个想法：是否可以利用LoRA的低秩矩阵的梯度来评估预训练参数的重要性。因此提出LoRAPrune：只使用LoRA的梯度。与下图中描述的梯度引导剪枝方法相比，LoRAPrune利用LoRA的梯度作为预先训练的参数梯度的近似值。因此，$LoRAPrune$实现了对lpm中的冻结参数进行修剪的目标。</p><p><img src="/../images/1.png" alt="两种剪枝方案"></p><p>上图将LoRAPrune（左）与现有的梯度引导剪枝方法（右）进行比较： (a) LoRAPrune通过在整个过程中只计算低秩矩阵，可以对大规模模型进行高效的调优和剪枝。(b)传统的剪枝方法需要从大尺度参数中获得梯度。颜色（红色）表示可训练参数，颜色（蓝色）表示冻结参数，颜色（黄色）表示渐变。</p><p><strong>本文贡献：</strong></p><ul><li>利用从模型的低秩分解中导出的梯度来近似估算预训练参数的重要性。通过这样做，它避免了直接计算所有参数的梯度的需求。</li><li>提出一种将PEFT与剪枝相结合的方法。与其他PEFT方法相比，LoRAPrune能够部署具有相似数量训练参数的轻量级预训练模型。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p><strong>Parameter-efficient tuning</strong>：参数高效微调</p><p><strong>Neural network pruning</strong>：神经网络剪枝。如何确定参数的重要性仍然需要进行研究</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h3 id="初步"><a href="#初步" class="headerlink" title="初步"></a>初步</h3><p>首先重新讨论了具有结构重参数化的参数高效自适应方法。为了有效地微调神经网络的参数，目标模块（如全连接层）可以以并行或顺序的方式插入一个LoRA到预先训练好的参数中。在训练过程中，预先训练的参数被冻结，不计算其梯度，而插入的LoRA是可训练的。</p><p><strong>Parallel low-rank adaption</strong>：给定两个低秩矩阵$A\in R^{r×k}$和$B\in R^{d×r}$，通过并行低秩自适应微调的目标模块的正向过程可以写为：</p><p><img src="/../images/2.png"></p><p>$W_0,z\in R^{n×k},x\in R^{n×d}$表示原始目标模块的权重、输出和输入。自适应后，新的权重$W$可以重新参数化为$W &#x3D; W_0 + BA$。</p><p><strong>Sequential low-rank adaption</strong>:目标模块在顺序低秩自适应中的前向过程可以写为:</p><p><img src="/../images/3.png"></p><p>自适应后，新的权重$W$可以重新参数化为$W &#x3D; (BA+E)W_0$。</p><h3 id="低秩梯度判据"><a href="#低秩梯度判据" class="headerlink" title="低秩梯度判据"></a>低秩梯度判据</h3><p>一个参数$w_{ij}∈W_0$的重要性可以通过去除它所引起的损失来量化。对于输入x和相应的标签y，$w_ij$的诱导误差可以被测量为有无参数的预测误差的平方差：</p><p><img src="/../images/4.png"></p><p>为每个参数计算成本很高。使用一阶泰勒展开式来近似重要性：</p><p><img src="/../images/5.png"></p><p>在LPM中获取$W_0$的梯度是困难的，因为它需要大量的计算能力和存储空间。在本文中，作者讨论了如何通过在下游任务自适应中插入可学习矩阵$A$和$B$来修剪预先训练好的参数$w0$。如上所述，$A$和$B$可以以平行或顺序的方式插入到预先训练好的模型中。因此，文章分别讨论了这两种情况下相应的剪枝方法。</p><p><strong>Pruning for parallel adapter</strong>：在并行的情况下，如果删除元素$w_{ij}∈W$，可以设置元素$（BA）_{ij} &#x3D;−w_{ij}$。等式（3）中每个参数的重要性可以重新表述如下：</p><p><img src="/../images/6.png"></p><p>利用$（BA）_{ij} &#x3D;−w_{ij}$的一阶泰勒展开式来近似等式(5)，参数$w_{ij}$的估计重要性可以表示为:<br><img src="/../images/7.png"></p><p>接下来只保存并使用两个低秩矩阵A和B的梯度来近似：</p><p><img src="/../images/8.png"></p><p><img src="/../images/9.png"></p><p><strong>在(8)中，是否可以改变系数？</strong></p><p>下所示，LoRA梯度准则只需要计算A和B的梯度，与预训练的总权值W0的梯度相比，节省了内存和计算量。</p><p><img src="/../images/10.png"></p><h3 id="LORA剪枝"><a href="#LORA剪枝" class="headerlink" title="LORA剪枝"></a>LORA剪枝</h3><p>使用移动平均值来评估参数的重要性。具体来说，第t次迭代时的参数重要性计算如下（它允许在模型训练的早期阶段就开始剪枝操作，而不必等待整个模型完全收敛，从而节省了时间和计算资源）：</p><p><img src="/../images/11.png"></p><p>插入一个二进制掩模$\beta∈{0,1}^{d×k}$作为参数，然后使用<strong>修剪-微调-修剪</strong>方法进行修剪。考虑到每个参数的重要性，在每次剪枝迭代中通过设置相应的掩码为1，并将其余参数设置为0来保留Top-k个重要参数。形式上，每一个修剪层的正向过程可以写成:</p><p><img src="/../images/12.png"></p><p>算法如下：</p><p><img src="/../images/13.png"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在3090GPU上运行实验，证明了将微调和剪枝过程相结合具有一定的时间效率，且不会影响plm的性能。</p><h1 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h1>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2023/09/20/SVD/"/>
      <url>/2023/09/20/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h1><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>奇异值分解$(Value Decomposition$，简称$SVD)$是一种在线性代数和矩阵分析中非常重要的数学技术，它可以将一个矩阵分解为三个矩阵的乘积，具体来说，将一个矩阵$A$分解为以下形式：<br>$$<br>A &#x3D; UΣV^T<br>$$<br>$SVD$的关键性质和应用包括：</p><ol><li>数据降维：$SVD$可用于将高维数据降维到低维，通过保留最重要的奇异值和对应的奇异向量，可以实现数据压缩和特征选择。</li><li>矩阵逆：$SVD$可用于计算矩阵的伪逆，对于非方阵或奇异矩阵尤其有用。</li><li>奇异值阈值截断：通过保留前k个最大的奇异值和相应的奇异向量，可以实现矩阵的低秩近似，用于图像压缩、推荐系统等。</li><li>主成分分析$(PCA)$：$SVD$可以用于$PCA$，通过对数据协方差矩阵进行$SVD$分解，可以找到数据的主成分。</li><li>推荐系统：$SVD$在协同过滤中有广泛应用，用于推荐用户可能感兴趣的物品。</li><li>图像压缩：$SVD$可用于图像压缩和去噪，通过保留奇异值较大的部分，可以减小图像尺寸并去除一些噪声。</li></ol><h3 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2 矩阵"></a>2 矩阵</h3><p>矩阵的意义：<a href="https://www.cnblogs.com/marsggbo/p/10144060.html">【转载】理解矩阵（三） - marsggbo - 博客园 (cnblogs.com)</a></p><p>以$Ma&#x3D;b$为例介绍矩阵$M$的含义：</p><ul><li>从变换的角度来说，矩阵$M$可以理解为对向量$ a$做变换得到了 $b$。</li><li>坐标系的角度来说，$M$可以理解成是一个坐标系（常用的坐标是笛卡尔坐标系，即 $I$），向量$a$就是在$M$这个坐标系下的坐标，$a$对应到$I$坐标系下的坐标是向量 $b$。</li></ul><h3 id="2-特征值分解"><a href="#2-特征值分解" class="headerlink" title="2 特征值分解"></a>2 特征值分解</h3><p><a href="https://blog.csdn.net/zpalyq110/article/details/86751064">奇异值分解(SVD)原理及实例分析_Freeman_zxp的博客-CSDN博客</a></p><h3 id="2-1-特征值和特征向量"><a href="#2-1-特征值和特征向量" class="headerlink" title="2.1 特征值和特征向量"></a>2.1 特征值和特征向量</h3><h5 id="特征值（Eigenvalues）："><a href="#特征值（Eigenvalues）：" class="headerlink" title="特征值（Eigenvalues）："></a>特征值（Eigenvalues）：</h5><ol><li><p>特征值是一个矩阵的标量性质，通常用λ表示。</p></li><li><p>特征值告诉我们矩阵在某个方向上的缩放因子或拉伸因子。</p></li><li><p>特征值的数目等于矩阵的维度。</p></li><li><p>特征值可以是实数或复数。</p></li></ol><p><strong>对于2：</strong>考虑一个二维平面上的线性变换，由一个矩阵A表示。我们有一个单位向量v（长度为1），它表示一个在该平面上的方向。当我们将这个向量v乘以矩阵A时，我们得到另一个向量Av。如果Av与v的方向相同（可能只是相反方向），那么这意味着矩阵A并没有改变该方向，只是对向量进行了缩放或拉伸。特征值就是用来表示这个缩放或拉伸的因子。具体来说，如果λ是矩阵A的一个特征值，而v是对应的特征向量，那么当我们将向量v乘以矩阵A时，结果是λv。这意味着矩阵A对向量v的作用只是将它缩放为原来的长度的λ倍。如果λ大于1，那么矩阵A在v的方向上对向量进行了拉伸；如果0 &lt; λ &lt; 1，那么矩阵A在v的方向上对向量进行了压缩；如果λ为负数，那么矩阵A对向量进行了反转，并改变了它的方向。所以，特征值λ告诉我们在特定方向v上矩阵A的作用是如何改变向量的大小或方向的。</p><h5 id="特征向量（Eigenvectors）："><a href="#特征向量（Eigenvectors）：" class="headerlink" title="特征向量（Eigenvectors）："></a>特征向量（Eigenvectors）：</h5><ol><li><p>特征向量是与特征值相关联的向量，通常用v表示。</p></li><li><p>特征向量表示在矩阵变换下不改变方向的向量。</p></li><li><p>特征向量描述了矩阵的变换性质，即它们定义了矩阵的主要方向。</p></li><li><p>特征向量通常标准化为单位向量。</p></li></ol><h3 id="2-2-特征值分解"><a href="#2-2-特征值分解" class="headerlink" title="2.2 特征值分解"></a>2.2 特征值分解</h3><p>特征值分解的实质是求解给定矩阵的特征值和 特征向盘，提取出矩阵最重要的特征。<br>既然我们知道一个矩阵是可以通过特征值和特征向量来表示，那假设存在一个$n×n$的满秩对称矩阵$A$，我们便可以通过特征值将$A$分解。首先求出$A$的$n$个特征值：$\lambda_1,\lambda_2,…,\lambda_n$,以及对应的特征向量(标准化处理后的):$x_1,x_2,…,x_n$。于是：<br>$$<br>Ax_1&#x3D;\lambda_1x_1 \\Ax_2&#x3D;\lambda_2x_2 \\……\\Ax_n&#x3D;\lambda_nx_n<br>$$<br>令$U&#x3D;[x_1,x_2,…,x_n]$,$\Lambda&#x3D;\begin{bmatrix} \lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_n \end{bmatrix}$化简公式为：$AU&#x3D;U\Lambda$。$U$是正交阵，有$U^T&#x3D;U^{-1}$。最终：<br>$$<br>A&#x3D;U\Lambda U^{-1}&#x3D;U\Lambda U^T<br>$$</p><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3 奇异值分解"></a>3 奇异值分解</h3><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p>对于满秩对称矩阵，可以简单的通过计算特征值进行分解，对于$m×n$的一般矩阵，需要使用奇异值分解$SVD$。已知对于任意矩阵都满足$A^TA,AA^T$，为对称矩阵，因此可以对$A^TA,AA^T$进行分解。</p><p>定义矩阵$A$的$SVD$为：<br>$$<br>A&#x3D;U\Sigma V^T<br>$$<br>其中$U$是$m×m$的矩阵，$\Sigma$是$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值。$V$是$m×m$的矩阵,$U$和$V$都是酉矩阵，满足$U^TU&#x3D;I,V^TV&#x3D;I$。</p><p><strong>计算：</strong></p><p>首先得到$n×n$的方阵$A^TA$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(A^TA)v_i&#x3D;\lambda_iv_i<br>$$<br>这样我们就可以得到矩阵 $A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将  $A^TA$ 的所有特征向量张成一个$n×n$的矩阵$V$,即$SVD$公式中的矩阵$V$,一般我们将$V$中的每个特征向量叫做A的右奇异向量。</p><p>然后得到$m×m$的方阵$AA^T$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(AA^T)u_i&#x3D;\lambda_iu_i<br>$$<br>这样我们就可以得到矩阵 $AA^T$的$n$个特征值和对应的$n$个特征向量$u$了。将  $AA^T$ 的所有特征向量张成一个$m×m$的矩阵$U$,即$SVD$公式中的矩阵$U$,一般我们将$U$中的每个特征向量叫做A的左奇异向量。</p><p>对于$\Sigma$,除了对角线上是奇异值其他位置都是0,因此只需要求出每个奇异值$\sigma$即可。注意到：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow AV&#x3D;U\Sigma V^TV\Rightarrow AV&#x3D;U\Sigma\Rightarrow Av_i&#x3D;\sigma_iu_i\Rightarrow \sigma_i&#x3D;Av_i&#x2F;u_i<br>$$<br>可以求出奇异值矩阵$\Sigma$。</p><p>证明$A^TA$的特征向量组成的就是$SVD$中的$V$矩阵：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow A^T&#x3D;V\Sigma U^T\Rightarrow A^TA&#x3D;V\Sigma U^TU\Sigma V^T&#x3D;V\Sigma^2V^T\ (U^TU&#x3D;I)<br>$$<br>进一步可以看到特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：<br>$$<br>\sigma_i&#x3D;\sqrt{\lambda_i}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoSparse</title>
      <link href="/2023/09/19/LoSparse/"/>
      <url>/2023/09/19/LoSparse/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1950343972&auto=1&height=66"></iframe><h1 id="文章题目：-LoSparse"><a href="#文章题目：-LoSparse" class="headerlink" title="文章题目：$LoSparse$"></a>文章题目：$LoSparse$</h1><p><strong>文章链接：</strong><a href="https://arxiv.org/pdf/2306.11222.pdf">https://arxiv.org/pdf/2306.11222.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/yxli2123/LoSparse">https://github.com/yxli2123/LoSparse</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>$transform$模型需要大量的计算资源，出于模型压缩的目的，作者提出了$LoSparse(Low-Rank and Sparse approximation)$模型，它通过一个<strong>低秩矩阵</strong>和一个<strong>稀疏矩阵</strong>的和来逼近一个权重矩阵。该方法结合了低秩近似和剪枝的优点，同时避免了它们的限制。剪枝增强了低秩近似的多样性，低秩近似可以防止剪枝损失太多的表达神经元</p><ul><li>低秩近似：压缩了神经元中的一致和表达丰富的部分</li><li>剪枝：去除了神经元中的不一致和非表达丰富的部分</li></ul><p>低秩可以压缩权重中相关性大的部分，但是完全忽略了相关性较小的部分（这些相关性小的部分在神经元之间可能是独特的，具有表达能力，并且对于模型的性能来说可能非常关键），结合剪枝，可以继续减少相关性小的那部分但是不至于全部去除导致网络多样性下降。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>目前模型压缩常用的是剪枝,分为结构性剪枝和非结构性剪枝。结构性剪枝一种方式为ITP，即迭代剪枝，可以同时训练和剪枝，本文用到的剪枝方法就是迭代剪枝。</p><p>为什么只用剪枝不行：</p><p><img src="/../images/image-20230918230903444.png" alt="神经元重要性得分直方图"></p><p>(b)为理想情况，大多数神经元是冗余的，对于网络来说不重要，只有小部分神经元是重要的，但是现实情况为(a)，很大一部分神经元是可表达的，因此大量剪枝可能导致重要的神经元会被减去。</p><p>低秩近似可以提取相关性大的权重的公共基。但是$transformer$模型的秩很高，它们包含许多参数，这使得简单地应用低秩近似来压缩这些矩阵可能会损害模型的性能。这是因为这种情况下忽略了神经元的多样性。</p><p>因此提出了低秩近似和稀疏近似的结合：低秩近似防止了修剪过度去除表达神经元，而稀疏近似增强了低秩近似的多样性。</p><p>同时该方法与知识蒸馏是正交关系，很容易再集成知识蒸馏手段，提高模型性能。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>介绍了transform模型和权重敏感性评分公式：</p><p><img src="/../images/image-20230918232543247.png"></p><p>为了减少随机抽样导致的可变性，本文使用平滑公式:</p><p><img src="/../images/image-20230918232355166.png"></p><p>结构性剪枝神经元的重要性评分公式：</p><p><img src="/../images/image-20230918232819344.png"></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>通过一个低秩矩阵和一个稀疏矩阵的和来近似一个权矩阵：</p><p><img src="/../images/image-20230918233005983.png" alt="LoSparse 在单个线性投影矩阵的示意图"></p><h3 id="低秩矩阵和稀疏矩阵的近似"><a href="#低秩矩阵和稀疏矩阵的近似" class="headerlink" title="低秩矩阵和稀疏矩阵的近似"></a>低秩矩阵和稀疏矩阵的近似</h3><p>给定一个权重矩阵$W\in R^{d_1×d_2}$,通常采用结构化剪枝稀疏矩阵$S\in R^{d_1×d_2}$来近似$W$以进行压缩。然而稀疏矩阵近似导致性能不佳，尤其是当压缩比率较高时。因此，本文引入了一个低秩矩阵来改进近似。具体来说，权重矩阵可以表示为：</p><p><img src="/../images/image-20230918233726382.png"></p><p>其中$U\in R^{d_1×d_2}$和$R\in R^{d_1×d_2}$的乘积表示秩为$r$的低秩矩阵。</p><p><strong>为什么需要低秩矩阵？</strong></p><p><img src="/../images/image-20230918234337880.png" alt="语言模型的奇异值"></p><p>首先，<strong>它可以有效地逼近神经元的相干部分</strong>，我们可以看到语言模型中权重矩阵的频谱在开始时迅速下降。<strong>频谱下降反映模型中特征之间的相关性。较大的特征值通常对应于具有更高相关性的特征。因此，快速下降的频谱表示模型中存在一些高度相关的特征，即一致部分</strong>，因此可以通过低秩来压缩公共部分。</p><p>其次<strong>低秩矩阵和稀疏矩阵的解耦使得剪枝变得容易</strong>。<strong>频谱的趋于平稳表示模型的权重矩阵中的特征或模式已经相对稳定地学习和表示。这也可能表明在训练过程中，神经元之间的相关性逐渐减弱，即神经元对不同的特征或模式变得更加独立</strong>而低秩矩阵无法捕获这些信息，但是低秩矩阵能够将相干部分与神经元的非相干部分解耦。因此可以添加一个矩阵$S$来近似剩余的不连贯部分，然后修剪非表达不连贯的部分。</p><p><img src="/../images/image-20230918230903444.png" alt="线性投影的神经元的重要性得分分布情况"></p><p>上图表示，大多数不连贯的部分在解耦之后具有较低的重要性分数，因此可用剪枝删除冗余参数。LoSparse算法成功地分离了神经元中不连贯的部分，并简化了非表达成分的修剪。(和ideal的图逼近)。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>给定一个预训练的权重矩阵$W^{(0)}$，首先基于$W^{(0)}$的奇异值分解（SVD）初始化秩 $r$的低秩矩阵。具体来说，本文选择：</p><p><img src="/../images/image-20230919000424683.png"></p><p>$u_1,u_2,…,u_r\in R^{d_1}$是左奇异向量，$v_1,v_2,…,v_r\in R^{d_1}$是左奇异向量，关于上面的$r$个奇异值$\sigma_1 \geq \sigma_2 \geq …\geq\sigma_3$在$W^{(0)}$的SVD中。</p><p>因此初始化$S^{(0)}$为：</p><p><img src="/../images/image-20230919001321431.png"></p><p>因此，原始的前向传递（$Y&#x3D;XW$）可替换为更高效的形式:</p><p><img src="/../images/image-20230919001529751.png"></p><p>作者将上面这种分解应用到了每一个权重矩阵并且将$S&#x3D;{S_m}_{m&#x3D;1}^{M}$表示为所有稀疏矩阵的集合。初始化$S$之后，对所有$S$进行迭代剪枝。</p><p><strong>具体方法：</strong></p><p>在第t次迭代时，首先采取随机梯度下降步骤来更新$U^{(t)}$,$V^{(t)}$和$S^{(t)}$。对于$S^{(t)}$，</p><p><img src="/../images/1695089652536.jpg"></p><p>这是损失函数 $L$ 对于变量 $S^{(t)}$的梯度。梯度是一个向量，它包含了损失函数相对于每个分量（或维度）的偏导数。</p><p>然后在公式(4)的基础上对$S^{(t)}$进行重要性评分，$\widetilde{S}^{(t)}$剪枝公式如下：</p><p><img src="/../images/1111.png"></p><p>$\tau( {\widetilde{S}^{(t)}},\Gamma({S}^{(t)}))$的第$i$列定义如下：</p><p><img src="/../images/1695090856116.jpg"></p><p>在每个迭代中，选择性地保留了权重矩阵 $S^{(t)}$中具有高重要性分数的一部分神经元，而剔除了贡献较低的神经元。这个策略通过逐渐减小 $p_t$ 的值来控制保留的神经元数量，以达到对模型进行剪枝$（pruning）$或精简的目的,$p_t$减小公式如下：</p><p><img src="/../images/1695091453574.jpg"></p><p>具体算法如下：</p><p><img src="/../images/1695091607438.jpg" alt="算法"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在自然语言理解、问答任务、自然语言生成任务与其他模型做了对比，显示出模型的优越性。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h5 id="稀疏逼近的有效性"><a href="#稀疏逼近的有效性" class="headerlink" title="稀疏逼近的有效性"></a>稀疏逼近的有效性</h5><p>作者将LoSparse与两种变体进行比较:(1)丢弃稀疏矩阵$S$，只微调低秩矩阵$UV$$(Low-rank \ I)$；(2)遵循(8)的初始化，但逐渐将初始化的$S$修剪为零$(Low-rank  \ II)$。</p><p><img src="/../images/1695092292904.jpg" alt="不同任务比较结果"></p><p>从图中可见，$(Low-rank  \ II)$的性能要比$(Low-rank \ I)$好得多。也就是说，修剪掉所有的稀疏矩阵比微调一个由奇异值阈值得到的低秩矩阵更有效，因此$LoSeparse$#可以增强低秩近似。</p><p><strong>解释：</strong>这是因为$Low-rank  \ I$的初始化与预训练的权重不同，因此它可能会从预训练的权重中丢失太多的知识。因此，下游任务的性能会严重下降。另一方面，$LoSeparse$弥补了低秩初始化与预训练权值之间的差距，从而保留了存储在预训练权值中的原始知识。这表明，尽管单独的低秩近似更有效和简洁，但我们应该利用稀疏近似来指导其训练过程。</p><h5 id="稀疏分配"><a href="#稀疏分配" class="headerlink" title="稀疏分配"></a>稀疏分配</h5><p>主要探究低秩近似和稀疏近似如何互相分配，主要方法是给定一个固定的剩余比例，改变低秩矩阵的比例，并相应改变稀疏矩阵的比例。不同分配条件下的结果如下图，可以看到低秩近似和稀疏近似对NLU任务的性能贡献几乎相等，因为当改变分配时性能保持稳定。</p><p><img src="/../images/786cac11a7d27a4dadbb82d0082e95a.png" alt="关于稀疏分配的结果"></p><h3 id="结合知识蒸馏"><a href="#结合知识蒸馏" class="headerlink" title="结合知识蒸馏"></a>结合知识蒸馏</h3><p>本文选择了一个针对特定任务进行微调的$DeBertaV3-base$模型作为教师模型，一个压缩的$DeBertaV3-base$模型作为学生模型。结果发现知识蒸馏可以进一步提高$LoSeparse$的性能,</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>文章提出了一种结合低秩近似和结构化稀疏近似的$transfomer$模型压缩方法$LoSparse$。在自然语言理解、问题回答和自然语言生成方面的实验表明，作者的方法明显优于以前的压缩方法。此外，其在自然语言生成任务和极高稀疏度的设置中特别有效。实验证明了$Losparse$是通用的，与其他流行的压缩方法是互补的。实验表明，$LoSparse$算法可以提高$CoFi$算法和传统的知识蒸馏迭代剪枝算法的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
