<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SVD</title>
      <link href="/2023/09/20/SVD/"/>
      <url>/2023/09/20/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h1><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>奇异值分解$(Value Decomposition$，简称$SVD)$是一种在线性代数和矩阵分析中非常重要的数学技术，它可以将一个矩阵分解为三个矩阵的乘积，具体来说，将一个矩阵$A$分解为以下形式：<br>$$<br>A &#x3D; UΣV^T<br>$$<br>$SVD$的关键性质和应用包括：</p><ol><li>数据降维：$SVD$可用于将高维数据降维到低维，通过保留最重要的奇异值和对应的奇异向量，可以实现数据压缩和特征选择。</li><li>矩阵逆：$SVD$可用于计算矩阵的伪逆，对于非方阵或奇异矩阵尤其有用。</li><li>奇异值阈值截断：通过保留前k个最大的奇异值和相应的奇异向量，可以实现矩阵的低秩近似，用于图像压缩、推荐系统等。</li><li>主成分分析$(PCA)$：$SVD$可以用于$PCA$，通过对数据协方差矩阵进行$SVD$分解，可以找到数据的主成分。</li><li>推荐系统：$SVD$在协同过滤中有广泛应用，用于推荐用户可能感兴趣的物品。</li><li>图像压缩：$SVD$可用于图像压缩和去噪，通过保留奇异值较大的部分，可以减小图像尺寸并去除一些噪声。</li></ol><h3 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2 矩阵"></a>2 矩阵</h3><p>矩阵的意义：<a href="https://www.cnblogs.com/marsggbo/p/10144060.html">【转载】理解矩阵（三） - marsggbo - 博客园 (cnblogs.com)</a></p><p>以$Ma&#x3D;b$为例介绍矩阵$M$的含义：</p><ul><li>从变换的角度来说，矩阵$M$可以理解为对向量$ a$做变换得到了 $b$。</li><li>坐标系的角度来说，$M$可以理解成是一个坐标系（常用的坐标是笛卡尔坐标系，即 $I$），向量$a$就是在$M$这个坐标系下的坐标，$a$对应到$I$坐标系下的坐标是向量 $b$。</li></ul><h3 id="2-特征值分解"><a href="#2-特征值分解" class="headerlink" title="2 特征值分解"></a>2 特征值分解</h3><p><a href="https://blog.csdn.net/zpalyq110/article/details/86751064">奇异值分解(SVD)原理及实例分析_Freeman_zxp的博客-CSDN博客</a></p><h3 id="2-1-特征值和特征向量"><a href="#2-1-特征值和特征向量" class="headerlink" title="2.1 特征值和特征向量"></a>2.1 特征值和特征向量</h3><h5 id="特征值（Eigenvalues）："><a href="#特征值（Eigenvalues）：" class="headerlink" title="特征值（Eigenvalues）："></a>特征值（Eigenvalues）：</h5><ol><li><p>特征值是一个矩阵的标量性质，通常用λ表示。</p></li><li><p>特征值告诉我们矩阵在某个方向上的缩放因子或拉伸因子。</p></li><li><p>特征值的数目等于矩阵的维度。</p></li><li><p>特征值可以是实数或复数。</p></li></ol><p><strong>对于2：</strong>考虑一个二维平面上的线性变换，由一个矩阵A表示。我们有一个单位向量v（长度为1），它表示一个在该平面上的方向。当我们将这个向量v乘以矩阵A时，我们得到另一个向量Av。如果Av与v的方向相同（可能只是相反方向），那么这意味着矩阵A并没有改变该方向，只是对向量进行了缩放或拉伸。特征值就是用来表示这个缩放或拉伸的因子。具体来说，如果λ是矩阵A的一个特征值，而v是对应的特征向量，那么当我们将向量v乘以矩阵A时，结果是λv。这意味着矩阵A对向量v的作用只是将它缩放为原来的长度的λ倍。如果λ大于1，那么矩阵A在v的方向上对向量进行了拉伸；如果0 &lt; λ &lt; 1，那么矩阵A在v的方向上对向量进行了压缩；如果λ为负数，那么矩阵A对向量进行了反转，并改变了它的方向。所以，特征值λ告诉我们在特定方向v上矩阵A的作用是如何改变向量的大小或方向的。</p><h5 id="特征向量（Eigenvectors）："><a href="#特征向量（Eigenvectors）：" class="headerlink" title="特征向量（Eigenvectors）："></a>特征向量（Eigenvectors）：</h5><ol><li><p>特征向量是与特征值相关联的向量，通常用v表示。</p></li><li><p>特征向量表示在矩阵变换下不改变方向的向量。</p></li><li><p>特征向量描述了矩阵的变换性质，即它们定义了矩阵的主要方向。</p></li><li><p>特征向量通常标准化为单位向量。</p></li></ol><h3 id="2-2-特征值分解"><a href="#2-2-特征值分解" class="headerlink" title="2.2 特征值分解"></a>2.2 特征值分解</h3><p>特征值分解的实质是求解给定矩阵的特征值和 特征向盘，提取出矩阵最重要的特征。<br>既然我们知道一个矩阵是可以通过特征值和特征向量来表示，那假设存在一个$n×n$的满秩对称矩阵$A$，我们便可以通过特征值将$A$分解。首先求出$A$的$n$个特征值：$\lambda_1,\lambda_2,…,\lambda_n$,以及对应的特征向量(标准化处理后的):$x_1,x_2,…,x_n$。于是：<br>$$<br>Ax_1&#x3D;\lambda_1x_1 \\Ax_2&#x3D;\lambda_2x_2 \\……\\Ax_n&#x3D;\lambda_nx_n<br>$$<br>令$U&#x3D;[x_1,x_2,…,x_n]$,$\Lambda&#x3D;\begin{bmatrix} \lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_n \end{bmatrix}$化简公式为：$AU&#x3D;U\Lambda$。$U$是正交阵，有$U^T&#x3D;U^{-1}$。最终：<br>$$<br>A&#x3D;U\Lambda U^{-1}&#x3D;U\Lambda U^T<br>$$</p><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3 奇异值分解"></a>3 奇异值分解</h3><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p>对于满秩对称矩阵，可以简单的通过计算特征值进行分解，对于$m×n$的一般矩阵，需要使用奇异值分解$SVD$。已知对于任意矩阵都满足$A^TA,AA^T$，为对称矩阵，因此可以对$A^TA,AA^T$进行分解。</p><p>定义矩阵$A$的$SVD$为：<br>$$<br>A&#x3D;U\Sigma V^T<br>$$<br>其中$U$是$m×m$的矩阵，$\Sigma$是$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值。$V$是$m×m$的矩阵,$U$和$V$都是酉矩阵，满足$U^TU&#x3D;I,V^TV&#x3D;I$。</p><p><strong>计算：</strong></p><p>首先得到$n×n$的方阵$A^TA$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(A^TA)v_i&#x3D;\lambda_iv_i<br>$$<br>这样我们就可以得到矩阵 $A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将  $A^TA$ 的所有特征向量张成一个$n×n$的矩阵$V$,即$SVD$公式中的矩阵$V$,一般我们将$V$中的每个特征向量叫做A的右奇异向量。</p><p>然后得到$m×m$的方阵$AA^T$。然后进行特征值分解，得到的特征值和特征向量满足下式：<br>$$<br>(AA^T)u_i&#x3D;\lambda_iu_i<br>$$<br>这样我们就可以得到矩阵 $AA^T$的$n$个特征值和对应的$n$个特征向量$u$了。将  $AA^T$ 的所有特征向量张成一个$m×m$的矩阵$U$,即$SVD$公式中的矩阵$U$,一般我们将$U$中的每个特征向量叫做A的左奇异向量。</p><p>对于$\Sigma$,除了对角线上是奇异值其他位置都是0,因此只需要求出每个奇异值$\sigma$即可。注意到：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow AV&#x3D;U\Sigma V^TV\Rightarrow AV&#x3D;U\Sigma\Rightarrow Av_i&#x3D;\sigma_iu_i\Rightarrow \sigma_i&#x3D;Av_i&#x2F;u_i<br>$$<br>可以求出奇异值矩阵$\Sigma$。</p><p>证明$A^TA$的特征向量组成的就是$SVD$中的$V$矩阵：<br>$$<br>A&#x3D;U\Sigma V^T\Rightarrow A^T&#x3D;V\Sigma U^T\Rightarrow A^TA&#x3D;V\Sigma U^TU\Sigma V^T&#x3D;V\Sigma^2V^T\ (U^TU&#x3D;I)<br>$$<br>进一步可以看到特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：<br>$$<br>\sigma_i&#x3D;\sqrt{\lambda_i}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoSparse</title>
      <link href="/2023/09/19/LoSparse/"/>
      <url>/2023/09/19/LoSparse/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1950343972&auto=1&height=66"></iframe><h1 id="文章题目：-LoSparse"><a href="#文章题目：-LoSparse" class="headerlink" title="文章题目：$LoSparse$"></a>文章题目：$LoSparse$</h1><p><strong>文章链接：</strong><a href="https://arxiv.org/pdf/2306.11222.pdf">https://arxiv.org/pdf/2306.11222.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/yxli2123/LoSparse">https://github.com/yxli2123/LoSparse</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>$transform$模型需要大量的计算资源，出于模型压缩的目的，作者提出了$LoSparse(Low-Rank and Sparse approximation)$模型，它通过一个<strong>低秩矩阵</strong>和一个<strong>稀疏矩阵</strong>的和来逼近一个权重矩阵。该方法结合了低秩近似和剪枝的优点，同时避免了它们的限制。剪枝增强了低秩近似的多样性，低秩近似可以防止剪枝损失太多的表达神经元</p><ul><li>低秩近似：压缩了神经元中的一致和表达丰富的部分</li><li>剪枝：去除了神经元中的不一致和非表达丰富的部分</li></ul><p>低秩可以压缩权重中相关性大的部分，但是完全忽略了相关性较小的部分（这些相关性小的部分在神经元之间可能是独特的，具有表达能力，并且对于模型的性能来说可能非常关键），结合剪枝，可以继续减少相关性小的那部分但是不至于全部去除导致网络多样性下降。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>目前模型压缩常用的是剪枝,分为结构性剪枝和非结构性剪枝。结构性剪枝一种方式为ITP，即迭代剪枝，可以同时训练和剪枝，本文用到的剪枝方法就是迭代剪枝。</p><p>为什么只用剪枝不行：</p><p><img src="/../images/image-20230918230903444.png" alt="神经元重要性得分直方图"></p><p>(b)为理想情况，大多数神经元是冗余的，对于网络来说不重要，只有小部分神经元是重要的，但是现实情况为(a)，很大一部分神经元是可表达的，因此大量剪枝可能导致重要的神经元会被减去。</p><p>低秩近似可以提取相关性大的权重的公共基。但是$transformer$模型的秩很高，它们包含许多参数，这使得简单地应用低秩近似来压缩这些矩阵可能会损害模型的性能。这是因为这种情况下忽略了神经元的多样性。</p><p>因此提出了低秩近似和稀疏近似的结合：低秩近似防止了修剪过度去除表达神经元，而稀疏近似增强了低秩近似的多样性。</p><p>同时该方法与知识蒸馏是正交关系，很容易再集成知识蒸馏手段，提高模型性能。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>介绍了transform模型和权重敏感性评分公式：</p><p><img src="/../images/image-20230918232543247.png"></p><p>为了减少随机抽样导致的可变性，本文使用平滑公式:</p><p><img src="/../images/image-20230918232355166.png"></p><p>结构性剪枝神经元的重要性评分公式：</p><p><img src="/../images/image-20230918232819344.png"></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>通过一个低秩矩阵和一个稀疏矩阵的和来近似一个权矩阵：</p><p><img src="/../images/image-20230918233005983.png" alt="LoSparse 在单个线性投影矩阵的示意图"></p><h3 id="低秩矩阵和稀疏矩阵的近似"><a href="#低秩矩阵和稀疏矩阵的近似" class="headerlink" title="低秩矩阵和稀疏矩阵的近似"></a>低秩矩阵和稀疏矩阵的近似</h3><p>给定一个权重矩阵$W\in R^{d_1×d_2}$,通常采用结构化剪枝稀疏矩阵$S\in R^{d_1×d_2}$来近似$W$以进行压缩。然而稀疏矩阵近似导致性能不佳，尤其是当压缩比率较高时。因此，本文引入了一个低秩矩阵来改进近似。具体来说，权重矩阵可以表示为：</p><p><img src="/../images/image-20230918233726382.png"></p><p>其中$U\in R^{d_1×d_2}$和$R\in R^{d_1×d_2}$的乘积表示秩为$r$的低秩矩阵。</p><p><strong>为什么需要低秩矩阵？</strong></p><p><img src="/../images/image-20230918234337880.png" alt="语言模型的奇异值"></p><p>首先，<strong>它可以有效地逼近神经元的相干部分</strong>，我们可以看到语言模型中权重矩阵的频谱在开始时迅速下降。<strong>频谱下降反映模型中特征之间的相关性。较大的特征值通常对应于具有更高相关性的特征。因此，快速下降的频谱表示模型中存在一些高度相关的特征，即一致部分</strong>，因此可以通过低秩来压缩公共部分。</p><p>其次<strong>低秩矩阵和稀疏矩阵的解耦使得剪枝变得容易</strong>。<strong>频谱的趋于平稳表示模型的权重矩阵中的特征或模式已经相对稳定地学习和表示。这也可能表明在训练过程中，神经元之间的相关性逐渐减弱，即神经元对不同的特征或模式变得更加独立</strong>而低秩矩阵无法捕获这些信息，但是低秩矩阵能够将相干部分与神经元的非相干部分解耦。因此可以添加一个矩阵$S$来近似剩余的不连贯部分，然后修剪非表达不连贯的部分。</p><p><img src="/../images/image-20230918230903444.png" alt="线性投影的神经元的重要性得分分布情况"></p><p>上图表示，大多数不连贯的部分在解耦之后具有较低的重要性分数，因此可用剪枝删除冗余参数。LoSparse算法成功地分离了神经元中不连贯的部分，并简化了非表达成分的修剪。(和ideal的图逼近)。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>给定一个预训练的权重矩阵$W^{(0)}$，首先基于$W^{(0)}$的奇异值分解（SVD）初始化秩 $r$的低秩矩阵。具体来说，本文选择：</p><p><img src="/../images/image-20230919000424683.png"></p><p>$u_1,u_2,…,u_r\in R^{d_1}$是左奇异向量，$v_1,v_2,…,v_r\in R^{d_1}$是左奇异向量，关于上面的$r$个奇异值$\sigma_1 \geq \sigma_2 \geq …\geq\sigma_3$在$W^{(0)}$的SVD中。</p><p>因此初始化$S^{(0)}$为：</p><p><img src="/../images/image-20230919001321431.png"></p><p>因此，原始的前向传递（$Y&#x3D;XW$）可替换为更高效的形式:</p><p><img src="/../images/image-20230919001529751.png"></p><p>作者将上面这种分解应用到了每一个权重矩阵并且将$S&#x3D;{S_m}_{m&#x3D;1}^{M}$表示为所有稀疏矩阵的集合。初始化$S$之后，对所有$S$进行迭代剪枝。</p><p><strong>具体方法：</strong></p><p>在第t次迭代时，首先采取随机梯度下降步骤来更新$U^{(t)}$,$V^{(t)}$和$S^{(t)}$。对于$S^{(t)}$，</p><p><img src="/../images/1695089652536.jpg"></p><p>这是损失函数 $L$ 对于变量 $S^{(t)}$的梯度。梯度是一个向量，它包含了损失函数相对于每个分量（或维度）的偏导数。</p><p>然后在公式(4)的基础上对$S^{(t)}$进行重要性评分，$\widetilde{S}^{(t)}$剪枝公式如下：</p><p><img src="/../images/1111.png"></p><p>$\tau( {\widetilde{S}^{(t)}},\Gamma({S}^{(t)}))$的第$i$列定义如下：</p><p><img src="/../images/1695090856116.jpg"></p><p>在每个迭代中，选择性地保留了权重矩阵 $S^{(t)}$中具有高重要性分数的一部分神经元，而剔除了贡献较低的神经元。这个策略通过逐渐减小 $p_t$ 的值来控制保留的神经元数量，以达到对模型进行剪枝$（pruning）$或精简的目的,$p_t$减小公式如下：</p><p><img src="/../images/1695091453574.jpg"></p><p>具体算法如下：</p><p><img src="/../images/1695091607438.jpg" alt="算法"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在自然语言理解、问答任务、自然语言生成任务与其他模型做了对比，显示出模型的优越性。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h5 id="稀疏逼近的有效性"><a href="#稀疏逼近的有效性" class="headerlink" title="稀疏逼近的有效性"></a>稀疏逼近的有效性</h5><p>作者将LoSparse与两种变体进行比较:(1)丢弃稀疏矩阵$S$，只微调低秩矩阵$UV$$(Low-rank \ I)$；(2)遵循(8)的初始化，但逐渐将初始化的$S$修剪为零$(Low-rank  \ II)$。</p><p><img src="/../images/1695092292904.jpg" alt="不同任务比较结果"></p><p>从图中可见，$(Low-rank  \ II)$的性能要比$(Low-rank \ I)$好得多。也就是说，修剪掉所有的稀疏矩阵比微调一个由奇异值阈值得到的低秩矩阵更有效，因此$LoSeparse$#可以增强低秩近似。</p><p><strong>解释：</strong>这是因为$Low-rank  \ I$的初始化与预训练的权重不同，因此它可能会从预训练的权重中丢失太多的知识。因此，下游任务的性能会严重下降。另一方面，$LoSeparse$弥补了低秩初始化与预训练权值之间的差距，从而保留了存储在预训练权值中的原始知识。这表明，尽管单独的低秩近似更有效和简洁，但我们应该利用稀疏近似来指导其训练过程。</p><h5 id="稀疏分配"><a href="#稀疏分配" class="headerlink" title="稀疏分配"></a>稀疏分配</h5><p>主要探究低秩近似和稀疏近似如何互相分配，主要方法是给定一个固定的剩余比例，改变低秩矩阵的比例，并相应改变稀疏矩阵的比例。不同分配条件下的结果如下图，可以看到低秩近似和稀疏近似对NLU任务的性能贡献几乎相等，因为当改变分配时性能保持稳定。</p><p><img src="/../images/786cac11a7d27a4dadbb82d0082e95a.png" alt="关于稀疏分配的结果"></p><h3 id="结合知识蒸馏"><a href="#结合知识蒸馏" class="headerlink" title="结合知识蒸馏"></a>结合知识蒸馏</h3><p>本文选择了一个针对特定任务进行微调的$DeBertaV3-base$模型作为教师模型，一个压缩的$DeBertaV3-base$模型作为学生模型。结果发现知识蒸馏可以进一步提高$LoSeparse$的性能,</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>文章提出了一种结合低秩近似和结构化稀疏近似的$transfomer$模型压缩方法$LoSparse$。在自然语言理解、问题回答和自然语言生成方面的实验表明，作者的方法明显优于以前的压缩方法。此外，其在自然语言生成任务和极高稀疏度的设置中特别有效。实验证明了$Losparse$是通用的，与其他流行的压缩方法是互补的。实验表明，$LoSparse$算法可以提高$CoFi$算法和传统的知识蒸馏迭代剪枝算法的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
