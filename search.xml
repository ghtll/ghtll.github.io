<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>模型压缩总结2</title>
      <link href="/2024/07/14/2024714/"/>
      <url>/2024/07/14/2024714/</url>
      
        <content type="html"><![CDATA[<h1 id="1-CBQ"><a href="#1-CBQ" class="headerlink" title="1 CBQ"></a>1 CBQ</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>CBQ: Cross-Block Quantization for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年4月</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化、低秩近似</td></tr></tbody></table></div><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目前的训练后量化面临的问题：</p><ul><li>处理单个块忽略了块之间的依赖</li></ul><p>CBQ提出：</p><ul><li>跨块重构的PTQ方法</li><li>使用同源重构方案，建立跨多块的依赖性，最小化累积误差</li><li>提出从粗道喜的预处理策略（CFP）：用于抑制权重中和激活中的异常值</li><li>提出自适用LoRARounding技术：用于精确的量化权重</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>量化和反量化处理表示：</p><p><img src="/images/2024714/1.png" alt=""></p><p>$\Delta w$表示权重的量化舍入矩阵。</p><h3 id="Cross-block-reconstruction"><a href="#Cross-block-reconstruction" class="headerlink" title="Cross-block reconstruction"></a>Cross-block reconstruction</h3><p>同时优化K个transfomers块：</p><p><img src="/images/2024714/2.png" alt=""></p><h5 id="Cross-block-dependency"><a href="#Cross-block-dependency" class="headerlink" title="Cross-block dependency"></a>Cross-block dependency</h5><p>逐块重建可以有效地节省计算内存，但它只考虑每个块内的局部信息，忽略了不同块之间的依赖性。文章引入了使用滑动窗口方法的跨块依赖（CBD）方案。该方案能够同时优化窗口内的多个块。此外，两个相邻的滑动窗具有<strong>重叠</strong>的块，确保窗口之间的块也相互连接。CBD 方案的优化表述为：</p><p><img src="/images/2024714/3.png" alt=""></p><p>尽管CBD技术有助于降低重建难度，但需要注意的是，它不能完全消除累积的误差，并且每个窗口内的优化仍然是局部的。为了解决这个限制并在确保稳定性的同时进一步增强重建过程，文章引入了一种额外的<strong>同源重建</strong>方案，如下图所示。在该方案中，将量化模型的前一个块的输出输入到全精度模型的当前块中。这会生成与量化模型的输出同源的附加输出。</p><p><img src="/images/2024714/4.png" alt=""></p><p>优化目标变为：</p><p><img src="/images/2024714/5.png" alt=""></p><p>对于距离度量，结合 L2 和 KullbackLeibler 散度 (KLD) 损失来测量重建误差。 它倾向于抑制特征空间中的异常值并增强优化过程的鲁棒性。通过合并这两个术语，捕获了空间距离和分布差异，从而实现了更全面、更稳健的优化过程。那么距离度量可以表示为：</p><p><img src="/images/2024714/6.png" alt=""></p><h3 id="Coarse-to-fine-pre-processing"><a href="#Coarse-to-fine-pre-processing" class="headerlink" title="Coarse-to-fine pre-processing"></a>Coarse-to-fine pre-processing</h3><p>先粗粒度，再细粒度：对于权重异常值直接截断，<strong>对于激活异常值，每个channel进行缩放。</strong></p><p><img src="/images/2024714/7.png" alt=""></p><h3 id="LoRA-Rounding-for-weight-quantization"><a href="#LoRA-Rounding-for-weight-quantization" class="headerlink" title="LoRA-Rounding for weight quantization"></a>LoRA-Rounding for weight quantization</h3><p>当将模型压缩为低位宽时，权重量化引起的性能下降是不可忽略的。权重的量化误差来自于舍入误差和裁剪误差，但之前针对LLM的PTQ方法只关注后者的优化而没有考虑考虑舍入误差。 文章通过可学习矩阵 V 和修正 sigmoid 函数获得权重舍入矩阵 ΔW：</p><p><img src="/images/2024714/8.png" alt=""></p><p>考虑到对于大模型V太大，这里文章提出了lora的方法即：</p><p><img src="/images/2024714/9.png" alt=""></p><p>并带有正则化损失：</p><p><img src="/images/2024714/10.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>所有实验在单卡GPU实现：</p><p><img src="/images/2024714/11.png" alt=""></p><h1 id="2-（图像编码不要用2）"><a href="#2-（图像编码不要用2）" class="headerlink" title="2 （图像编码不要用2）"></a>2 （图像编码不要用2）</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Efficient Low-Dimensional Compression of Overparameterized Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年3月</td></tr><tr><td>代码</td><td><a href="https://github.com/soominkwon/comp-deep-nets">https://github.com/soominkwon/comp-deep-nets</a></td></tr><tr><td>压缩技术</td><td>微调动力学，深度矩阵分解</td></tr></tbody></table></div><h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>作者观察到，对于许多深度模型，权重矩阵的<strong>更新</strong>发生在低维不变子空间内。对于深度线性模型，文章证明了它们的主要组件在一个小子空间内增量地拟合，并利用这些见解提出了一种深度线性网络的压缩算法，其中涉及减少中间层的宽度。文章观察到压缩网络比原始网络<strong>收敛得更快</strong>，<strong>始终产生更小的恢复错误</strong>。</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Implicit Regularization in Deep Matrix Factorization</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2019</td></tr><tr><td>代码</td><td><a href="https://github.com/roosephu/deep_matrix_factorization">https://github.com/roosephu/deep_matrix_factorization</a></td></tr><tr><td>压缩技术</td><td>微调动力学，深度矩阵分解</td></tr></tbody></table></div><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>RPTQ: Reorder-based Post-training Quantization for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023年5</td></tr><tr><td>代码</td><td><a href="https://github.com/hahnyuan/RPTQ4LLM">https://github.com/hahnyuan/RPTQ4LLM</a></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h2><p>量化大语言模型的挑战来自跨channel的不同范围，文章引入RPTQ量化方法，基于重新排序的方法，重新排列各个channel并按照簇进行量化。</p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>概括图：</p><p><img src="/images/2024714/41.png" alt=""></p><h3 id="Channel的重新排序和聚类"><a href="#Channel的重新排序和聚类" class="headerlink" title="Channel的重新排序和聚类"></a>Channel的重新排序和聚类</h3><p>采用k-mean聚类然后讲统一集群中的channel聚类在一起，每个簇使用相同的缩放因子S和零点Z。</p><h3 id="Avoid-Explicit-Reordering-and-Misalignment"><a href="#Avoid-Explicit-Reordering-and-Misalignment" class="headerlink" title="Avoid Explicit Reordering and Misalignment"></a>Avoid Explicit Reordering and Misalignment</h3><p>重排序操作融合到层规范操作中：</p><p><img src="/images/2024714/42.png" alt=""></p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/43.png" alt=""></p><h1 id="5"><a href="#5" class="headerlink" title="5"></a>5</h1><h2 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>OneBit: Towards Extremely Low-bit Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年4</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h2><p>通过矩阵分解将大语言模型量化到1位。</p><h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><h3 id="1-bit-Linear-Layer-Architecture"><a href="#1-bit-Linear-Layer-Architecture" class="headerlink" title="1-bit Linear Layer Architecture"></a>1-bit Linear Layer Architecture</h3><p>文章(BitNet: Scaling 766 1-bit transformers for large language models)从零开始训练1位模型，在W1A16模型中，线性层被设计为：</p><p><img src="/images/2024714/51.png" alt=""></p><p>受到启发，使用$Sign$函数对权重进行量化，但是$\mathbf{W}_{\pm1}$缺少浮点数，减少了精度，因此文章引入两个FP16的两个向量：</p><p><img src="/images/2024714/52.png" alt=""></p><h3 id="Sign-Value-Independent-Decomposition"><a href="#Sign-Value-Independent-Decomposition" class="headerlink" title="Sign-Value-Independent Decomposition"></a>Sign-Value-Independent Decomposition</h3><p>对$\mathbf{W}$进行秩为1的矩阵分解：</p><p><img src="/images/2024714/53.png" alt=""></p><p>由于$\mathbf{W}$被提取了符号，因此对$\mathbf{W}$的绝对值进行分解可以采用SVD分解和非负矩阵分解NMF。SVID将线性层重新表述为（这样做比上式占用内存更小）：</p><p><img src="/images/2024714/54.png" alt=""></p><p>文章提出，SVID更近似与原始权重矩阵：</p><p><img src="/images/2024714/55.png" alt=""></p><h3 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h3><p>采用量化感知知识蒸馏进行训练：</p><p><img src="/images/2024714/56.png" alt=""></p><p>隐藏层的误差定义为：</p><p><img src="/images/2024714/57.png" alt=""></p><p>最终的损失函数为：</p><p><img src="/images/2024714/58.png" alt=""></p><h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/59.png" alt=""></p><h1 id="6"><a href="#6" class="headerlink" title="6"></a>6</h1><h2 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LCQ: Low-Rank Codebook based Quantization for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年4</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-4"><a href="#摘要-4" class="headerlink" title="摘要"></a>摘要</h2><p>目前的权重量化大多采用rank-1的码本进行量化，当压缩比例较高时，精度低，文章提出了一种基于低秩码本的量化方法。</p><h2 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h2><p>有校准数据集，每个校准数据$\mathbf{X}_i\in \R^{L\times D} $的sequence length为$L$,feature dimension 为$D$：</p><p><img src="/images/2024714/61.png" alt=""></p><p>在一些工作比如AWQ、OminiQuant中，采用分组量化，具体而言。对于一个线性层有$N_W$个权重，将所有权重分成$N_V$个子集，每个子集有$N_W/N_V$个权重。对于每一个子集，将其分成$N_G$个组。指定每个子集的权重为：$\mathbf{W}\in \R^{N_{G}\times G}$,其中$G$表示组大小并且$N_G=\frac{N_W}{N_V\times G}$表示每个子集的权重组数量。需要为每组权重维护一个包含量化值的量化码本。</p><p>对于$b$比特量化，每组权重的所有量化值个数为:$N_Q=2^b$。定义所有组的量化码本为：$\mathbf{C}\in \R^{N_G\times N_Q}$,第$i$行是第$i$组权重对应的码本。量化函数以逐元素的方式对权重进行操作，将每个权重量化为码本中最接近的量化值。令$\mathbf{Z}\in[0,1, \dots,N_Q]^{N_G \times G}$表示权重的量化索引，即$Z_{i,j}$表示$W_{i,j}$的量化索引：</p><p><img src="/images/2024714/62.png" alt=""></p><p>量化函数被定义为：</p><p><img src="/images/2024714/63.png" alt=""></p><p><img src="/images/2024714/64.png" alt=""></p><h3 id="Low-Rank-Codebook"><a href="#Low-Rank-Codebook" class="headerlink" title="Low-Rank Codebook"></a>Low-Rank Codebook</h3><p>在AWQ、GPTQ中，量化值的间隔是相等的，因此不需要显式存储量化码本$\mathbf{C}$,具体来说$\mathbf{C}$可以被两个向量表示，一个缩放向量$\mathbf{S}_1\in \R^{1\times N_G} $（根据$\mathbf{W}$自适应计算）和一个固定点集向量$\mathbf{V}_1\in \R^{1\times N_Q} $（在-1到1之间均匀分布的向量）。</p><p><img src="/images/2024714/65.png" alt=""></p><p>然后通过计算$\mathbf{S}_1$和$\mathbf{V}_1$的外积得到码本。然后为了由于权重的不对称分布，引入一个量化偏移$\mathbf{B}\in \R^{N_G \times 1} $，每组权重有他们各自的偏移。最终的量化码本被表示为：</p><p><img src="/images/2024714/66.png" alt=""></p><p>因此存储为rank-1。为了提升表示能力，这里使用low-rank表示，引入两个矩阵：</p><p><img src="/images/2024714/67.png" alt=""></p><p><img src="/images/2024714/68.png" alt=""></p><p>量化码本被表示为：</p><p><img src="/images/2024714/69.png" alt=""></p><p><img src="/images/2024714/610.png" alt=""></p><h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>使用Transformer块的输出重建误差作为目标函数：</p><p><img src="/images/2024714/611.png" alt=""></p><p><img src="/images/2024714/612.png" alt=""></p><h3 id="Gradient-based-Optimization"><a href="#Gradient-based-Optimization" class="headerlink" title="Gradient-based Optimization"></a>Gradient-based Optimization</h3><h5 id="Gradient-Approximation-for-Quantization-Function"><a href="#Gradient-Approximation-for-Quantization-Function" class="headerlink" title="Gradient Approximation for Quantization Function"></a>Gradient Approximation for Quantization Function</h5><p>$argmin(·)$函数无法反向传播，根据Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation这篇文章，</p><p>将量化函数重写为多个片段相加的形式，每个片段对应两个相邻量化值之间的间隔：</p><p><img src="/images/2024714/613.png" alt=""></p><p>训练过程：这里训练过程中W不变。</p><p><img src="/images/2024714/616.png" alt=""></p><h5 id="Reparameterization-of-Quantization-Parameters"><a href="#Reparameterization-of-Quantization-Parameters" class="headerlink" title="Reparameterization of Quantization Parameters"></a>Reparameterization of Quantization Parameters</h5><p>在梯度下降中，直接学习S很难，主要原因是这些值在不同层之间会有很大的变化，直接学习S导致某些权重梯度更新过多。</p><p>根据OmniQuant文章，提出了重新参数化策略：</p><p><img src="/images/2024714/614.png" alt=""></p><h5 id="Initialization-of-Quantization-Parameters"><a href="#Initialization-of-Quantization-Parameters" class="headerlink" title="Initialization of Quantization Parameters"></a>Initialization of Quantization Parameters</h5><p>使用AWQ方法来初始化S、B、V。</p><h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>由于rank高，存储高了，这里对S、b、v进行了二次量化：</p><p><img src="/images/2024714/615.png" alt=""></p><h2 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h2><p>RTX A6000 GPU card of 48GB</p><p><img src="/images/2024714/617.png" alt=""></p><h1 id="7（一般）"><a href="#7（一般）" class="headerlink" title="7（一般）"></a>7（一般）</h1><h2 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>QQQ: Quality Quattuor-Bit Quantization for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年7</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-5"><a href="#摘要-5" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出四位权重和8位激活的量化方法</p><h2 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h2><p>采用smoothquant那种方法，只对异常通道进行缩放。采用GPTQ方法对权重进行量化</p><p>但是文章设计了W4A8加速内核：</p><p><img src="/images/2024714/71.png" alt=""></p><h1 id="8"><a href="#8" class="headerlink" title="8"></a>8</h1><h2 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LeanQuant: Accurate Large Language Model Quantization with Loss-Error-Aware Grid</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024年7</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-6"><a href="#摘要-6" class="headerlink" title="摘要"></a>摘要</h2><p>以GPTQ为基础，个根据GPTQ的最优解来寻找量化网格。</p><h2 id="方法-5"><a href="#方法-5" class="headerlink" title="方法"></a>方法</h2><h3 id="Revisiting-the-Loss-Error"><a href="#Revisiting-the-Loss-Error" class="headerlink" title="Revisiting the Loss Error"></a>Revisiting the Loss Error</h3><p>文章发现以GPTQ方法的最优误差正比于海森矩阵的逆的倒数和量化的均方误差：</p><p><img src="/images/2024714/81.png" alt=""></p><h3 id="Loss-Error-Aware-Network-Quantization"><a href="#Loss-Error-Aware-Network-Quantization" class="headerlink" title="Loss-Error-Aware Network Quantization"></a>Loss-Error-Aware Network Quantization</h3><p>一般来说均匀量化网格是均匀间隔的，无法保持异常权重的量化精度，文章提出了非均匀的损失误差量化，根据以下目标来学习b位量化的网格线集合：</p><p><img src="/images/2024714/82.png" alt=""></p><p>文章的方法是采用k-means方法来学习网格线集合。</p><h2 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/83.png" alt=""></p><h1 id="9"><a href="#9" class="headerlink" title="9"></a>9</h1><h2 id="简介-8"><a href="#简介-8" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>DB-LLM: Accurate Dual-Binarization for Efficient LLMs</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>量化</td></tr></tbody></table></div><h2 id="摘要-7"><a href="#摘要-7" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出新的二值化方法，在微观层面，同时考虑了2位宽的精度优势和二值化的效率优势，将二位量化权重拆分为两个独立的二进制，然后提出偏差感知蒸馏方法。</p><h2 id="方法-6"><a href="#方法-6" class="headerlink" title="方法"></a>方法</h2><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>对于b位均匀量化，量化和反量化可以表示为：</p><p><img src="/images/2024714/91.png" alt=""></p><p>权重二值化可以表示为：</p><p><img src="/images/2024714/92.png" alt=""></p><h3 id="Flexible-Dual-Binarization"><a href="#Flexible-Dual-Binarization" class="headerlink" title="Flexible Dual Binarization"></a>Flexible Dual Binarization</h3><p>研究人员发现语言模型的权重呈对称高斯分布，并且一小部分显著权重对量化性能至关重要。文章对多低位视角的优化进行了深入研究（图4）。二值化表示能力较差，其余两个水平向 0 收敛（图 3 ），忽略了显着权重并归因于最高损失值。2 位量化自然地克服了表示瓶颈（表达跨度超过图 3 中二值化的两倍）。最小损失点显着降低，但损失面依然陡峭，给优化带来了难度。</p><p><img src="/images/2024714/93.png" alt="图三：第一个输出投影矩阵 (LLaMA-1-7B) 的分布。彩色级别表示网格搜索的最佳解决方案，最大限度地减少二值化、2 位量化和 FDB 的代理量化误差（输出的 MSE 损失）。受权值分布正态性的影响，二值化由于不存在代表0的级别，将两个级别压缩得更接近0，从而阻碍了大量数值较高的重要权值的精确表示，其表达跨度还不到2位的一半量化"></p><p><img src="/images/2024714/94.png" alt="图四：基于二值化 (a)、2 位量化 (b) 和我们的 FDB (c) 的单个量化线性层的损失情况。对于(a)、(b)和(c)，我们扰动单层的训练参数并计算MSE损失，将量化层的输出与全精度模型的输出进行比较。 (d) 通过将三个表面并置在一个坐标框架内来突出它们之间的差异。"></p><p>为了结合二值化固有的效率和2位量化的灵活表示能力，提出了FDB，首先从高位llm中继承高性能获得初始化，然后微调增强表示能力。将2位llm拆分为两个独立的1位：</p><p><img src="/images/2024714/95.png" alt=""></p><p>为了实现等式4中量化级别之间的等距步长，将二值化调整为{0，1}。$\alpha_1$和$\alpha_2$初始值表示为：</p><p><img src="/images/2024714/96.png" alt=""></p><p>微调阶段，量化参数$\alpha_1$和$\alpha_2$会被优化，从而导致非等距量化水平。目标是比较图5中值和水平中心之间的大小（<strong>这里公式6为什么不和$\alpha_1 /2$比大小呢？解释如下</strong>）：</p><p><img src="/images/2024714/910.png" alt=""></p><p><img src="/images/2024714/97.png" alt=""></p><p><img src="/images/2024714/99.png" alt="图五"></p><p>$\mathbf{H}(.)$表示单位阶跃函数，复制定义为0，正值定义为1。FDP前向过程表示为：</p><p><img src="/images/2024714/98.png" alt=""></p><h3 id="Discussion-on-compression-and-acceleration"><a href="#Discussion-on-compression-and-acceleration" class="headerlink" title="Discussion on compression and acceleration."></a>Discussion on compression and acceleration.</h3><p>这样做会产生大量的稀疏性（约百分之60）潜在增加了计算速度。</p><h3 id="Discussion-on-flexibility"><a href="#Discussion-on-flexibility" class="headerlink" title="Discussion on flexibility."></a>Discussion on flexibility.</h3><p>从图四可以看到，FDB具有更平坦的优化表面。</p><h3 id="Deviation-aware-Distillation"><a href="#Deviation-aware-Distillation" class="headerlink" title="Deviation-aware Distillation"></a>Deviation-aware Distillation</h3><p>·文章观察到全精度llm的同一侧偏好服从下图的长尾分布，并表现出增加的失真。文章探究失真原因，对失效预测进行探讨，利用信息熵衡量相应的不确定性：</p><p><img src="/images/2024714/911.png" alt=""></p><p>如下图所示，师生模型的熵于任务损失（交叉熵是一致的），文章假设处理模糊样本时，量化模型有效性会下降，导致倾向于更保守的预测。文章提出偏差感知蒸馏：</p><p><img src="/images/2024714/912.png" alt=""></p><p>利用一对熵即师生熵作为难度指标来优先考虑不确定样本：</p><p><img src="/images/2024714/913.png" alt=""></p><p>总损失为：</p><p><img src="/images/2024714/914.png" alt=""></p><h2 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h2><p>8 NVIDIA A800 GPUs with 80 GB memory. Compressing a 7B model approximately requires 20 GPU hours.</p><p><img src="/images/2024714/915.png" alt=""></p><h1 id="10"><a href="#10" class="headerlink" title="10"></a>10</h1><h2 id="简介-9"><a href="#简介-9" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>BitNet: Scaling 1-bit Transformers for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td></tr></tbody></table></div><h2 id="摘要-8"><a href="#摘要-8" class="headerlink" title="摘要"></a>摘要</h2><p>提出BitNet，可扩展和稳定的1为transformer架构模型，目的时从0开始训练1位权重</p><h2 id="方法-7"><a href="#方法-7" class="headerlink" title="方法"></a>方法</h2><p>BitNet架构如下图：使用BitLinear代替传统的矩阵乘法，采用二值化模型权重。保留其他组件的高精度</p><p><img src="/images/2024714/101.png" alt=""></p><h3 id="BitLinear"><a href="#BitLinear" class="headerlink" title="BitLinear"></a>BitLinear</h3><p>首先使用Sign函数将权重二值化为$±1$,在二值化之前将权重集中为零均值，二值化之后采用比例因子$\beta$来减少实值和二值化后的误差：</p><p><img src="/images/2024714/102.png" alt=""></p><p>文章进一步将激活量化到b位精度，使用absmax量化方法，缩放激活到$<a href="Q_b=2^{b-1}">-Q_b,Q_b</a>$，通过与$Q_b$相乘并除以矩阵的绝对最大值:</p><p><img src="/images/2024714/103.png" alt=""></p><p>对于非线性函数之前的激活，例如（ReLU）,通过减去输入最小值将其缩放到$[0,Q_b]$以便所有值都是非负的：</p><p><img src="/images/2024714/104.png" alt=""></p><p>文章将激活量化为8位，为了稳定性和效率，在训练期间执行每张量量化，在推理期间执行每token量化，矩阵乘法为：</p><p><img src="/images/2024714/105.png" alt=""></p><p>假设$W$和$x$互相独立，则输出的估计方差为：</p><p><img src="/images/2024714/106.png" alt=""></p><p>对于全精度计算，使用标准初始化方法（例如 Kaiming 初始化或 Xavier 初始化），输出 Var(y) 的方差为 1，这对训练稳定性有很大好处。为了保留量化后的方差，我们在激活量化之前引入了 LayerNorm [BKH16] 函数。这样，输出 y 的方差估计为 Var(y) ≈ E[LN(ex)2] = 1，其与全精度对应项 Var(y) 具有相同的幅度，因此BitLinear公式为：</p><p><img src="/images/2024714/107.png" alt=""></p><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>直通估计器。为了训练我们的 1 位模型，我们使用直通估计器 (STE)[BLC13] 来近似反向传播期间的梯度。此方法在向后传递过程中绕过不可微函数，例如 Sign（等式 2）和 Clip（等式 5）函数。 STE 允许梯度流过网络，而不受这些不可微函数的影响，从而可以训练我们的量化模型。</p><ol><li><strong>低精度量化</strong>：在训练过程中，权重（weights）和激活（activations）被量化为低精度格式。低精度通常指的是16位浮点数（例如FP16）或者更低的精度，如8位整数（INT8）。这样做可以减少内存占用和加速计算，因为低精度数值需要的存储空间和计算资源更少。</li><li><strong>高精度存储</strong>：然而，为了确保训练的稳定性和准确性，梯度（gradients）和优化器状态（optimizer states）仍然以高精度格式存储。高精度通常指的是32位浮点数（FP32），这种格式提供了更高的数值精度，有助于减少训练过程中的数值误差。</li><li><strong>潜在权重</strong>：根据之前的研究工作[LSL+21]，对于可学习的参数，我们维护一个高精度的潜在权重（latent weight）。这个潜在权重用于累积参数更新，以确保参数更新的精度。</li><li><strong>前向传播中的二值化</strong>：在模型的前向传播过程中，潜在权重会被即时二值化（binarized），即转换为二进制形式（通常是1位，即-1或+1）。这种二值化可以进一步减少计算复杂度和内存需求。</li><li><strong>推理过程</strong>：值得注意的是，这些二值化的权重并不用于模型的推理过程。推理时，模型通常使用全精度的权重，以确保输出的准确性。</li></ol><h2 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/108.png" alt=""></p><h1 id="11"><a href="#11" class="headerlink" title="11"></a>11</h1><h2 id="简介-10"><a href="#简介-10" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Compressible Dynamics in Deep Overparameterized Low-Rank Learning &amp; Adaptation</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td></tr></tbody></table></div><h1 id="12"><a href="#12" class="headerlink" title="12"></a>12</h1><h2 id="简介-11"><a href="#简介-11" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>DATA-AWARE LOW-RANK COMPRESSION FOR LARGE NLP MODELS</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2024</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td></tr></tbody></table></div><h1 id="13"><a href="#13" class="headerlink" title="13"></a>13</h1><h2 id="简介-12"><a href="#简介-12" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Extreme Compression of Large Language Models via Additive Quantization</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="14"><a href="#14" class="headerlink" title="14"></a>14</h1><h2 id="简介-13"><a href="#简介-13" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Few-Shot Diffusion Models Escape the Curse of Dimensionality</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="15"><a href="#15" class="headerlink" title="15"></a>15</h1><h2 id="简介-14"><a href="#简介-14" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Few-Shot Diffusion Models Escape the Curse of Dimensionality</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="16"><a href="#16" class="headerlink" title="16"></a>16</h1><h2 id="简介-15"><a href="#简介-15" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="17"><a href="#17" class="headerlink" title="17"></a>17</h1><h2 id="简介-16"><a href="#简介-16" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Literature survey on low rank approximation of matrices∗</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="18"><a href="#18" class="headerlink" title="18"></a>18</h1><h2 id="简介-17"><a href="#简介-17" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-9"><a href="#摘要-9" class="headerlink" title="摘要"></a>摘要</h2><p>训练后量化在低位量化中失效，文章提出了一种无数据蒸馏方法，利用预训练模型产生的输出，保留了原始输出的分布。文章量化了权重激活和KV缓存。</p><h2 id="方法-8"><a href="#方法-8" class="headerlink" title="方法"></a>方法</h2><p>概述：</p><p><img src="/images/2024714/181.png" alt="图1"></p><p>量化感知训练存在的挑战：</p><ul><li>选择合适的微调数据很重要，微调数据太窄，或者与原始的预训练分布不同，可能会损害模型性能，</li><li>LLM训练规模和复杂性，很难准确复制其原始的训练设置</li></ul><h3 id="Data-free-Distillation"><a href="#Data-free-Distillation" class="headerlink" title="Data-free Distillation"></a>Data-free Distillation</h3><ol><li><strong>方法概述</strong>：<ul><li>该方法通过从原始预训练模型中生成下一个标记（token）来合成数据。</li><li>如图1(a)所示，首先从词汇表中随机选择一个起始标记 <code>&lt;start&gt;</code>，然后让预训练模型生成下一个标记 <code>&lt;out1&gt;</code>。</li><li>生成的标记 <code>&lt;out1&gt;</code> 被附加到起始标记 <code>&lt;start&gt;</code> 上，用于生成新的输出 <code>&lt;out2&gt;</code>。</li><li>这个迭代过程重复进行，直到达到句子结束标记或达到最大生成长度。</li></ul></li><li><strong>采样策略</strong>：<ul><li>最直接的方法是选择排名第一的候选标记作为下一个标记，但这种方法生成的句子缺乏多样性，并且可能会循环重复某些标记。</li><li>为了解决这个问题，采用了从预训练模型的 SoftMax 输出中随机采样下一个标记的方法，这种方法基于概率分布，生成的句子更加多样化，并且显著提高了微调学生模型的准确性。</li></ul></li><li><strong>关键发现</strong>：<ul><li>研究发现，最初的几个标记对预测趋势起着至关重要的作用，因此这些标记需要有较高的置信度。</li><li>在生成过程中，采用了一种混合采样策略：对于前3到5个标记，确定性地选择排名第一的预测，而对于剩余的标记，则随机采样。</li></ul></li></ol><h3 id="Quantization-Aware-Training"><a href="#Quantization-Aware-Training" class="headerlink" title="Quantization-Aware Training"></a>Quantization-Aware Training</h3><h5 id="Preliminaries-1"><a href="#Preliminaries-1" class="headerlink" title="Preliminaries"></a>Preliminaries</h5><p>对于最小最大量化，公式可以表述为：</p><p><img src="/images/2024714/182.png" alt=""></p><p>对于对称量化：</p><p><img src="/images/2024714/183.png" alt=""></p><p>对于非对称量化：</p><p><img src="/images/2024714/184.png" alt=""></p><p>现在的工作一般采用剪切的量化：</p><p><img src="/images/2024714/185.png" alt=""></p><h5 id="Quantization-for-Large-Language-Models"><a href="#Quantization-for-Large-Language-Models" class="headerlink" title="Quantization for Large Language Models"></a>Quantization for Large Language Models</h5><p><img src="/images/2024714/186.png" alt="图2"></p><p>上图为量化的模型，由于权重和激活中存在显著的异常值，他们对量化精度有影响，在量化过程中修剪这些异常值会导致精度下降，因此文章选择保留这些异常值，而且文章发现激活权重大多是对称分布的，因此选择MinMax量化：</p><p><img src="/images/2024714/187.png" alt=""></p><p>为了确保高效的量化，文章采用per-token激活量化和per-channel权重量化。同时对KV缓存采用激活量化的类似方法</p><p>文章使用交叉熵的蒸馏训练量化网络：</p><p><img src="/images/2024714/188.png" alt=""></p><p><em><u>QAT的关键思想是在模型训练过程中引入量化的操作，让模型“意识”到量化过程，并通过反向传播优化模型参数，以适应量化带来的影响。具体来说，QAT遵循以下步骤：</u></em></p><ol><li><em><u>模拟量化：在模型的前向传播过程中，将权重和激活值通过量化和反量化的过程，模拟量化在实际部署中的效果。这意味着，权重和激活值先被量化到低位宽的整数表示，然后再被反量化回浮点数，以供后续的计算使用。</u></em></li><li><em><u>梯度近似：由于量化操作（如取整）是不可微分的，为了在反向传播过程中计算梯度，QAT采用了梯度近似的技术。常见的方法包括直接通过量化操作传递梯度（即假设量化操作的梯度为1）或使用“直通估计”（Straight Through Estimator, STE）。</u></em></li><li><em><u>优化参数：通过模拟量化的前向传播和梯度近似的反向传播，模型参数在训练过程中得到优化，使模型适应量化后的表示。</u></em></li></ol><p><em><u>QAT优点：</u></em></p><ol><li><em><u>减少量化损失：由于QAT在训练过程中考虑了量化的影响，它可以显著减少量化对模型精度的负面影响，相比于PTQ，通常能够获得更好的性能。</u></em></li><li><em><u>提高模型兼容性：QAT使模型适应了量化后的权重和激活值的分布，从而提高了模型在特定硬件上的兼容性和运行效率。</u></em></li><li><em><u>灵活性和适应性：QAT允许开发者根据目标平台的特定需求，调整量化方案（如量化位宽、量化策略等），优化模型的性能。</u></em></li></ol><h2 id="实验-7"><a href="#实验-7" class="headerlink" title="实验"></a>实验</h2><p>All of our experiments are conducted using a single 8-gpu training node.</p><p><img src="/images/2024714/189.png" alt=""></p><h1 id="19"><a href="#19" class="headerlink" title="19"></a>19</h1><h2 id="简介-18"><a href="#简介-18" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="20"><a href="#20" class="headerlink" title="20"></a>20</h1><h2 id="简介-19"><a href="#简介-19" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td><a href="https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization">https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization</a></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-10"><a href="#摘要-10" class="headerlink" title="摘要"></a>摘要</h2><p>一般的非均匀量化忽略了复杂的投影过程，这在硬件部署中会产生不可忽略的时间和开销，文章提出了非均匀到均匀量化N2UQ方法。通过学习灵活的不等距输入阈值来拟合底层分布，同时i将这些实值输入量化为等距输出水平。为了训练具有可学习输入阈值的量化网络，文章提出了G-STE。此外文章考虑保持熵的正则化，进一步减少权值量化中的信息损失。</p><p>(a) 先前的非均匀量化函数以等距级别输出权重和激活，这需要将浮点级别映射到二进制数字的后处理，以获得量化的加速效果[1,13,50] 。 (b) 所提出的 N2UQ 学习输入阈值以提供更大的灵活性，同时输出统一的量化值，从而实现硬件友好的线性映射和高效的按位运算。棘手的梯度计算输入阈值通过提出的广义直通估计器（G-STE）来解决</p><p><img src="/images/2024714/201.png" alt="图1"></p><h2 id="方法-9"><a href="#方法-9" class="headerlink" title="方法"></a>方法</h2><h3 id="Preliminaries-2"><a href="#Preliminaries-2" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>均匀量化实现矩阵乘法加速：</p><p><img src="/images/2024714/202.png" alt=""></p><h3 id="Nonuniform-to-Uniform-Quantization"><a href="#Nonuniform-to-Uniform-Quantization" class="headerlink" title="Nonuniform-to-Uniform Quantization"></a>Nonuniform-to-Uniform Quantization</h3><ul><li>量化网络加速的前提是量化后的权重和激活可以用二进制表示</li></ul><p><img src="/images/2024714/203.png" alt=""></p><ol><li><strong>均匀量化与非均匀量化</strong>：<ul><li><strong>均匀量化</strong>：当激活$ a_q$和权重$ w_q$ 被均匀量化时，它们可以很容易地通过线性映射转换为二进制表示。这意味着每个量化级别之间的间隔是相等的。</li><li><strong>非均匀量化</strong>：对于非均匀量化的 $a_q$和 $w_q$，情况则不同。非均匀量化意味着量化级别之间的间隔不相等。</li></ul></li><li><strong>非均匀量化的挑战</strong>：<ul><li>非均匀量化的输出实际上是 $2^n$ 个不等距的浮点值（其中 $n$ 是量化位数），如图1所示。</li><li>将这些非均匀量化的值转换为$ n$ 位二进制数字通常需要额外的操作或者使用查找表（LUTs）。</li></ul></li></ol><ul><li>量化器的特性：</li></ul><ol><li><strong>量化函数</strong>：量化函数$ x_q = F_Q(x_r)$描述了将输入$ x_r$(原始信号）转换为输出 $x_q$（量化后的信号）的过程。</li><li><strong>均匀量化级别</strong>：均匀量化级别指的是量化后的输出 $x_q$的各个级别之间的间隔是相等的。</li><li><strong>量化器设计的灵活性</strong>：这段话指出，通过适当的量化器设计，可以实现输出 $x_q$的均匀量化级别，而不必要求输入$ x_r$ 的各个范围也是均匀的。换句话说，即使输出 $x_q$ 的量化级别是均匀的，输入 $x_r$的量化范围也可以是非均匀的。</li></ol><p>总结来说，这段话强调了量化器设计的一个关键点：即使输出是均匀量化的，输入的量化范围也可以是非均匀的。这表明量化器的设计具有一定的灵活性，可以根据具体需求来调整输入和输出的量化特性。</p><h5 id="Forward-Pass-Threshold-Learning-Quantization"><a href="#Forward-Pass-Threshold-Learning-Quantization" class="headerlink" title="Forward Pass: Threshold Learning Quantization"></a>Forward Pass: Threshold Learning Quantization</h5><p>基于上述观察，文章开发了用于激活量化的非均匀到均匀量化器，其前向传递函数为：</p><p><img src="/images/2024714/204.png" alt=""></p><h5 id="Backward-Pass-Generalized-Straight-Through-Estimator-G-STE"><a href="#Backward-Pass-Generalized-Straight-Through-Estimator-G-STE" class="headerlink" title="Backward Pass: Generalized Straight-Through Estimator (G-STE)"></a>Backward Pass: Generalized Straight-Through Estimator (G-STE)</h5><p>反向传播的困难：</p><ol><li>上述公式对于输入$x_r$的倒数都是0</li><li>关于阈值参数的梯度计算是难以处理的</li></ol><p>对于问题1，以前的量化工作采用直通估计器STE来近似量化函数的反向梯度：</p><p><img src="/images/2024714/205.png" alt=""></p><p>这个简单的近似函数非常适合均匀量化器。然而，STE 在量化器的输入和输出间隔中隐式强制执行等轴纵横比，因为它将量化函数视为向后传递中的恒等函数。这阻碍了量化器设计在固定输出电平的同时允许可学习的输入阈值。STE在均匀量化中效果良好，但它隐含地强制了输入和输出区间在量化函数中具有相同的轴向比例。因为在反向传播时，STE将量化函数视为恒等函数，这阻碍了量化器设计中允许学习输入阈值同时固定输出水平的能力。因此文章提出了广义直通估计器（原文推导）</p><p><strong>引理1</strong>.在二值化中，前向确定性二值化函数的梯度近似的直通估计（STE）可以从随机二值化函数的期望中导出[19]。</p><p>在随机二值化中，实值变量根据与-1和1的距离随机二值化：</p><p><img src="/images/2024714/206.png" alt=""></p><p>这里$\tilde{x}_{i,l}^{b}$表示随机的二元变量，为了更新$W_{ij,l}$，通过随机二值化函数计算预期梯度：</p><p><img src="/images/2024714/207.png" alt=""></p><p><em><u>推导</u></em>：</p><p><img src="/images/2024714/209.png" alt=""></p><p>这里$\mathbb{E}$,$\mathbb{E}_{\tilde{x}_{i,l}^{b}}$和$\mathbb{E}_{/\tilde{x}_{i,l}^{b}}$分别表示对整个网络，仅随机二元量化和除随机二元变量之外的期望，有：</p><p><img src="/images/2024714/208.png" alt=""></p><p>这里得到了二值化函数的估计，在阈值附件相同地传递梯度，在实值输入距离阈值太远时忽略梯度。而且，采用确定性二值化函数可以在等式1中设置概率阈值p=0.5来获得：</p><p><img src="/images/2024714/210.png" alt=""></p><p>为此，我们证明 STE 将随机二值化的期望编码为前向确定性二值化函数的后向近似。</p><p><strong>引理 2</strong>. 量化函数$x^q$ 可以被视为具有不同阈值的二值化函数$x_b$的求和,如下图所示：</p><p><img src="/images/2024714/211.png" alt=""></p><p><img src="/images/2024714/212.png" alt=""></p><p>从第一个量化段开始，将初始点表示为$s$，它的长度表示为$a_1$。遵循随机二值化的概念，在$[s,s+a_1]$范围内，真实值变量可以随机量化为0/1，概率与其到$s/(s+a_1)$的距离称正比（这里将$s=-1,a_1=2$带入，正好为公式4）：</p><p><img src="/images/2024714/213.png" alt=""></p><p>类似于公式6，该量化段的推导可以根据等式6的期望来计算：</p><p><img src="/images/2024714/214.png" alt="公式9"></p><p>这样，阈值参数$a_1$对网络的影响很好的被编码在后向逼近公式9中，在前向传播中，不需要随机种子，而是采用确定性量化，将概率阈值设为0.5：</p><p><img src="/images/2024714/215.png" alt=""></p><p><strong>定理1</strong>:Generalized straight-through estimator:</p><p><img src="/images/2024714/216.png" alt=""></p><p>非均匀到均匀量化器的后向梯度近似函数:</p><p><img src="/images/2024714/217.png" alt=""></p><p>并且：</p><p><img src="/images/2024714/218.png" alt=""></p><h5 id="Entropy-Preserving-Weight-Regularization"><a href="#Entropy-Preserving-Weight-Regularization" class="headerlink" title="Entropy Preserving Weight Regularization"></a>Entropy Preserving Weight Regularization</h5><p>文章提出了权重正则化，鼓励量化权重具有更多的信息承载能力。一个重要的观察是，真实网络中的权重数量级小，但是量化后的权重通常会在$[-1,1]$范围内扩展，这种幅度上的不匹配将导致量化权重崩溃到接近于零的几个量化级别。如下图所示，导致信息损失：</p><p><img src="/images/2024714/219.png" alt=""></p><p>从信息论的角度来看，当权重包含更多熵时，可以保留更多信息，文章因此在量化器之前对真实权重进行正则化，来获得量化权重的最大熵：</p><p><img src="/images/2024714/220.png" alt=""></p><p>这里$p_i$时被量化到第$i$个量化级别的真实权重的比例，N是总的量化级别的数量。基于朗格朗日乘数：</p><p><img src="/images/2024714/221.png" alt=""></p><p>当实值权重被量化到多个量化级别的比例相等时，量化后的权重中的信息熵达到最大值。</p><p><img src="/images/2024714/222.png" alt=""></p><h2 id="实验-8"><a href="#实验-8" class="headerlink" title="实验"></a>实验</h2><p>卷积神经网络上的实验</p><h1 id="21"><a href="#21" class="headerlink" title="21"></a>21</h1><h2 id="简介-20"><a href="#简介-20" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>OMNIQUANT: OMNIDIRECTIONALLY CALIBRATED QUANTIZATION FOR LARGE LANGUAGE MODELS</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-11"><a href="#摘要-11" class="headerlink" title="摘要"></a>摘要</h2><p>训练后量化在低bit量化性能低，文章提出了OmniQuant，包括两个组件：</p><ul><li>可学习权重裁剪（LWC）:通过优化裁剪阈值来调节权重极值</li><li>可学习等效变换（LET）：将量化挑战从激活转移到权重来解决激活异常</li></ul><p>OmniQuant在逐块误差最小化的可微框架中运行，可以有效优化仅权重量化和权重激活量化的量化过程。</p><h2 id="方法-10"><a href="#方法-10" class="headerlink" title="方法"></a>方法</h2><p>大语言模型量化的两个困难：</p><ul><li>异常channel的存在，导致激活很难量化，考虑到激活是平坦且均匀的，可以将激活量化难度转移到权重量化熵；</li><li>权重的量化误差：可以以全精度保留这些权重</li></ul><p>文章提出可微分量化技术，学习量化参数。</p><h3 id="BLOCK-WISE-QUANTIZATION-ERROR-MINIMIZATION"><a href="#BLOCK-WISE-QUANTIZATION-ERROR-MINIMIZATION" class="headerlink" title="BLOCK-WISE QUANTIZATION ERROR MINIMIZATION"></a>BLOCK-WISE QUANTIZATION ERROR MINIMIZATION</h3><p>之前基于梯度的PTQ量化方法误差应用到数十亿模型中，文章提出了逐块量化误差最小化的设计，目标为：</p><p><img src="/images/2024714/2101.png" alt=""></p><p>这里$\mathcal{F}$表示transformer块的映射函数，$\Theta_{1,2}$分别是LWC和LET中的量化参数，逐块量化一个变压器的参数然后进入下一个。</p><h3 id="LEARNABLE-WEIGHT-CLIPPING"><a href="#LEARNABLE-WEIGHT-CLIPPING" class="headerlink" title="LEARNABLE WEIGHT CLIPPING"></a>LEARNABLE WEIGHT CLIPPING</h3><p>传统方法中，剪切阈值是直接学习的，而在LWC中，剪切强度被视为一个可学习的参数，通过训练过程来优化。这意味着LWC方法可能会更加灵活，能够根据数据和任务的特性来自适应地调整剪切强度，以获得更好的性能：</p><p><img src="/images/2024714/2102.png" alt=""></p><p>其中$\gamma \in[0,1]$和$\beta \in[0,1]$属于可学习系数。文章通过sigmoid函数实例化$\gamma$和$\beta $。且$\Theta_1 ={\gamma,\beta}$</p><p><img src="/images/2024714/2103.png" alt=""></p><h3 id="LEARNABLE-EQUIVALENT-TRANSFORMATION"><a href="#LEARNABLE-EQUIVALENT-TRANSFORMATION" class="headerlink" title="LEARNABLE EQUIVALENT TRANSFORMATION"></a>LEARNABLE EQUIVALENT TRANSFORMATION</h3><p>一个线性层可以表示为：</p><p><img src="/images/2024714/2104.png" alt=""></p><p>最后，对变换后的激活和权重进行量化，如下所示</p><p><img src="/images/2024714/2105.png" alt=""></p><p>$Q_a$表示普通的MinMax量化，$Q_w$是具有LEC的MinMax量化。文章在 LLM 的所有线性层中都采用了这种等效变换，除了 FFN 的第二个线性层，如下图 所示。这可能是因为非线性层之后特征的高度稀疏性导致应用可学习的等效变换时梯度不稳定。</p><p><img src="/images/2024714/2106.png" alt=""></p><p>除了线性层之外，注意力操作也占计算的很大一部分。此外，LLM 的自回归模式需要为每个令牌存储键值 (KV) 缓存，这会导致长序列需要大量内存。因此，文章还在权重激活量化设置中将 Q/K/V 矩阵量化为低位。具体来说，自注意力亲和力矩阵的可学习等效变换可以写为：</p><p><img src="/images/2024714/2107.png" alt=""></p><p>$\Theta_2 ={\delta,s,s_a}$</p><h2 id="实验-9"><a href="#实验-9" class="headerlink" title="实验"></a>实验</h2><p>Single A100-40G GPU，1-16h</p><p><img src="/images/2024714/2108.png" alt=""></p><h1 id="22"><a href="#22" class="headerlink" title="22"></a>22</h1><h2 id="简介-21"><a href="#简介-21" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE MODELS</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="23"><a href="#23" class="headerlink" title="23"></a>23</h1><h2 id="简介-22"><a href="#简介-22" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td><a href="https://github.com/Cornell-RelaxML/QuIP">https://github.com/Cornell-RelaxML/QuIP</a></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="摘要-12"><a href="#摘要-12" class="headerlink" title="摘要"></a>摘要</h1><p>文章引入一种新的量化与非相干处理（QuIP）方法其原理是，量化过程可以从非相干的权重和Hessian矩阵中获益。具体来说，这意味着权重的大小是均匀的，并且需要精确舍入的方向与坐标轴不对齐。</p><p>“非相干性”（incoherence）通常指的是两个或多个信号或向量在相位、方向或频率上的随机分布，它们之间没有固定的相位关系或对齐。在QuIP（Quantization with Incoherence Processing）的上下文中，权重的非相干性和海森矩阵的非相干性可以这样理解：</p><ol><li><strong>权重的非相干性</strong>：<ul><li>在深度学习中，权重是神经网络中的参数，它们决定了输入特征如何影响输出。</li><li>权重的非相干性意味着这些权重在不同的方向上没有固定的相位关系，它们的分布是随机的或无序的。物理上，这可以类比于一个多粒子系统，其中每个粒子的运动方向是随机的，没有一个统一的模式。</li></ul></li><li><strong>海森矩阵的非相干性</strong>：<ul><li>海森矩阵是二阶导数矩阵，用于描述函数的曲率，即权重参数的局部变化如何影响损失函数。</li><li>海森矩阵的非相干性意味着矩阵中的元素（即二阶导数）在不同的方向上没有固定的模式或对齐。物理上，这可以类比于一个多维空间中的力场，其中力的方向在空间中是随机分布的，没有一个统一的方向。</li></ul></li></ol><p>QuIP包含两个步骤：</p><ol><li>自适应舍入过程：一个自适应舍入来最小化损失函数</li><li>高效的预处理和后处理：通过随机正交矩阵的乘法确保权重和hessian不相干性</li></ol><h2 id="方法-11"><a href="#方法-11" class="headerlink" title="方法"></a>方法</h2><p>问题定义：</p><p>遵循现有得到训练后量化方法，最小化代理目标来舍入权重：</p><p><img src="/images/2024714/2301.png" alt="公式1"></p><p>这个公式中$W$是权重矩阵，这样可是让量化在神经元之间并行运算。</p><h3 id="LDLQ-An-Optimal-Adaptive-Rounding-Method"><a href="#LDLQ-An-Optimal-Adaptive-Rounding-Method" class="headerlink" title="LDLQ: An Optimal Adaptive Rounding Method"></a>LDLQ: An Optimal Adaptive Rounding Method</h3><p>文章定义一系列自适应舍入策略来优化目标方程，针对$k=1,2,\dots,n$次迭代，对权重进行更新：</p><p><img src="/images/2024714/2302.png" alt=""></p><p>这里$W_k$表示第$k$列，$W_{1:(k-1)}$表示前$k-1$列，$\mathcal{Q}$表示最近舍入或者标准无偏舍入到整数（即$\mathbf{E}[\mathcal{Q}(z)=z]$），$a_k\in \R^{k-1}$是一些向量序列。</p><p>该设计方案一次对一列进行四舍五入，每一步中，都会添加一个校正项，最终可以表示为：</p><p><img src="/images/2024714/2303.png" alt=""></p><p>这里$U$是严格的上三角矩阵，它的列向量是$a_k$，$\mathcal{Q}$按元素进行操作。</p><p>令$\eta$表示$\mathcal{Q}$的量化误差：</p><p><img src="/images/2024714/2304.png" alt=""></p><p>可以推导出（把公式（2）带入即可）：</p><p><img src="/images/2024714/2305.png" alt=""></p><p>可以将目标公式重写为：</p><p><img src="/images/2024714/2306.png" alt=""></p><p><strong>The LDLQ Method</strong> 如何选择$U$？如果选择对$H$进行LDL分解：</p><p><img src="/images/2024714/2307.png" alt=""></p><p><img src="/images/2024714/2308.png" alt=""></p><h3 id="Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure"><a href="#Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure" class="headerlink" title="Deriving the Optimality of the LDLQ Adaptive Rounding Procedure"></a>Deriving the Optimality of the LDLQ Adaptive Rounding Procedure</h3><p>文章推理了LDLQ的最优性，考虑代理损失的最差和平均情况，令$\mathcal{A}$表示舍入方法，令$\mathcal{A}(W,H)$表示量化的结果，则最坏和平均代理损失定义为：</p><p><img src="/images/2024714/2309.png" alt=""></p><p><strong>定理1</strong>LDLQ在舍入方法中是最差和平均情况下最优的：</p><p><img src="/images/2024714/2310.png" alt=""></p><p>对于非QuIP框架的，设置$U=0$则比较$tr(D)$和$tr(H)$实验发现：</p><p><img src="/images/2024714/2311.png" alt=""></p><h3 id="Incoherence-Optimality-with-a-Spectral-Bound"><a href="#Incoherence-Optimality-with-a-Spectral-Bound" class="headerlink" title="Incoherence: Optimality with a Spectral Bound"></a>Incoherence: Optimality with a Spectral Bound</h3><p>文章观察到H为低秩，是否可以使用H的频谱来限制LQLQ的行为，从而限制$tr(D)$</p><p><img src="/images/2024714/2312.png" alt=""></p><p><img src="/images/2024714/2313.png" alt=""></p><p><strong>定理2</strong></p><p><img src="/images/2024714/2314.png" alt=""></p><p>文章通过不相干处理来获得$tr(D)$的界限，该界限取决于H频谱。根据下列公式可以发现，H的秩k越小，LDLQ损失越小：</p><p><img src="/images/2024714/2315.png" alt=""></p><h2 id="Quantization-With-Incoherence-Processing-Incoherence-Processing-Step"><a href="#Quantization-With-Incoherence-Processing-Incoherence-Processing-Step" class="headerlink" title="Quantization With Incoherence Processing: Incoherence Processing Step"></a>Quantization With Incoherence Processing: Incoherence Processing Step</h2><p>经过上述分析，不相干有利于降低精度，文章接下来预处理权重和hession矩阵，确保不相干属性。使对称矩阵不相干的一种直接方法是通过均匀随机正交矩阵将其共轭。令$U\in \R^{m×m}$,$V\in \R^{n×n}$通过随机正交乘法确保权重和海森矩阵不相干：</p><p><img src="/images/2024714/2316.png" alt=""></p><p>注意这种变化没有改变目标函数的形式：</p><p><img src="/images/2024714/2317.png" alt=""></p><h3 id="Incoherence-via-Efficient-Orthogonal-Multiplication"><a href="#Incoherence-via-Efficient-Orthogonal-Multiplication" class="headerlink" title="Incoherence via Efficient Orthogonal Multiplication"></a>Incoherence via Efficient Orthogonal Multiplication</h3><p>上述对于存储不会引入开销，但是如果对于推理则会产生额外的开销。文章将 $n$ 分解为两个近似相等的因子 $p$和 $q$，其中 $p$ 和 $q$都近似等于 $n$的平方根。然后设置 $U$ 为 $U_L$ 和 $U_R$的克罗内克积（Kronecker product），其中 $U_L$是从 $p \times p$正交矩阵中均匀采样的，$U_R$是从 $q \times q$正交矩阵中均匀采样的。通过将向量$x$重塑为一个$p \times q$的矩阵，可以完成$ x$与矩阵 $U$的乘法。左侧乘以 $U_L$，在右侧乘以 $U_R$的转置$ U^T_R$，然后再重塑回来。</p><p><img src="/images/2024714/2318.png" alt=""></p><h2 id="Additional-Heuristics"><a href="#Additional-Heuristics" class="headerlink" title="Additional Heuristics"></a>Additional Heuristics</h2><p><img src="/images/2024714/2319.png" alt=""></p><h2 id="实验-10"><a href="#实验-10" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2320.png" alt=""></p><h1 id="24"><a href="#24" class="headerlink" title="24"></a>24</h1><h2 id="简介-23"><a href="#简介-23" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>R2 Loss: Range Restriction Loss for Model Compression and Quantization</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-13"><a href="#摘要-13" class="headerlink" title="摘要"></a>摘要</h2><p>为了提高量化精度，文章专注于权重的异常值。文章提出范围限制损失，通过在预训练期间从权重中删除异常值来构建低位量化。在训练期间提出了三种不同的损失作为辅助损失。文章指出：$L_{\infty}$损失和$Margin \ R^2$损失对于对称量化非常有效，而$Soft-Min-Max \ R^2$损失显示出更好的模型压缩性能 。</p><h2 id="方法-12"><a href="#方法-12" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/2024714/2405.png" alt=""></p><p>文章引入范围限制损失作为辅助损失，以减少每一层权重的范围，以获得更好的预训练模型来进一步的压缩或者量化。与传统的$L_1$或$L_2$正则化不同，Range Restriction Loss专门针对权重中的异常值（outliers），通过惩罚这些异常值来限制它们的范围。文章从$L_{\infty}$损失扩展到margin 损失，最后引入soft-min-max损失将$R^2$损失。</p><p>$L_{\infty}\ R^2 \ \text{loss}$：该方法通过在训练过程中添加$L_{\infty}(W)$作为模型中每一层辅助损失，以迭代方式惩罚离群值：</p><p><img src="/images/2024714/2401.png" alt=""></p><p>$\text{Margin} \ R^2 \ \text{loss}$：这是上面的扩展，文章对允许的权重范围定义了一个余量，任何超出此界限的权重都会受到处罚，文章还对边距的宽度进行处罚，来确保整体权重分布范围较小：</p><p><img src="/images/2024714/2402.png" alt=""></p><p>$\text{Soft-min-max} \ R^2 \ \text{loss}$：这是不对称的损失，消除对全权重大小的约束并严格执行权重范围：</p><p><img src="/images/2024714/2403.png" alt=""></p><p>其中温度$\alpha$是可学习参数。这种损失不仅会平滑地惩罚离群值，而且还会惩罚接近离群值的权重。</p><p>各种损失的可视化如下：</p><p><img src="/images/2024714/2404.png" alt=""></p><h2 id="实验-11"><a href="#实验-11" class="headerlink" title="实验"></a>实验</h2><p>卷积神经网络</p><p>eight GPUs</p><p><img src="/images/2024714/2406.png" alt=""></p><h1 id="25"><a href="#25" class="headerlink" title="25"></a>25</h1><h2 id="简介-24"><a href="#简介-24" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h1 id="26"><a href="#26" class="headerlink" title="26"></a>26</h1><h2 id="简介-25"><a href="#简介-25" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td><a href="https://github.com/mit-han-lab/smoothquant">https://github.com/mit-han-lab/smoothquant</a></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-14"><a href="#摘要-14" class="headerlink" title="摘要"></a>摘要</h2><p>现有的模型量化无法同时保持精度和硬件效率，文章提出了SmoothQuant，这是一种无需训练、保持精度的训练后量化方法，SmoothQuant通过使用数字等效转化将量化难度从激活转移到权重来离线平滑激活异常值。</p><h2 id="方法-13"><a href="#方法-13" class="headerlink" title="方法"></a>方法</h2><p>量化将高精度值映射到离散级别。文章研究整数均匀量化，量化过程可以表示为（这里是per-tensor量化）：</p><p><img src="/images/2024714/2601.png" alt=""></p><p>通过校准样本的激活来离线计算$\Delta$即静态量化，使用激活的运行时统计来获得$\Delta$即动态量化，并且量化具有不同的粒度级别：</p><p><img src="/images/2024714/2602.png" alt=""></p><h3 id="Review-of-Quantization-Difficulty"><a href="#Review-of-Quantization-Difficulty" class="headerlink" title="Review of Quantization Difficulty"></a>Review of Quantization Difficulty</h3><ol><li>激活比权重更难量化：激活存在异常值</li><li>异常值使激活量化变得困难</li><li>异常值持续存在于固定渠道中：异常值出现在一小部分channel中，会持续出现在所有token中，但是跨token的给定通道的幅度之间方差很小。由于异常值存在，而在每个channel的方差很小，如果对激活执行怕per-channel量化，则量化误差很小，下表为模拟结果：</li></ol><p><img src="/images/2024714/2604.png" alt=""></p><p><img src="/images/2024714/2603.png" alt=""></p><p>但是per-channel量化无法很好的映射到硬件加速的GEMM内核。在这些内核中，只能沿着矩阵乘法的外部维度（激活的token维度，权重的输出通道）进行缩放。</p><p><img src="/images/2024714/2605.png" alt=""></p><p>可以用下式表示：</p><p><img src="/images/2024714/2606.png" alt=""></p><h3 id="SmoothQuant"><a href="#SmoothQuant" class="headerlink" title="SmoothQuant"></a>SmoothQuant</h3><p>对激活进行缩放，对权重进行反缩放：</p><p><img src="/images/2024714/2607.png" alt=""></p><p>选择缩放因子，平滑权重量化和激活量化：</p><p><img src="/images/2024714/2608.png" alt=""></p><p>应用位置：</p><p><img src="/images/2024714/2609.png" alt=""></p><h2 id="实验-12"><a href="#实验-12" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2610.png" alt=""></p><h1 id="27"><a href="#27" class="headerlink" title="27"></a>27</h1><h2 id="简介-26"><a href="#简介-26" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td><a href="https://github.com/Vahe1994/SpQR">https://github.com/Vahe1994/SpQR</a></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-15"><a href="#摘要-15" class="headerlink" title="摘要"></a>摘要</h2><p>文章提出系数量化表示（SpQR）：识别和隔离导致特别大的量化误差的离群权重，并高精度存储他们，同时将其他权重压缩为3-4位。</p><h2 id="方法-14"><a href="#方法-14" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/2024714/2701.png" alt=""></p><p>识别异常值，对非常异常值采用小分组量化，对小分组量化的系数采用二次量化。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/images/2024714/2702.png" alt=""></p><h1 id="28"><a href="#28" class="headerlink" title="28"></a>28</h1><h2 id="简介-27"><a href="#简介-27" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div><h2 id="摘要-16"><a href="#摘要-16" class="headerlink" title="摘要"></a>摘要</h2><p>将权重变为三元组{-1，0，1}，从头开始训练语言模型</p><h2 id="方法-15"><a href="#方法-15" class="headerlink" title="方法"></a>方法</h2><p>文章基于BitNet方法，相对于BitNet修改有：</p><h3 id="Quantization-Function"><a href="#Quantization-Function" class="headerlink" title="Quantization Function"></a>Quantization Function</h3><p>为了将权重限制在-1,0,1。文章采用$absmean$量化方法，首先按权重矩阵的平均绝对值缩放，然后将每个值舍入为{-1，0，1}中最接近的整数：</p><p><img src="/images/2024714/2801.png" alt=""></p><p>激活量化函数遵循Bitnet中的相同实现，文章没有将非线性函数之前的激活缩放到范围$[0,Q_b]$，而是每个激活都缩放为$[-Q_b,Q_b]$，以消除零点量化</p><h2 id="实验-13"><a href="#实验-13" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/2024714/2802.png" alt=""></p><h1 id="29"><a href="#29" class="headerlink" title="29"></a>29</h1><h2 id="简介-28"><a href="#简介-28" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td></td></tr><tr><td>代码</td><td></td></tr><tr><td>技术</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>quip</title>
      <link href="/2024/04/10/quip/"/>
      <url>/2024/04/10/quip/</url>
      
        <content type="html"><![CDATA[<h1 id="Quip-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#Quip-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="Quip:2-Bit Quantization of Large Language Models With Guarantees"></a>Quip:2-Bit Quantization of Large Language Models With Guarantees</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>作者发现当权重和代理Hessian矩阵不相关时，量化过程更容易进行，因为权重的变化不会受到不同方向上的重要性差异的影响，从而减少了舍入误差的可能性，提高了量化的效率。具体来说，文章引入了不相干处理量化，包含两个步骤：1、自适应舍入过程。2、处理权重矩阵和海森矩阵不相干</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p> 略</p><h2 id="3-Quantization-With-Incoherence-Processing-Adaptive-Rounding-Step"><a href="#3-Quantization-With-Incoherence-Processing-Adaptive-Rounding-Step" class="headerlink" title="3 Quantization With Incoherence Processing: Adaptive Rounding Step"></a>3 Quantization With Incoherence Processing: Adaptive Rounding Step</h2><p>自适应舍入目标函数：</p><p><img src="/images/quip/1.png" alt="公式1"></p><p><strong>推导</strong></p><p><img src="/images/quip/2.png" alt="公式1推导"></p><h3 id="3-1-LDLQ-An-Optimal-Adaptive-Rounding-Method"><a href="#3-1-LDLQ-An-Optimal-Adaptive-Rounding-Method" class="headerlink" title="3.1 LDLQ: An Optimal Adaptive Rounding Method"></a>3.1 LDLQ: An Optimal Adaptive Rounding Method</h3><p>对于$k=1,2\dots n:$迭代更新${W}$:</p><p><img src="/images/quip/3.png" alt=""></p><p>$W_k$表示第$k$列，$W_{1:(k-1)}$表示前$k-1$列，$\mathcal{Q}$表示对整数的最近舍入或者标准无偏摄入：$E[\mathcal{Q}(z)]=z$并且$a_k\in R^{k-1}$</p><p>每一次迭代，都会添加一个<strong>校正项</strong>，”校正”项是指在每一步迭代中，根据到目前为止的舍入结果与原始数据的差异（残差），计算出的一个线性函数。这个函数的作用是对当前列进行微调，以使得舍入结果更接近原始数据，或者满足特定的优化目标。具体地说，对于第 $k$ 列 $ \hat{W}_k $ ，修正项  $(W_{1:(k-1)} - \hat{W}_{1:(k-1)})a_k$是由前 $k-1$ 列舍入结果与原始数据的差异（即残差）乘以一个向量 $a_k$ 得到的。这个向量控制着对当前列的修正程度，可以看作是一个权重向量，用于调整前面列的残差对当前列的影响。</p><p>最终得到：</p><p><img src="/images/quip/4.png" alt="公式2"></p><p>$U$是一个严格的上三角矩阵，他的列是向量$a_k$.</p><p>定义$\mathcal{Q}$的量化误差：</p><p><img src="/images/quip/30.png" alt=""></p><p>计算得到：</p><p><img src="/images/quip/5.png" alt=""></p><p><strong>推理：</strong></p><script type="math/tex; mode=display">\hat{W}=\eta+(W+(W-\hat{W})U)\\\\\hat{W}-W-(W-\hat{W})U=\eta\\\\\hat{W}-W-WU+\hat{W}U=\hat{W}(I+U)-W(I+U)=(\hat{W}-W)(I+U)=\eta</script><p>重写目标函数为：</p><p><img src="/images/quip/6.png" alt="公式3"></p><p>LDLQ方法：<br>对于上面的公式，如何确定$U$，根据重建的目标函数，令$U=\acute{U}$，对$H$进行LDL分解得到$\acute{U} $（它可以将一个对称正定矩阵A分解为一个下三角矩阵L、一个对角矩阵D和一个下三角矩阵L的转置的乘积，即$A = LDL^T$。其中，L是单位下三角矩阵（对角线元素为1，其它元素的下三角部分可能不为0，上三角部分全为0），D是对角线元素为正数的对角矩阵。：</p><p><img src="/images/quip/7.png" alt="公式4"></p><h3 id="3-2-Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure"><a href="#3-2-Deriving-the-Optimality-of-the-LDLQ-Adaptive-Rounding-Procedure" class="headerlink" title="3.2 Deriving the Optimality of the LDLQ Adaptive Rounding Procedure"></a>3.2 Deriving the Optimality of the LDLQ Adaptive Rounding Procedure</h3><p>考虑最差和平均情况，令$\mathcal{A}$表示舍入方法，令$\mathcal{A}(W,H)$表示量化的权重，接下来定义最坏情况和平均情况：</p><p><img src="/images/quip/8.png" alt=""></p><p><strong>定理1：</strong>在将线性反馈 U规定为 H 的函数，并且舍入到整数时，LDLQ是最差情况和平均情况下最优的舍入方法。对于所有正半定 H，并且对于 $\mathcal{Q}$ 作为最近舍入或随机舍入：</p><p><img src="/images/quip/9.png" alt=""></p><p><strong>定理1证明：</strong></p><p>令$X $为与舍入过程 $\mathcal{A}$ 相关的严格上三角矩阵，使得方程中的$ U ← X$,令$B \equiv (X+I)^{-1}(\acute{U}+I)$，损失为：</p><p><img src="/images/quip/10.png" alt="公式9"></p><p>如果$U$是通过LDL分解得到的，则：</p><p><img src="/images/quip/11.png" alt="公式10"></p><p>首先，考虑最坏情况损失，$L_{\text{worst}}$。目标是构造一个特别糟糕的情况，其中$\tilde{W}$的条目为$\frac{1}{2} \pm \epsilon$，因此当舍入到整数时，我们总是会有误差$\frac{1}{2}$。构造一个权重矩阵$\tilde{W} \in \mathbb{R}^{m \times n}$，使得每个条目满足，$ \tilde{W}_{ij} = \frac{1}{2} \pm \epsilon $</p><p><img src="/images/quip/13.png" alt=""></p><p>$\eta\in \R^{m,n}$,最坏情况的损失为,无论是随机还是最近舍入都有：</p><p><img src="/images/quip/14.png" alt=""></p><p><strong>推导:</strong></p><script type="math/tex; mode=display">\text{tr}(\eta BDB^{T}\eta^T)=\text{tr}(\eta \eta^TBDB^{T}),\eta \eta^T为m*m矩阵，对角元素为\eta _i^2,所以tr(\eta \eta^T)=m(1/2(0.5)^2+1/2(-0.5)^2)</script><p>因此有：</p><p><img src="/images/quip/15.png" alt=""></p><p>对于LDLQ，最坏的舍入误差也是0.5，因此：</p><script type="math/tex; mode=display">\mathcal{L}_{worst(LDLQ,H)}=\frac{m}{4}\text{tr}(D)</script><p>$B \equiv (X+I)^{-1}(\acute{U}+I)$,$X+I$是单位上三角矩阵$\acute{U}+I$是单位下三角矩阵，当$B=I$时，$\frac{m}{4}\text{tr}(BDB^T)_{min}=\frac{m}{4}\text{tr}(D)$，</p><p>因此：</p><p><img src="/images/quip/16.png" alt=""></p><p>接下来证明平均损失：</p><p><img src="/images/quip/17.png" alt=""></p><p>如果 $\mathcal{Q}$是最近舍入，则整个量化误差$\eta \sim Unif[-\frac{1}{2},\frac{1}{2}]$,所以：</p><p><img src="/images/quip/18.png" alt=""></p><p>最终得到：</p><script type="math/tex; mode=display">\mathcal{L}_{avg}(\mathcal{A},H)=E_{W\sim Unif[0,1]^{m×n}}[tr(\eta BDB^T\eta ^T)]=\frac{m}{12}tr(BDB^T)</script><p>如果 $\mathcal{Q}$是随机舍入，则整个量化误差$\eta \sim Unif[-1,1]$,所以：</p><script type="math/tex; mode=display">E[\eta_{ij}^2]=\int _0^1 x(1-x)dx=\frac{1}{6}</script><p>注意误差为$x$的概率为$(1-|x|)$,最终得到：</p><p><img src="/images/quip/19.png" alt=""></p><p>所以LDLQ量化的平均损失为：</p><p>最近舍入：</p><script type="math/tex; mode=display">\mathcal{L}_{avg}(LDLQ,H)=\frac{m}{12}tr(D)</script><p>随机舍入：</p><script type="math/tex; mode=display">\mathcal{L}_{avg}(LDL,H)=\frac{m}{6}tr(D)</script><p>所以有：</p><p><img src="/images/quip/31.png" alt=""></p><p><strong>注意：</strong> $\mathcal{Q}$作为最接近的舍入实现了与随机舍入相同的最坏情况代理损失，但实现了更好的平均代理损失。<br>对于这些基线方法，它们与 LDLQ 的最优性差距由 tr (D) 与 tr (H) 决定.(公式4),从OPT125m到2.7B，实验表明，tr(D)/tr(H)&lt;0.65,因此这个差距并非微不足道。</p><h3 id="3-3-Incoherence-Optimality-with-a-Spectral-Bound"><a href="#3-3-Incoherence-Optimality-with-a-Spectral-Bound" class="headerlink" title="3.3 Incoherence: Optimality with a Spectral Bound"></a>3.3 Incoherence: Optimality with a Spectral Bound</h3><p>观察到H是低秩的：</p><p><img src="/images/quip/20.png" alt=""></p><p>绝大多数的 H 矩阵的特征值中，有不到四分之一大于最大特征值的1%，基于对 H 低秩特性的这一观察，我们是否可以利用 H 的谱来限制 LDLQ 的行为，从而限制 tr(D) 的范围呢？</p><p><strong>定义1：</strong></p><p>定义“$\mu$-不一致”的概念，称对称的 Hessian 矩阵 $H$ 是 $\mu$-不一致的，如果它具有特征分解 $H = Q\Lambda Q^T$，使得对于所有的 $i$ 和 $j$，都满足 $|Q_{ij}| = \langle e_i, Qe_j \rangle \leq \frac{\mu}{\sqrt{n}}$。其中，$Q$ 是正交矩阵，$\Lambda$ 是对角矩阵，$e_i$ 是第 $i$ 个单位向量。这里的 $\langle \cdot, \cdot \rangle$ 表示向量的内积。进一步，通过推广，定义了“$\mu$-不一致”的概念，称权重矩阵 $W$ 是 $\mu$-不一致的，如果对于所有的 $i$ 和 $j$，都满足 $|W_{ij}| = \langle e_i, W e_j \rangle \leq \frac{\mu | W |_F}{\sqrt{mn}}$。其中，$| W |_F$ 表示权重矩阵 $W$ 的 Frobenius 范数。换句话说，对称的 Hessian 矩阵 $H$ 是 $\mu$-不一致的，意味着其特征向量 $Q$ 的每一列与单位向量之间的内积的绝对值都不超过 $\frac{\mu}{\sqrt{n}}$。而权重矩阵 $W$ 是 $\mu$-不一致的，则表示每个元素的绝对值与其对应的行向量和列向量的内积之积都不超过 $\frac{\mu | W |_F}{\sqrt{mn}}$ 与权重矩阵的 Frobenius 范数的乘积。</p><p><img src="/images/quip/21.png" alt=""></p><p>为什么在权重矩阵 $W$ 和 Hessian 矩阵 $H$ 中引入不一致性是有益的。首先指出，大多数的 $n \times n$ 矩阵具有与 $\mu = O(\sqrt{\log n}) = \tilde{O}(1)$ 的不一致性，这是因为一个随机正交矩阵的元素的平方大小集中在它们的平均值 $1/n$ 附近。因此，在权重矩阵 $W$ 中引入不一致性可以被视为一种异常值（outlier）减少的形式：对其元素的大小施加一个较小的边界意味着我们不需要那么大程度地对其进行缩放，以使其适应可表示的低精度数的有限范围。Figures 2 和 3 绘制了在 OPT-2.7b 的所有层中，在我们的不一致性处理前后的最大权重和 Hessian 特征向量的绝对值。参考线上的斜率为1。我们看到在我们的不一致性处理应用后，权重矩阵 $W$ 和 Hessian 矩阵 $H$ 更不一致（$\mu 更小$）。虽然使 Hessian 矩阵 $H$ 不一致性不太直观，但其效用由以下引理所激发。</p><p><strong>定理2</strong>：设 $H ∈ \mathbb{R}^{n \times n} $是一个 μ-不一致的半正定对称矩阵，其 $Cholesky$分解为 $H = (\acute{U} + I)D(\acute{U} + I)^T$，其中 $\acute{U} $是一个严格上三角矩阵，D 是一个（非负）对角矩阵,那么：</p><p><img src="/images/quip/22.png" alt=""></p><p><strong>推理</strong>：不会</p><p>这是一项利用不一致性来获得仅依赖于矩阵 H 的谱的迹 $\text{tr}(D)$ 的界限的新颖结果.为了帮助解释这个结果，作者推导了普通最近邻和随机舍入的明确代理损失，然后将其与通过引理 2 获得的 LDLQ 方法进行比较。考虑矩阵 H的秩为 k，并且满足$ \mu^2 k &lt; n$。根据柯西-施瓦茨不等式，我们有$ \text{tr}(H^{1/2})^2 \leq k \text{tr}(H)$,得到:</p><p><img src="/images/quip/23.png" alt=""></p><p>在这个情况下，$B \in \{Near, Stoch\}$，而 c如定理 1 中所示。这表明，对于足够低秩的 H，通过一个与$ \mu^2 k $有关的因子，LDLQ 在渐近意义下优于普通的最近邻和随机舍入。</p><p><strong>没有不相干性：光谱界限没有改善。</strong></p><p><strong>定理4：</strong></p><p>对于任何正半定的矩阵 H，如果考虑所有与 H具有相同谱的矩阵 $\tilde{H}$，那么在最坏情况下的损失上，LDLQ 方法实现的误差与随机舍入方法相同。</p><p><img src="/images/quip/24.png" alt=""></p><p>在平均情况下也是相同的误差：</p><p><img src="/images/quip/25.png" alt=""></p><h2 id="4-Quantization-With-Incoherence-Processing-Incoherence-Processing-Step"><a href="#4-Quantization-With-Incoherence-Processing-Incoherence-Processing-Step" class="headerlink" title="4 Quantization With Incoherence Processing: Incoherence Processing Step"></a>4 Quantization With Incoherence Processing: Incoherence Processing Step</h2><p>使对称矩阵具有不一致性的一种直接方法是将其通过一个均匀随机正交矩阵进行共轭：这将导致其每个特征向量都是一个随机单位向量，其元素将集中在大小为 $n^{-1/2}$附近。</p><p>具体来说，假设 $U \in \mathbb{R}^{m \times m}$ 和 $V \in \mathbb{R}^{n \times n}$ 是两个随机正交矩阵。我们通过随机正交矩阵的乘法来确保权重和 Hessian 矩阵具有较高概率的不一致性，即:</p><p>$\tilde{H} \leftarrow V H V^T$</p><p>$\tilde{W} \leftarrow U W V^T$</p><p>这个转换保持了代理二次形式，因为</p><p>$\text{tr}(\tilde{W}\tilde{H}\tilde{W}^T) = \text{tr}((U W V^T)(V H V^T)(V W^T U^T)) = \text{tr}(W^T H W^T)$</p><h3 id="4-1-Incoherence-via-Efficient-Orthogonal-Multiplication"><a href="#4-1-Incoherence-via-Efficient-Orthogonal-Multiplication" class="headerlink" title="4.1 Incoherence via Efficient Orthogonal Multiplication"></a>4.1 Incoherence via Efficient Orthogonal Multiplication</h3><p>在神经网络上运行推断任务而言，我们需要乘以权重矩阵 W，在这种情况下，需要生成并乘以n<em>×</em>n 的随机正交矩阵 U、V将会是不切实际的。解决方案：</p><p>即使用随机正交矩阵的分布，而不是单独生成随机正交矩阵进行乘法运算。该方法可以保证乘法运算的速度。首先，将 n 分解为 p和 q的乘积，其中 p 和 q大约等于 $\sqrt{n}$。然后，设置 $U = U_L \otimes U_R$，其中 $U_L$ 是从大小为 $p \times p$的正交矩阵集中随机采样，而 $U_R$是从大小为 $q \times q$ 的正交矩阵集中随机采样。对一个向量 $x \in \mathbb{R}^n$乘以矩阵 U 可以通过以下步骤实现：将向量重塑为一个大小为$ p \times q $的矩阵，在左侧乘以$ U_L$，在右侧乘以$ U_R^T$，然后将结果重塑回原始的向量形式。这个过程的时间复杂度为 $O(n(p+q)) = o(n^2)$。</p><p><strong>定理5：</strong></p><p>设 H是一个正半定矩阵，位于 $\mathbb{R}^{n \times n}$，W是一个矩阵，位于 $\mathbb{R}^{m \times n}$并假设 $m = p_1 \cdot p_2 \cdot \ldots \cdot p_k$ 以及 $n = q_1 \cdot q_2 \cdot \ldots \cdot q_k$。$ U_1, U_2, \ldots, U_k$和$ V_1, V_2, \ldots, V_k$是独立的随机正交矩阵，分别位于 $\mathbb{R}^{p_i \times p_i} $和$ \mathbb{R}^{q_i \times q_i}$。将 U 定义为Kronecker积 $U = U_1 \otimes U_2 \otimes \ldots \otimes U_K$，将 V定义为 $V = V_1 \otimes V_2 \otimes \ldots \otimes V_k$(实验中k=2),那么$VHV^T$是$\mu_H$不相关并且概率为$1-\delta$,$UWV^T$是$\mu_W$不相关并且概率为$1-\delta$:</p><p><img src="/images/quip/26.png" alt=""></p><p>如果 A和 B都是正交矩阵，则它们的Kronecker积 $A \otimes B$也是正交矩阵。这是因为正交矩阵的乘积仍然是正交矩阵。Kronecker积的性质会保留矩阵的正交性.</p><h3 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h3><p><img src="/images/quip/27.png" alt=""></p><p><img src="/images/quip/28.png" alt=""></p><p>第 4 行对角重新调整 W 和 H 的比例，以最小化 $\mathcal{L}(\hat {W}) ≈ tr (H) ∥W ∥^2_ F $，有效地权衡这些矩阵的频谱以找到最小值</p><p>受 W 不相干性的影响，第 6 行根据频谱 $∥W ∥^2_ F$ 计算量化范围，而不是典型的 $max_{i,j} |W_{ij}|$。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="/images/quip/29.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>对目标函数进行变体之后发现，目标函数从原来的H的函数变为了D的函数，而且D比H小，如果H不相干，那么变体之后的损失小于变体之前的损失，于是对H进行了不相干处理。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>软件精英挑战赛</title>
      <link href="/2024/03/29/ruanjianjingyingtiaozhansai/"/>
      <url>/2024/03/29/ruanjianjingyingtiaozhansai/</url>
      
        <content type="html"><![CDATA[<h1 id="华为软件精英挑战赛——普朗克计划"><a href="#华为软件精英挑战赛——普朗克计划" class="headerlink" title="华为软件精英挑战赛——普朗克计划"></a>华为软件精英挑战赛——普朗克计划</h1><p>2024华为软件精英挑战赛——普朗克计划</p><p><img src="../images/软件精英挑战赛/1.png" alt=""></p><p>赛题：<a href="https://bbs.huaweicloud.com/forum/thread-0209145106256505005-1-1.html">https://bbs.huaweicloud.com/forum/thread-0209145106256505005-1-1.html</a></p><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><p>题目概括：地图随机生成价值不等的货物，控制10个机器人收集货物到港口；地图中会有10个固定港口存储机器人的货物，控制5艘轮船前往港口搬运货物到销售处获得金额。</p><p>我们队伍的思路是设计多个模块，<strong>运动模块</strong>控制机器人运动，包括上下左右、搬运、放等动作；<strong>目标选择模块</strong>帮助机器人选择目标货物和目标港口；<strong>路径规划模块</strong>帮助机器人进行路径查找；<strong>初始化模块</strong>获取初始化地图信息，预先查找部分路径和获取机器人状态；<strong>运输模块</strong>控制船收集买卖货物；<strong>防碰撞模块</strong>避免多个机器人之间互相碰撞与撞死；</p><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="一、初始化模块"><a href="#一、初始化模块" class="headerlink" title="一、初始化模块"></a><strong>一、初始化模块</strong></h2><p>与判题器交互采用标准输入输出，在初始化阶段，我们获取地图信息，包括：障碍物、船、机器人、港口、机器人状态等信息。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> map_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        line <span class="token operator">=</span> sys<span class="token punctuation">.</span>stdin<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        ch<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># line = input()</span>        <span class="token comment"># ch.append([c for c in line.split(sep=" ")]) # 读取地图</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>berth_num<span class="token punctuation">)</span><span class="token punctuation">:</span>        line <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        berth_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span>sep<span class="token operator">=</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token builtin">id</span> <span class="token operator">=</span> berth_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token operator">=</span> berth_list<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>y <span class="token operator">=</span> berth_list<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time <span class="token operator">=</span> berth_list<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time <span class="token operator">></span> <span class="token number">1500</span><span class="token punctuation">:</span>            berth_final<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">)</span>            berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>choose<span class="token operator">=</span><span class="token number">1</span>            <span class="token comment"># None</span>        berths<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">]</span><span class="token punctuation">.</span>loading_speed <span class="token operator">=</span> berth_list<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>     <span class="token comment">#初始化泊位</span>    boat_capacity <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment">#初始化轮船容积</span>    okk <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment">#初始化完成，读取ok</span>    <span class="token comment">#确定船的坐标</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>berth_num<span class="token punctuation">)</span><span class="token punctuation">:</span>        berth_x<span class="token operator">=</span>berths<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>x        berth_y<span class="token operator">=</span>berths<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>y        berth_con<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>berth_x<span class="token punctuation">,</span>berth_y<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>berth_x<span class="token punctuation">,</span>berth_y<span class="token operator">+</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>berth_x<span class="token operator">+</span><span class="token number">3</span><span class="token punctuation">,</span>berth_y<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>berth_x<span class="token operator">+</span><span class="token number">3</span><span class="token punctuation">,</span>berth_y<span class="token operator">+</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        berth_nums<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>        index <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> con <span class="token keyword">in</span> berth_con<span class="token punctuation">:</span>            nums <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> y <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> x<span class="token operator">==</span><span class="token number">0</span> <span class="token keyword">and</span> y<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>                        <span class="token keyword">continue</span>                    xx <span class="token operator">=</span> con<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">+</span>x<span class="token punctuation">;</span>                    yy <span class="token operator">=</span> con<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">+</span>y<span class="token punctuation">;</span>                    <span class="token keyword">if</span> <span class="token builtin">min</span><span class="token punctuation">(</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token builtin">max</span><span class="token punctuation">(</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">199</span> <span class="token keyword">and</span> ch<span class="token punctuation">[</span>xx<span class="token punctuation">]</span><span class="token punctuation">[</span>yy<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'.'</span><span class="token punctuation">:</span>                        nums <span class="token operator">=</span> nums<span class="token operator">+</span><span class="token number">1</span>            berth_nums<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>index <span class="token punctuation">,</span> nums<span class="token punctuation">]</span><span class="token punctuation">)</span>            index <span class="token operator">=</span> index <span class="token operator">+</span> <span class="token number">1</span>        sorted_num <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>berth_nums<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x <span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        point_1 <span class="token operator">=</span> berth_con<span class="token punctuation">[</span>sorted_num<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        point_2 <span class="token operator">=</span> berth_con<span class="token punctuation">[</span>sorted_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        berths<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token operator">=</span> <span class="token punctuation">(</span>point_1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">+</span>point_2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span>        berths<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token punctuation">(</span>point_1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">+</span>point_2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span>                    obstacles<span class="token operator">=</span>get_obstacles<span class="token punctuation">(</span>ch<span class="token punctuation">)</span>    <span class="token comment"># print(obstacles,file=sys.stderr)</span>        <span class="token keyword">for</span> obstacle <span class="token keyword">in</span> obstacles<span class="token punctuation">:</span>        obstacles_set<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Node<span class="token punctuation">(</span>obstacle<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>obstacle<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> obstacles <span class="token punctuation">,</span>boat_capacity<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="二、目标选择模块"><a href="#二、目标选择模块" class="headerlink" title="二、目标选择模块"></a>二、目标选择模块</h2><h4 id="2-1-定义状态表"><a href="#2-1-定义状态表" class="headerlink" title="2.1 定义状态表"></a>2.1 定义状态表</h4><p>总共10个机器人，每个机器人有7个信息需要存储：<strong><u>目标货物的x坐标</u></strong>、<strong><u>目标货物的y坐标</u></strong>、<u><strong>目标港口的id</strong></u>、<u><strong>机器人此时的状态位</strong></u>（0表示闲置、1表示确定目标货物，需要寻找路径、2表示正在前往目标货物、3表示已经取到货物，需要搜索目标港口的路径、4表示正在前往目标港口）、<strong><u>目标货物的价格</u></strong>、<strong><u>该机器人是否是初始的机器人</u></strong>、<strong><u>上一次的目标港口</u></strong>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">target_table <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span>target_table<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>  <span class="token comment">#最初所有机器人都是初始机器人，当机器人完成第一次寻路之后即不是初始机器人</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="2-2-定义策略"><a href="#2-2-定义策略" class="headerlink" title="2.2 定义策略"></a>2.2 定义策略</h4><p>选择货物的衡量标准是单位时间获得的金额大小即：$p=\frac{货物价值}{初始点到货物的距离+货物到港口的距离}$</p><p>公式中的距离我们采用真实距离，即真实路径的长度。但是如果是初始化的机器人，我们不采用真实路径长短，初始的货物很少，只要是机器人可达货物，即可前往。<strong>需要注意的是，如果某一个货物已经成为机器人的目标，则其他机器热无法选择该货物，我们将该货物到其他机器人的距离设置成99999</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">robot2goods</span><span class="token punctuation">(</span>goods<span class="token punctuation">,</span>robots<span class="token punctuation">,</span>berths<span class="token punctuation">,</span>paths<span class="token punctuation">,</span>now_frame<span class="token punctuation">,</span>robot_to_init_goods<span class="token punctuation">,</span>boat_capacity<span class="token punctuation">,</span>berth_final<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">global</span> target_table    index_good_choosen <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    available_goods <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">)</span>    <span class="token comment">#第一步计算出船厂与所有good的距离</span>    berth2good <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> index_berth<span class="token punctuation">,</span>berth <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>berths<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> index_good <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            berth2good<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">[</span>index_good<span class="token punctuation">]</span> <span class="token operator">=</span> paths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">(</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">[</span>index_good<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>goods<span class="token punctuation">.</span>available_goods<span class="token punctuation">[</span>index_good<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment">#当船厂号在最终不可达船厂号，设置为无限大</span>            ber2good_final <span class="token operator">=</span> berth2good<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> i <span class="token keyword">in</span> berth_final<span class="token punctuation">:</span>            ber2good_final<span class="token punctuation">[</span>i<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>        <span class="token comment">#获得所有物品的死亡帧</span>    death_good <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>available_goods<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1000</span>    <span class="token comment">#获取所有物品价值</span>    good_value <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>available_goods<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment">#获得good到其最近船厂的距离和对应船厂id</span>    good2berth_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>ber2good_final<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#1*n</span>    <span class="token comment"># print(good2berth_id,file=sys.stderr)</span>    good2berth_value <span class="token operator">=</span> np<span class="token punctuation">.</span>amin<span class="token punctuation">(</span>ber2good_final<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#1*n</span>    <span class="token keyword">for</span> index_robot<span class="token punctuation">,</span>robot <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>robots<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment">#判断是否为初始机器人</span>        <span class="token keyword">if</span> target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> robot<span class="token punctuation">.</span>live <span class="token keyword">and</span> np<span class="token punctuation">.</span><span class="token builtin">any</span><span class="token punctuation">(</span>good2berth_value <span class="token operator">&lt;</span> <span class="token number">99999</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment">#获取机器人所在的berth号</span>                berth_id <span class="token operator">=</span> target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>                robot2good <span class="token operator">=</span> berth2good<span class="token punctuation">[</span>berth_id<span class="token punctuation">]</span>                <span class="token comment">#确定机器人是否能拿到货物,不能拿到货的r2g距离设置成99999</span>                RobotArriveTime <span class="token operator">=</span> robot2good <span class="token operator">+</span> now_frame <span class="token operator">+</span> <span class="token number">10</span>                CanNotReach <span class="token operator">=</span> RobotArriveTime <span class="token operator">></span> death_good                robot2good<span class="token punctuation">[</span>CanNotReach<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                all_path_len <span class="token operator">=</span> robot2good <span class="token operator">+</span> good2berth_value                 <span class="token comment">#当最后一轮时要考虑船厂死亡帧</span>                <span class="token keyword">if</span> now_frame <span class="token operator">></span> <span class="token number">12500</span><span class="token punctuation">:</span>                    <span class="token keyword">for</span> ind <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                        list_berths <span class="token operator">=</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>good2berth_id <span class="token operator">==</span> ind<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>list_berths<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>                            need_to_op <span class="token operator">=</span> all_path_len<span class="token punctuation">[</span>list_berths<span class="token punctuation">]</span> <span class="token operator">+</span> now_frame <span class="token operator">+</span> <span class="token number">5</span>                            CanNotReach <span class="token operator">=</span> need_to_op <span class="token operator">></span> berth_death<span class="token punctuation">[</span>ind<span class="token punctuation">]</span>                            cannot <span class="token operator">=</span> list_berths<span class="token punctuation">[</span>CanNotReach<span class="token punctuation">]</span>                            all_path_len<span class="token punctuation">[</span>cannot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                m <span class="token operator">=</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>good_value<span class="token punctuation">,</span> all_path_len<span class="token punctuation">)</span>                Choose_good_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>m<span class="token punctuation">)</span>                Choose_berth_id <span class="token operator">=</span> good2berth_id<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span>                <span class="token comment">#如果选择的还是大，那么不选</span>                <span class="token keyword">if</span> robot2good<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token operator">>=</span><span class="token number">99999</span> <span class="token keyword">or</span> all_path_len<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token operator">>=</span><span class="token number">99999</span><span class="token punctuation">:</span>                    <span class="token keyword">continue</span>                index_good_choosen<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Choose_good_id<span class="token punctuation">)</span>                good2berth_value<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> Choose_berth_id                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token operator">=</span> berth_id        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> robot<span class="token punctuation">.</span>live <span class="token keyword">and</span> np<span class="token punctuation">.</span><span class="token builtin">any</span><span class="token punctuation">(</span>good2berth_value <span class="token operator">&lt;</span> <span class="token number">99999</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                                second_goods2robot<span class="token operator">=</span>robot_to_init_goods<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span>                all_path_len <span class="token operator">=</span> second_goods2robot <span class="token operator">+</span> good2berth_value                m <span class="token operator">=</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>good_value<span class="token punctuation">,</span> all_path_len<span class="token punctuation">)</span>                Choose_good_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>m<span class="token punctuation">)</span>                <span class="token comment">#如果选择的还是大，那么不选</span>                <span class="token keyword">if</span> second_goods2robot<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token operator">>=</span><span class="token number">99999</span> <span class="token keyword">or</span> all_path_len<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token operator">>=</span><span class="token number">99999</span><span class="token punctuation">:</span>                    <span class="token keyword">continue</span>                Choose_berth_id <span class="token operator">=</span> good2berth_id<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span>                index_good_choosen<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Choose_good_id<span class="token punctuation">)</span>                good2berth_value<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> Choose_berth_id                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                target_table<span class="token punctuation">[</span>index_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> available_goods<span class="token punctuation">[</span>Choose_good_id<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> index_good_choosen<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="三、路径规划模块"><a href="#三、路径规划模块" class="headerlink" title="三、路径规划模块"></a>三、路径规划模块</h2><h4 id="3-1-A-算法寻找路径"><a href="#3-1-A-算法寻找路径" class="headerlink" title="3.1 A*算法寻找路径"></a>3.1 A*算法寻找路径</h4><p>机器人路径规划问题中，我们采用常见的A*算法来实现对机器人路径优化。在算法中，我们采用欧氏距离作为启发式函数：</p><script type="math/tex; mode=display">   h=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># A*算法</span><span class="token keyword">def</span> <span class="token function">astar</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> goal<span class="token punctuation">,</span> obstacles<span class="token punctuation">)</span><span class="token punctuation">:</span>    open_set <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    closed_set <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    current <span class="token operator">=</span> start    open_set<span class="token punctuation">.</span>add<span class="token punctuation">(</span>current<span class="token punctuation">)</span>    <span class="token comment"># counter = 1</span>    <span class="token keyword">while</span> open_set<span class="token punctuation">:</span>        <span class="token comment"># counter += 1</span>        <span class="token comment"># if counter >= 10000:</span>        <span class="token comment">#     return None</span>        current <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>open_set<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>g <span class="token operator">+</span> x<span class="token punctuation">.</span>h<span class="token punctuation">)</span>        open_set<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>current<span class="token punctuation">)</span>        closed_set<span class="token punctuation">.</span>add<span class="token punctuation">(</span>current<span class="token punctuation">)</span>        <span class="token keyword">if</span> get_distance<span class="token punctuation">(</span>current<span class="token punctuation">,</span> goal<span class="token punctuation">)</span> <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>            path <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">while</span> current<span class="token punctuation">:</span>                path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current<span class="token punctuation">)</span>                current <span class="token operator">=</span> current<span class="token punctuation">.</span>parent            <span class="token keyword">return</span> path<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>         <span class="token keyword">for</span> neighbor <span class="token keyword">in</span> get_neighbors<span class="token punctuation">(</span>current<span class="token punctuation">,</span> obstacles<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> neighbor <span class="token keyword">in</span> closed_set<span class="token punctuation">:</span>                <span class="token keyword">continue</span>            <span class="token comment"># tentative_g = current.g+ 1/3*get_distance(current, neighbor)</span>            tentative_g <span class="token operator">=</span>  get_distance<span class="token punctuation">(</span>current<span class="token punctuation">,</span> neighbor<span class="token punctuation">)</span>            <span class="token keyword">if</span> neighbor <span class="token keyword">not</span> <span class="token keyword">in</span> open_set<span class="token punctuation">:</span>                open_set<span class="token punctuation">.</span>add<span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span>                neighbor<span class="token punctuation">.</span>h <span class="token operator">=</span> get_distance<span class="token punctuation">(</span>neighbor<span class="token punctuation">,</span> goal<span class="token punctuation">)</span>            <span class="token keyword">elif</span> tentative_g <span class="token operator">>=</span> neighbor<span class="token punctuation">.</span>g<span class="token punctuation">:</span>                <span class="token keyword">continue</span>            neighbor<span class="token punctuation">.</span>parent <span class="token operator">=</span> current            neighbor<span class="token punctuation">.</span>g <span class="token operator">=</span> tentative_g    <span class="token keyword">return</span> <span class="token boolean">None</span><span class="token comment"># 获取当前点的邻居点</span><span class="token keyword">def</span> <span class="token function">get_neighbors</span><span class="token punctuation">(</span>node<span class="token punctuation">,</span> obstacles<span class="token punctuation">)</span><span class="token punctuation">:</span>    neighbors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> y <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> x <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> y <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span><span class="token punctuation">(</span>x<span class="token operator">*</span>y<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">continue</span>            <span class="token keyword">if</span> node<span class="token punctuation">.</span>x <span class="token operator">+</span> x <span class="token operator">&lt;</span><span class="token number">0</span> <span class="token keyword">or</span> node<span class="token punctuation">.</span>x <span class="token operator">+</span> x <span class="token operator">></span><span class="token number">200</span> <span class="token keyword">or</span> node<span class="token punctuation">.</span>y <span class="token operator">+</span> y <span class="token operator">&lt;</span><span class="token number">0</span> <span class="token keyword">or</span> node<span class="token punctuation">.</span>y <span class="token operator">+</span> y <span class="token operator">></span> <span class="token number">200</span> <span class="token keyword">or</span> Node<span class="token punctuation">(</span>                    node<span class="token punctuation">.</span>x <span class="token operator">+</span> x <span class="token punctuation">,</span> node<span class="token punctuation">.</span>y <span class="token operator">+</span> y <span class="token punctuation">)</span> <span class="token keyword">in</span> obstacles<span class="token punctuation">:</span>                <span class="token keyword">continue</span>            neighbor <span class="token operator">=</span> Node<span class="token punctuation">(</span>node<span class="token punctuation">.</span>x <span class="token operator">+</span> x <span class="token punctuation">,</span> node<span class="token punctuation">.</span>y <span class="token operator">+</span> y <span class="token punctuation">)</span>            neighbors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>neighbor<span class="token punctuation">)</span>    <span class="token keyword">return</span> neighbors <span class="token keyword">def</span> <span class="token function">get_distance</span><span class="token punctuation">(</span>node1<span class="token punctuation">,</span> node2<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span>node1<span class="token punctuation">.</span>x <span class="token operator">-</span> node2<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token punctuation">(</span>node1<span class="token punctuation">.</span>y <span class="token operator">-</span> node2<span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_manhadundis</span><span class="token punctuation">(</span>node1<span class="token punctuation">,</span> node2<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>node1<span class="token punctuation">.</span>x <span class="token operator">-</span> node2<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>node1<span class="token punctuation">.</span>y <span class="token operator">-</span> node2<span class="token punctuation">.</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>A*算法查找路径效果如下：</p><p><img src="../images/软件精英挑战赛/2.png" alt="A*"></p><p>A<em>可以很好的动态找到路径，但是经过多次优化，A\</em>查找一次路径的时间还是很长：</p><p><img src="../images/软件精英挑战赛/3.png" alt=""></p><p>考虑到机器人与判题器交互时间上限是15ms，如果多个机器人同时查找路径，那时间显然是不够的。考虑到除了初始化的机器人，即机器人在初始化之后每次运货都是从某个港口出发，再回到某个港口。因此很自然的想到，我们可以存储每个港口到所有位置的所有路径，机器人每次寻找路径只需要查表即可，于是改变思路，我们只在初始化阶段使用A*寻找路径。</p><h4 id="3-2-广度优先搜索遍历，查找港口到所有点的路径。"><a href="#3-2-广度优先搜索遍历，查找港口到所有点的路径。" class="headerlink" title="3.2 广度优先搜索遍历，查找港口到所有点的路径。"></a>3.2 广度优先搜索遍历，查找港口到所有点的路径。</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">is_valid</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> rows<span class="token punctuation">,</span> cols<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> x <span class="token operator">&lt;</span> rows <span class="token keyword">and</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> y <span class="token operator">&lt;</span> cols<span class="token keyword">def</span> <span class="token function">find_paths</span><span class="token punctuation">(</span>initial_positions<span class="token punctuation">,</span> obstacle_coordinates<span class="token punctuation">,</span> rows<span class="token punctuation">,</span> cols<span class="token punctuation">)</span><span class="token punctuation">:</span>    obstacles <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>obstacle_coordinates<span class="token punctuation">)</span>    paths <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    <span class="token keyword">for</span> start_x<span class="token punctuation">,</span> start_y <span class="token keyword">in</span> initial_positions<span class="token punctuation">:</span>        visited <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        queue <span class="token operator">=</span> deque<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>start_x<span class="token punctuation">,</span> start_y<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">while</span> queue<span class="token punctuation">:</span>            current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">,</span> path <span class="token operator">=</span> queue<span class="token punctuation">.</span>popleft<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">)</span> <span class="token keyword">not</span> <span class="token keyword">in</span> visited<span class="token punctuation">:</span>                visited<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">(</span>current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">)</span> <span class="token keyword">not</span> <span class="token keyword">in</span> obstacles<span class="token punctuation">:</span>                    current_path <span class="token operator">=</span> path <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">)</span><span class="token punctuation">]</span>                    paths<span class="token punctuation">[</span><span class="token punctuation">(</span>start_x<span class="token punctuation">,</span> start_y<span class="token punctuation">,</span> current_x<span class="token punctuation">,</span> current_y<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> current_path                    <span class="token keyword">for</span> dx<span class="token punctuation">,</span> dy <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        new_x<span class="token punctuation">,</span> new_y <span class="token operator">=</span> current_x <span class="token operator">+</span> dx<span class="token punctuation">,</span> current_y <span class="token operator">+</span> dy                        <span class="token keyword">if</span> is_valid<span class="token punctuation">(</span>new_x<span class="token punctuation">,</span> new_y<span class="token punctuation">,</span> rows<span class="token punctuation">,</span> cols<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>new_x<span class="token punctuation">,</span> new_y<span class="token punctuation">)</span> <span class="token keyword">not</span> <span class="token keyword">in</span> visited<span class="token punctuation">:</span>                            queue<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>new_x<span class="token punctuation">,</span> new_y<span class="token punctuation">,</span> current_path<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> paths<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样我们只需要在初始化阶段遍历10次地图（因为每个港口都需要遍历一次地图）即可查找10个港口到所有点的路径表。经过我们的计算，在200<em>200的地图中，可达点大约有25000，也就是每个港口我们要存储25000个路径，每个路径是一个长度平局为100的列表，经过我们的计算，这个路径表会占用存储将近<em>*3GB</em></em>。担心比赛的提交系统有存储上限，因此这个方案我们没有使用。</p><h4 id="3-2-路径表转换方向表"><a href="#3-2-路径表转换方向表" class="headerlink" title="3.2 路径表转换方向表"></a>3.2 路径表转换方向表</h4><p>我们为每一个港口存储一个200*200的方向表，每个位置存储01234、0表示该位置无法到达该港口，1表示该位置到达港口则需要机器人向下移动，以此类推如下图：绿色表示港口，黑色表示障碍物：</p><p><img src="../images/软件精英挑战赛/4.png" alt=""></p><p>这样我们只需要存储10个200*200的表格，然后需要路径时，根据方向表格遍历提取路径，这样存储仅为：</p><p><img src="../images/软件精英挑战赛/5.png" alt=""></p><p>在表格中提取路径时间为：</p><p><img src="../images/软件精英挑战赛/6.png" alt="image-20240329150928478"></p><p>同时，在广搜过程中，我们记录方向表和单通道。方向表为查找路径，单通道为防止机器人在单通道撞死。效果如下，红色表示从目标港口到目标货物提取的路径，黄色表示地图中的单通道：</p><p><img src="../images/软件精英挑战赛/7.png" alt=""></p><h2 id="四、运动模块"><a href="#四、运动模块" class="headerlink" title="四、运动模块"></a>四、运动模块</h2><p>简单通过标准输出控制机器人和船运动：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">robots_movation</span><span class="token punctuation">(</span>robot_num<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>robot_num<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"move"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_up</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"move"</span><span class="token punctuation">,</span> robot_id<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_down</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"move"</span><span class="token punctuation">,</span> robot_id<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_left</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"move"</span><span class="token punctuation">,</span> robot_id<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_right</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"move"</span><span class="token punctuation">,</span> robot_id<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_get_goods</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"get"</span><span class="token punctuation">,</span> robot_id<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">robot_pull_goods</span><span class="token punctuation">(</span>robot_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"pull"</span><span class="token punctuation">,</span>robot_id<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">boat_ship</span><span class="token punctuation">(</span>boat_id<span class="token punctuation">,</span>berth_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ship"</span><span class="token punctuation">,</span>boat_id<span class="token punctuation">,</span>berth_id<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">boat_go_virtual</span><span class="token punctuation">(</span>boat_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"go"</span><span class="token punctuation">,</span>boat_id<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="五、防碰撞模块"><a href="#五、防碰撞模块" class="headerlink" title="五、防碰撞模块"></a>五、防碰撞模块</h2><p>防碰撞策略为：单通道进行锁死，非单通道采用优先停止，其次左右最后后退的策略防止机器人碰撞：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">detect_collision_1</span><span class="token punctuation">(</span>paths <span class="token punctuation">,</span> robots <span class="token punctuation">,</span> track_index<span class="token punctuation">,</span> connected_points <span class="token punctuation">,</span> connected_locks<span class="token punctuation">)</span><span class="token punctuation">:</span>        bot_num <span class="token operator">=</span> <span class="token number">10</span>    collision_flag <span class="token operator">=</span> <span class="token boolean">True</span>    reback_status <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">10</span>    now_position <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    robot_next_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment"># 存储机器人的位置信息</span>    robot_flag <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">10</span> <span class="token comment"># 标记机器人下一个执行位置是否合法</span>    robot_next_pos_1 <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    waits <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment"># nums = []</span>    <span class="token comment">#初始化robot_pos</span>    <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bot_num<span class="token punctuation">)</span><span class="token punctuation">:</span>        now_position<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Node<span class="token punctuation">(</span>robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            robot_flag<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            robot_next_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Node<span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            robot_next_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Node<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    robot_next_pos_1 <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>robot_next_pos<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>connected_locks<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">15</span><span class="token punctuation">:</span>        <span class="token comment">#进行单通道的解锁</span>        <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>connected_locks<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> connected_locks<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>                pos <span class="token operator">=</span> Node<span class="token punctuation">(</span>robots<span class="token punctuation">[</span>connected_locks<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>connected_locks<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span>                <span class="token keyword">if</span> pos <span class="token keyword">not</span> <span class="token keyword">in</span> connected_points<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">:</span>                    connected_locks<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment">#进行单通道的上锁</span>        <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bot_num<span class="token punctuation">)</span><span class="token punctuation">:</span>            step <span class="token operator">=</span> <span class="token number">0</span>            index_1 <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">+</span>step<span class="token punctuation">)</span><span class="token punctuation">)</span>            cancel_flag <span class="token operator">=</span> <span class="token boolean">True</span>            <span class="token keyword">if</span> index_1 <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>index_1<span class="token punctuation">]</span> <span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>index_1<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>                pos <span class="token operator">=</span> Node<span class="token punctuation">(</span>paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>index_1<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>index_1<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>connected_points<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> pos <span class="token keyword">in</span> connected_points<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>                        cancel_flag <span class="token operator">=</span> <span class="token boolean">False</span>                        <span class="token keyword">if</span> connected_locks<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> index<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span>                            <span class="token keyword">continue</span>                        <span class="token keyword">elif</span> connected_locks<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                            connected_locks<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> index<span class="token operator">+</span><span class="token number">1</span>                        <span class="token keyword">else</span><span class="token punctuation">:</span>                            <span class="token comment"># track_index[index] = max(0 , track_index[index]-1)</span>                            <span class="token comment"># 单通道门口执行左右移动</span>                            <span class="token keyword">if</span> direction_flag<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                                reback_status<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span>                                left_right_flag <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># 判断其他两个方向有没有空地</span>                                wait_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment">#构建可选点，长度为1或者2</span>                                <span class="token keyword">for</span> <span class="token builtin">dict</span> <span class="token keyword">in</span> direction<span class="token punctuation">:</span>                                    pos <span class="token operator">=</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                                    pos_1 <span class="token operator">=</span> Node<span class="token punctuation">(</span>robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                                    <span class="token keyword">if</span> <span class="token builtin">min</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token builtin">max</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">199</span> <span class="token keyword">and</span> pos <span class="token operator">!=</span> paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> obstacles_set <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> robot_next_pos_1 <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> now_position<span class="token punctuation">:</span> <span class="token comment"># 测试时修改为live_points_1，同时传入</span>                                        <span class="token keyword">if</span> track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token number">2</span> <span class="token keyword">and</span> pos <span class="token operator">==</span> paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                                            <span class="token keyword">continue</span>                                        wait_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>                                        left_right_flag <span class="token operator">=</span> <span class="token boolean">True</span>                                <span class="token keyword">if</span> <span class="token keyword">not</span> left_right_flag<span class="token punctuation">:</span> <span class="token comment"># 没有左右的执行回退</span>                                    track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>                                    reback_status<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>                                <span class="token keyword">else</span><span class="token punctuation">:</span>                                    pos <span class="token operator">=</span> wait_pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                                    waits<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> wait_pos                                    paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>insert<span class="token punctuation">(</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">)</span>                                    paths<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>insert<span class="token punctuation">(</span>track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span> <span class="token punctuation">,</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>                                    reback_status<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># 发生过左右的情况</span>                                direction_flag<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                            <span class="token keyword">else</span><span class="token punctuation">:</span>                                reback_status<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>                                track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>                        <span class="token keyword">break</span>                <span class="token keyword">if</span> cancel_flag<span class="token punctuation">:</span>                    direction_flag<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">while</span><span class="token punctuation">(</span>collision_flag<span class="token punctuation">)</span><span class="token punctuation">:</span>        collision_flag <span class="token operator">=</span> <span class="token boolean">False</span>        bots <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>itertools<span class="token punctuation">.</span>combinations<span class="token punctuation">(</span>robots<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>bots<span class="token punctuation">)</span>        <span class="token keyword">for</span> bot <span class="token keyword">in</span> bots<span class="token punctuation">:</span>            robot_1 <span class="token operator">=</span> bot<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">id</span>            robot_2 <span class="token operator">=</span> bot<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">id</span>            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">or</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">or</span> track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">0</span> <span class="token keyword">or</span> track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">0</span> <span class="token keyword">or</span> track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">or</span> track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">continue</span>                        <span class="token comment"># 两种情况，直接冲突和交换冲突</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">and</span> paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>robots<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">and</span> robots<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">.</span>y <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">and</span> robots<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">.</span>y <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                                target_robot <span class="token operator">=</span> robot_1 <span class="token keyword">if</span> reback_status<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> reback_status<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span> <span class="token keyword">else</span> robot_2                <span class="token comment">#编号小的保持不变，编号大的进行处理，即robot_2，这里规定0代表未发生碰撞，1代表停止，2代表其他两个方向寻路，==3代表回退</span>                reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span>                <span class="token keyword">if</span> reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token comment"># 已经进行过所有避障操作</span>                    <span class="token keyword">continue</span>                <span class="token keyword">elif</span> reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span> <span class="token comment"># 已经产生了左右退的现象，弹出插入的点执行回退</span>                    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>waits<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token comment"># 如果还有备选位置，执行另一个躲避位置</span>                        paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> waits<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                        waits<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                        reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        <span class="token keyword">del</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span>                        track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token keyword">elif</span> reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token comment">#产生相撞于一点</span>                    track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token keyword">elif</span> <span class="token punctuation">(</span>reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>robots<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>robot_2<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">==</span> paths<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>robot_1<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    left_right_flag <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># 判断其他两个方向有没有空地</span>                    wait_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment">#构建可选点，长度为1或者2</span>                    <span class="token keyword">for</span> <span class="token builtin">dict</span> <span class="token keyword">in</span> direction<span class="token punctuation">:</span>                        pos <span class="token operator">=</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        pos_1 <span class="token operator">=</span> Node<span class="token punctuation">(</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        <span class="token keyword">if</span> <span class="token builtin">min</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token builtin">max</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">199</span> <span class="token keyword">and</span> pos <span class="token operator">!=</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> obstacles_set <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> robot_next_pos_1 <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> now_position<span class="token punctuation">:</span> <span class="token comment"># 测试时修改为live_points_1，同时传入</span>                            <span class="token keyword">if</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token number">2</span> <span class="token keyword">and</span> pos <span class="token operator">==</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                                <span class="token keyword">continue</span>                            wait_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>                            left_right_flag <span class="token operator">=</span> <span class="token boolean">True</span>                    <span class="token keyword">if</span> <span class="token keyword">not</span> left_right_flag<span class="token punctuation">:</span> <span class="token comment"># 没有左右的执行回退</span>                        track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>                        reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        pos <span class="token operator">=</span> wait_pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                        waits<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> wait_pos                        paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>insert<span class="token punctuation">(</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">)</span>                        paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>insert<span class="token punctuation">(</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span> <span class="token punctuation">,</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>x <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>                        reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># 发生过左右的情况</span>                <span class="token keyword">elif</span> reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token comment"># 这里是对之前相撞于一点的情况进行处理</span>                    left_right_flag <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># 判断其他两个方向有没有空地</span>                    wait_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment">#构建可选点，长度为1或者2</span>                    <span class="token keyword">for</span> <span class="token builtin">dict</span> <span class="token keyword">in</span> direction<span class="token punctuation">:</span>                        pos <span class="token operator">=</span> <span class="token punctuation">(</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        pos_1 <span class="token operator">=</span> Node<span class="token punctuation">(</span>robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> robots<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token operator">+</span><span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        <span class="token keyword">if</span> <span class="token builtin">min</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token builtin">max</span><span class="token punctuation">(</span>pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">199</span> <span class="token keyword">and</span> pos <span class="token operator">!=</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">and</span> pos <span class="token keyword">not</span> <span class="token keyword">in</span> obstacles <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> robot_next_pos_1 <span class="token keyword">and</span> pos_1 <span class="token keyword">not</span> <span class="token keyword">in</span> now_position<span class="token punctuation">:</span> <span class="token comment"># 测试时修改为live_points_1，同时传入</span>                            <span class="token keyword">if</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">>=</span> <span class="token number">1</span> <span class="token keyword">and</span> pos <span class="token operator">==</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                                <span class="token keyword">continue</span>                            wait_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>                            left_right_flag <span class="token operator">=</span> <span class="token boolean">True</span>                    <span class="token keyword">if</span> <span class="token keyword">not</span> left_right_flag<span class="token punctuation">:</span> <span class="token comment"># 没有左右的执行回退</span>                        track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token punctuation">,</span> track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                        reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        pos <span class="token operator">=</span> wait_pos<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                        waits<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> wait_pos                        paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">.</span>insert<span class="token punctuation">(</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token punctuation">,</span> pos<span class="token punctuation">)</span>                        reback_status<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># 发生过左右的情况</span>                collision_flag <span class="token operator">=</span> <span class="token boolean">True</span>                    <span class="token comment"># # 更新robot_next_pos</span>                <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>                    robot_next_pos<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span> <span class="token operator">=</span> Node<span class="token punctuation">(</span>paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token punctuation">,</span> paths<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">[</span>track_index<span class="token punctuation">[</span>target_robot<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    robot_next_pos_1 <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>robot_next_pos<span class="token punctuation">)</span>    <span class="token keyword">return</span> track_index<span class="token punctuation">,</span>paths<span class="token punctuation">,</span>connected_locks<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="六、运输模块"><a href="#六、运输模块" class="headerlink" title="六、运输模块"></a>六、运输模块</h2><p>五艘船选取港口的指标为：$p=\frac{船可以装的货物价值}{买卖点到港口的时间+装货时间+离开时间}$，五艘船轮流选择目标港口，到达港口之后如果可以装满，则装满离开，如果无法装满，则根据$p$考虑是否前往下一个港口继续装。</p><p>设置轮船的状态表，分别表示:<strong><u>目的港口id</u>、<u>轮船状态位</u></strong>（0表示闲置，1表示确定好港口，2表示在去港口的路上，3表示正在装货，4表示准备离开港口，5表示正在回买卖点）、<u><strong>轮次</strong></u>、<strong><u>船的转折次数</u></strong>：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">target_table_boat_berth <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span>target_table_boat_berth<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>运输选择：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">update_BoatStatus</span><span class="token punctuation">(</span>boats<span class="token punctuation">,</span>berths<span class="token punctuation">,</span>capacity<span class="token punctuation">,</span>now_frame<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">global</span> target_table_boat_berth    <span class="token keyword">global</span> berth_final    <span class="token keyword">global</span> berth_death    <span class="token keyword">global</span> berth_death_num    <span class="token keyword">for</span> index_boat <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment">#（共用）</span>        <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">5</span> <span class="token keyword">and</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>update_status<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            <span class="token comment">#第五轮直接设置为转移过一次了，不会再转了</span>            <span class="token comment"># if target_table_boat_berth[index_boat][2] == 4:</span>            <span class="token comment">#     target_table_boat_berth[index_boat][3] = 1</span>            boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>idle<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment">#第一轮6000帧以内，涉及到它只回去一次，也可能回去两次。不管如何6000帧必须在虚拟点，并且前期就是要么装满走人，如果转移的话考虑船内装没装够0.9cap，装够的话去虚拟点卖就行。只有前两轮用这个</span>        <span class="token keyword">if</span> now_frame <span class="token operator">&lt;</span> <span class="token number">12000</span> <span class="token punctuation">:</span>            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                    <span class="token comment">#选取目标</span>                    berth_index <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>                    <span class="token comment">#第一轮就正常选</span>                    <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                        berth_index <span class="token operator">=</span> select_first_two_first<span class="token punctuation">(</span>berths<span class="token punctuation">,</span>now_frame<span class="token punctuation">)</span>                    <span class="token comment">#只有能选的berth才会执行</span>                    <span class="token keyword">if</span> berth_index <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> berth_index                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                        berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">1</span>                    <span class="token comment">#如果没选到直接啥也不干轮次+1</span>                    <span class="token comment"># else:</span>                    <span class="token comment">#     target_table_boat_berth[index_boat][2] += 1</span>                <span class="token comment">#确定是否到船厂</span>            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span> <span class="token keyword">and</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>update_status<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>            <span class="token comment">#装货状态判断</span>            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>                index_berth <span class="token operator">=</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                <span class="token comment">#判断是否可以继续装货</span>                <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>GoodsOfBerth<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">and</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory <span class="token operator">&lt;</span> capacity<span class="token punctuation">:</span>                    boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>load<span class="token punctuation">(</span>berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>unload<span class="token punctuation">(</span>capacity <span class="token operator">-</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token comment">#不能继续装货有两种情况，第一种是港口空了，第二种是船满了</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    <span class="token comment">#先解锁</span>                    berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">0</span>                    <span class="token comment">#如果是装满了就直接回去</span>                    <span class="token keyword">if</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory <span class="token operator">>=</span> capacity<span class="token punctuation">:</span>                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>                    <span class="token comment">#没装满。</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        <span class="token comment">#如果装够0.9个cap直接走人就行了</span>                        <span class="token keyword">if</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory <span class="token operator">>=</span> <span class="token number">0.9</span> <span class="token operator">*</span> capacity<span class="token punctuation">:</span>                            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>                        <span class="token comment">#如果没装够0.9</span>                        <span class="token keyword">else</span><span class="token punctuation">:</span>                            berth_index <span class="token operator">=</span> select_first_two_second<span class="token punctuation">(</span>berths<span class="token punctuation">,</span>capacity <span class="token operator">-</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory<span class="token punctuation">,</span>now_frame<span class="token punctuation">)</span>                            <span class="token keyword">if</span> berth_index <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> berth_index                                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                                berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">1</span>                                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>                 <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token number">12000</span> <span class="token operator">-</span> now_frame<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time<span class="token punctuation">:</span>                    target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment">#全新改版</span>            <span class="token keyword">if</span> <span class="token number">12000</span> <span class="token operator">&lt;=</span> now_frame <span class="token operator">&lt;=</span> <span class="token number">12005</span> <span class="token punctuation">:</span>                <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token comment">#只有最后一轮直接定</span>                        berth_index <span class="token operator">=</span> select<span class="token punctuation">(</span>berths<span class="token punctuation">,</span>capacity<span class="token punctuation">)</span>                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> berth_index                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                        berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">1</span>                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>                        <span class="token comment">#最后一轮直接加时间</span>                        <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                            l_time <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>GoodsOfBerth<span class="token punctuation">)</span> <span class="token operator">/</span> berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>loading_speed                            <span class="token comment">#这样可以</span>                            berth_death<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span> <span class="token operator">=</span> now_frame <span class="token operator">+</span> berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time <span class="token operator">+</span> l_time <span class="token operator">+</span> <span class="token number">5</span>                            berth_death_num <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment">#确定是否到船厂(共用)</span>            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span> <span class="token keyword">and</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>update_status<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>            <span class="token comment">#装货状态</span>            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>                index_berth <span class="token operator">=</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                <span class="token comment">#判断是否可以继续装货</span>                <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>GoodsOfBerth<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">and</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory <span class="token operator">&lt;</span> capacity<span class="token punctuation">:</span>                    boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>load<span class="token punctuation">(</span>berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>unload<span class="token punctuation">(</span>capacity <span class="token operator">-</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    <span class="token comment">#先解锁</span>                    berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">0</span>                    <span class="token comment">#如果是倒数第二轮最后走的时候不解锁</span>                    <span class="token comment"># if target_table_boat_berth[index_boat][2] == 3 and target_table_boat_berth[index_boat][3] == 1:</span>                    <span class="token comment">#     berths[index_berth].choosen = 1</span>                    <span class="token comment">#如果是装满了就直接回去（实际不会出现）</span>                    <span class="token keyword">if</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory <span class="token operator">>=</span> capacity<span class="token punctuation">:</span>                        target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>                        <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berth_final<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>                                berth_final<span class="token punctuation">.</span>clear<span class="token punctuation">(</span><span class="token punctuation">)</span>                                berth_death<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                               <span class="token comment"># print('清空',file=sys.stderr)</span>                            <span class="token comment">#最多为4因此应该小于9，这样最后加完应该为9</span>                            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berth_final<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">9</span><span class="token punctuation">:</span>                                berth_final<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                            <span class="token comment"># None</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span><span class="token comment">#没装满考虑怎么走</span>                    <span class="token comment">#如果没装满并且没有转移过，找下家</span>                        <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                            <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                                berth_final<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                                <span class="token comment">#print(f'frame:&#123;now_frame&#125;,boat_id:&#123;index_boat&#125;,left:&#123;boats[index_boat].inventory&#125;,boat_table:\n&#123;target_table_boat_berth&#125;',file=sys.stderr)</span>                            berth_index <span class="token operator">=</span> select_two<span class="token punctuation">(</span>berths<span class="token punctuation">,</span>capacity <span class="token operator">-</span> boats<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">.</span>inventory<span class="token punctuation">,</span>now_frame<span class="token punctuation">)</span>                            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> berth_index                            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                            berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>choosen <span class="token operator">=</span> <span class="token number">1</span>                            target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                             <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                                <span class="token comment">#记录船厂被锁时间</span>                                berth_death<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">15000</span> <span class="token operator">-</span> berths<span class="token punctuation">[</span>berth_index<span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time                                berth_death_num <span class="token operator">+=</span> <span class="token number">1</span>                                <span class="token keyword">if</span> berth_death_num <span class="token operator">==</span> <span class="token number">10</span><span class="token punctuation">:</span>                                    <span class="token comment"># 找到不是 999 的值的下标</span>                                    indices <span class="token operator">=</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>berth_death <span class="token operator">!=</span> <span class="token number">99999</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                                    <span class="token comment"># 通过下标获取对应的值</span>                                    values <span class="token operator">=</span> berth_death<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>                                    <span class="token comment"># 找到最大值的下标</span>                                    max_index <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>values<span class="token punctuation">)</span>                                    <span class="token comment"># 在原始矩阵中找到最大值的位置</span>                                    max_value_index <span class="token operator">=</span> indices<span class="token punctuation">[</span>max_index<span class="token punctuation">]</span>                                    berth_death<span class="token punctuation">[</span>max_value_index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                                                        <span class="token comment">#如果转移过只有呆满才能走</span>                        <span class="token keyword">else</span><span class="token punctuation">:</span>                             <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token number">3000</span> <span class="token operator">-</span> <span class="token punctuation">(</span>now_frame <span class="token operator">%</span> <span class="token number">3000</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> berths<span class="token punctuation">[</span>index_berth<span class="token punctuation">]</span><span class="token punctuation">.</span>transport_time<span class="token punctuation">:</span>                                target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>                                  <span class="token keyword">if</span> target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>                                    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berth_final<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>                                        berth_final<span class="token punctuation">.</span>clear<span class="token punctuation">(</span><span class="token punctuation">)</span>                                        berth_death<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">99999</span>                                        <span class="token comment">#print('清空',file=sys.stderr)</span>                                    <span class="token comment">#最多为4因此应该小于9，这样最后加完应该为9</span>                                    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>berth_final<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">9</span><span class="token punctuation">:</span>                                        berth_final<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target_table_boat_berth<span class="token punctuation">[</span>index_boat<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><ul><li><p>采用静态和贪心的策略进行货物的选择，可能导致机器人运货量达不到最优；、</p></li><li><p>防碰撞没有没有做到完美，机器人仍会出现卡死的情况</p></li><li><p>整体代码链接：<a href="https://github.com/ghtll/huaweibisai2024">https://github.com/ghtll/huaweibisai2024</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 比赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 路径规划 </tag>
            
            <tag> 避障 </tag>
            
            <tag> 目标选取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>emacs</title>
      <link href="/2024/02/29/emacs/"/>
      <url>/2024/02/29/emacs/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>最近了解了一个强大的编辑器Emacs，被它方便快捷的功能深深吸引，经过几天的琢磨，对Emacs有了一个初步的了解，记录一下自己的学习历程</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><p>初步的学习了解是通过哔哩哔哩学习的，本人看的Emacs高手修炼手册视频（<a href="https://www.bilibili.com/video/BV13g4y167Zn/?p=51&amp;spm_id_from=333.880.my_history.page.click">47-LSP+Go语言编程环境配置_哔哩哔哩_bilibili</a>），觉得视频比较简单易懂。动手跟着教学配置了一遍Emacs的界面，对其有了初步的了解。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>参考：<a href="https://github.com/doomemacs/doomemacs/blob/master/docs/getting_started.org#install">doomemacs/docs/getting_started.org at master · doomemacs/doomemacs · GitHub</a></p><p>我是在windows下安装的Emacs，通过教程的“WSL + Ubuntu 20.04 LTS“进行安装。</p><ol><li><p>安装WSL：打开Powershell，然后输入</p><pre class="line-numbers language-none"><code class="language-none">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>重启电脑</p></li><li><p>在Microsoft Store安装Ubuntu 20.04L&gt;TS</p></li><li><p>启动Ubuntu，我这里一直报错，于是参考了这篇文章：<a href="https://zhuanlan.zhihu.com/p/599286889">解决WSL2的 0x800701bc错误 - 知乎 (zhihu.com)</a></p><p><img src="../images/Emacs/1.png" alt="image-20240229152911205"></p></li><li><p>更新升级Ubuntu：</p><pre class="line-numbers language-none"><code class="language-none">sudo apt update &amp;&amp; sudo apt upgrade<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装Emacs：</p><pre class="line-numbers language-none"><code class="language-none">sudo add-apt-repository ppa:kelleyk&#x2F;emacssudo apt updatesudo apt install emacs28<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>安装依赖</p><pre class="line-numbers language-none"><code class="language-none"># required dependenciessudo apt-get install git ripgrep# optional dependenciessudo apt-get install fd-find<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>配置文件地址：<a href="https://github.com/syl20bnr/spacemacs">GitHub - syl20bnr/spacemacs: A community-driven Emacs distribution - The best editor is neither Emacs nor Vim, it’s Emacs <em>and</em> Vim!</a></p><p>Spacemacs学习地址：<a href="https://github.com/liuzhijun-source/spacemacs-14-days">GitHub - liuzhijun-source/spacemacs-14-days: 一个 Spacemacs 的入门教程</a></p><h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><div class="table-container"><table><thead><tr><th style="text-align:center">快捷键</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">c-c c-t</td><td style="text-align:center">org模式todo</td></tr><tr><td style="text-align:center">c-x b</td><td style="text-align:center">查找buffer</td></tr><tr><td style="text-align:center">SPC x</td><td style="text-align:center">查找并跳转到“单词x或者X开头的单词位置”，x是char字符，可以是任意字符</td></tr><tr><td style="text-align:center">!</td><td style="text-align:center">执行shell命令</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">*或者/</td><td style="text-align:center">在project里搜索关键词</td></tr><tr><td style="text-align:center">1 2 3 … 9</td><td style="text-align:center">跳转到第n个window窗口</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">;</td><td style="text-align:center">用来注释代码的，这个可以查看一下帮助手册。各种注释操作</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">a u</td><td style="text-align:center">undo作用。可以上下操作，是undo的history列表。</td></tr><tr><td style="text-align:center">b b</td><td style="text-align:center">查找并切换到buffer/recent-file</td></tr><tr><td style="text-align:center">b d</td><td style="text-align:center">删除当前buffer</td></tr><tr><td style="text-align:center">f f</td><td style="text-align:center">查找文件以及recent-files</td></tr><tr><td style="text-align:center">f y</td><td style="text-align:center">复制并显示当前buffer文件名。完整路径</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">layout的保存，输入layout的名称</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">p p</td><td style="text-align:center">打开某个project。</td></tr><tr><td style="text-align:center">s s</td><td style="text-align:center">在当前buffer里搜索关键词。</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">w -</td><td style="text-align:center">window横切</td></tr><tr><td style="text-align:center">w /</td><td style="text-align:center">window竖切</td></tr><tr><td style="text-align:center">w c</td><td style="text-align:center">window close</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Emacs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩总结</title>
      <link href="/2023/10/16/summery1016/"/>
      <url>/2023/10/16/summery1016/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Losparse"><a href="#1-Losparse" class="headerlink" title="1 Losparse"></a>1 Losparse</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LoSparse: Structured Compression of Large LanguageModels based on Low-Rank and Sparse Approximation</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2023.7</td></tr><tr><td>代码</td><td><a href="https://github.com/yxli2123/LoSparse">yxli2123/LoSparse (github.com)</a></td></tr><tr><td>压缩技术</td><td>低秩近似、结构剪枝</td></tr></tbody></table></div><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>将权重近似为(将这种分解应用于模型的每个权重矩阵)：</p><script type="math/tex; mode=display">W=UV+S</script><p><img src="/images/summery1016_imgs/1.png" alt=""></p><p>重要性评分公式方法：</p><p><img src="/images/summery1016_imgs/2.png" alt="image-20231016142137668"></p><p><img src="/images/summery1016_imgs/43.png" alt="    "></p><h2 id="i"><a href="#i" class="headerlink" title="i"></a>i</h2><p>剪枝采用迭代剪枝，在迭代过程中S的重要性评估不稳定，原文采用平滑+大batchsize。由于样本造成的不稳定，这里可以想一个办法。要不让样本更稳定，要不更换一个重要性评估方法。</p><p>初始化$U^0$和$V^0$会丢失很多知识，在文中，矩阵分解占比仅有1-5％，其余都是S的占比。如果在不增加UV的占比情况下提高他的知识，是否可以去除更多的S？考虑加权分解。先加权分解UV，在迭代剪枝。最后微调？</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>评估任务：</strong>natural language understanding (NLU)、question answering (QA)、natural language generation (NLG)</p><p><strong>压缩模型：</strong>DeBERTaV3-base、, BERT-base、BART-large</p><p><strong>Baselines:</strong> Full fine-tuning 、Movement pruning、Iterative pruning (ITP)</p><h3 id="Natural-Language-Understanding"><a href="#Natural-Language-Understanding" class="headerlink" title="Natural Language Understanding"></a>Natural Language Understanding</h3><p>在通用语言理解评估（GLUE）基准上修剪DeBERTaV3-base模型：</p><p><img src="/images/summery1016_imgs/3.png" alt=""></p><p>修剪bert：</p><p><img src="/images/summery1016_imgs/4.png" alt=""></p><h3 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h3><p>在SQuADv1.1上压缩了DeBERTaV3-base和BERT-base:</p><p><img src="/images/summery1016_imgs/5.png" alt=""></p><h3 id="Natural-Language-Generation"><a href="#Natural-Language-Generation" class="headerlink" title="Natural Language Generation"></a>Natural Language Generation</h3><p>在XSum和CNN/DailyMail数据集上压缩BART-large：</p><p><img src="/images/summery1016_imgs/6.png" alt=""></p><h1 id="2-LoRAPrune"><a href="#2-LoRAPrune" class="headerlink" title="2 LoRAPrune"></a>2 LoRAPrune</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>PRUNING MEETS LOW-RANK PARAMETER-EFFICIENT FINE-TUNING</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.3</td></tr><tr><td>代码</td><td>无</td></tr><tr><td>压缩技术</td><td>剪枝 SOTA</td></tr></tbody></table></div><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/10.png" alt=""></p><p>插入lora，用lora的梯度代替权重的梯度。</p><p>使用一阶泰勒展开式来近似的重要性过于复杂，因此改为使用BA的梯度：</p><p><img src="/images/summery1016_imgs/7.png" alt=""></p><p><img src="/images/summery1016_imgs/8.png" alt=""></p><p>BA梯度仍然复杂，因此继续近似：</p><p><img src="/images/summery1016_imgs/9.png" alt=""></p><p><img src="/images/summery1016_imgs/44.png" alt=""></p><h2 id="i-1"><a href="#i-1" class="headerlink" title="i"></a>i</h2><p>用lora代替w进行计算。前百分之10与后百分之30只涉及参数更新。修剪过程采用“修剪-微调-修剪方法”。</p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><strong>任务</strong>：VTB-1k(19 个图像分类数据集)、FGVC(CUB-200-2011、NABirds、Oxford Flowers、Stanford Cars 和 Stanford Dogs)、GLUE(CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE 等任务)</p><p>对于CV任务采用ViT-B/16模型，对于NLP任务采用 BERT-base模型（RTX3090）</p><p>对比：幅值修剪（MaP）、带LoRA的幅值修剪（MaP-LoRA）、运动修剪（MvP）、随机修剪（RaP）、参数高效稀疏性（PST）</p><p>结果（<strong>微调和剪枝过程结合在一起是高效</strong>）：</p><p><img src="/images/summery1016_imgs/11.png" alt=""></p><p><img src="/images/summery1016_imgs/12.png" alt=""></p><h1 id="3-PruneOFA"><a href="#3-PruneOFA" class="headerlink" title="3 PruneOFA"></a>3 PruneOFA</h1><h2 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Prune Once for All: Sparse Pre-Trained Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2021.11</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>非结构化剪枝、蒸馏</td></tr></tbody></table></div><h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><p>采用渐进幅度修剪：</p><p><img src="/images/summery1016_imgs/13.png" alt=""></p><p>流程：</p><p><img src="/images/summery1016_imgs/14.png" alt=""></p><p>提出模式锁的方法，它可以防止在训练模型时改变模型中发现的零</p><h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><p>模型：BERT-Base, BERT-Large and DistilBERT</p><p><img src="/images/summery1016_imgs/15.png" alt=""></p><p><img src="/images/summery1016_imgs/16.png" alt=""></p><p><img src="/images/summery1016_imgs/17.png" alt=""></p><h1 id="4-oBert"><a href="#4-oBert" class="headerlink" title="4 oBert"></a>4 oBert</h1><h2 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2022.10</td></tr><tr><td>代码</td><td><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">sparseml/research/optimal_BERT_surgeon_oBERT at main · neuralmagic/sparseml (github.com)</a></td></tr><tr><td>压缩技术</td><td>非结构化剪枝</td></tr></tbody></table></div><h2 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h2><p>采用二阶Hassian矩阵近似重要性进行剪枝：</p><p><img src="/images/summery1016_imgs/18.png" alt=""></p><p>采用经验fisher矩阵近似hassian矩阵：</p><p><img src="/images/summery1016_imgs/19.png" alt=""></p><h2 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h2><p>比较方法：Movement Pruning (MvP)、Lottery Ticket (LT-BERT)</p><p><img src="/images/summery1016_imgs/20.png" alt=""></p><p><img src="/images/summery1016_imgs/21.png" alt=""></p><h1 id="5-FLOP"><a href="#5-FLOP" class="headerlink" title="5 FLOP"></a>5 FLOP</h1><h2 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Structured Pruning of Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2021.5</td></tr><tr><td>代码</td><td><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">sparseml/research/optimal_BERT_surgeon_oBERT at main · neuralmagic/sparseml (github.com)</a></td></tr><tr><td>压缩技术</td><td>低秩近似、结构化剪枝</td></tr></tbody></table></div><h3 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h3><p>端到端的训练方法，优化目标：</p><p><img src="/images/summery1016_imgs/22.png" alt=""></p><p>重新参数化技巧：</p><p><img src="/images/summery1016_imgs/23.png" alt=""></p><p>利用低秩因子分解的权重矩阵参数化方法：</p><p><img src="/images/summery1016_imgs/24.png" alt=""></p><p>增强拉格朗日方法：</p><p><img src="/images/summery1016_imgs/25.png" alt=""></p><p>优化的最终目标：</p><p><img src="/images/summery1016_imgs/26.png" alt=""></p><h2 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/27.png" alt=""></p><h1 id="6-FWSVD"><a href="#6-FWSVD" class="headerlink" title="6 FWSVD"></a>6 FWSVD</h1><h2 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LANGUAGE MODEL COMPRESSION WITH WEIGHTED LOW-RANK FACTORIZATION</th></tr></thead><tbody><tr><td>期刊</td><td>ICLR</td></tr><tr><td>发表时间</td><td>2022.6</td></tr><tr><td>代码</td><td><a href="https://github.com/RahulSChand/Weighted-low-rank-factorization-Pytorch">https://github.com/RahulSChand/Weighted-low-rank-factorization-Pytorch</a></td></tr><tr><td>压缩技术</td><td>加权分解</td></tr></tbody></table></div><h3 id="方法-5"><a href="#方法-5" class="headerlink" title="方法"></a>方法</h3><p>矩阵分解：</p><p><img src="/images/summery1016_imgs/28.png" alt=""></p><p>Fisher信息：</p><p><img src="/images/summery1016_imgs/29.png" alt=""></p><p>费雪加权低秩近似：</p><p><img src="/images/summery1016_imgs/30.png" alt=""></p><h2 id="i-2"><a href="#i-2" class="headerlink" title="i"></a>i</h2><p>微调lora（保留增量梯度）-&gt;计算重要性-&gt;加权分解SVD-&gt;W=BA+S剪枝（剪枝S，微调BA）</p><p>W每一行相同的权重，否则没有闭环解？如何改进。</p><p><strong>单词嵌入层(非负矩阵分解，考虑人脸识别和数字分解)</strong></p><h2 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h2><p>压缩路径：</p><p><img src="/images/summery1016_imgs/31.png" alt=""></p><p><img src="/images/summery1016_imgs/32.png" alt=""></p><h1 id="7-LLM-Pruner"><a href="#7-LLM-Pruner" class="headerlink" title="7 LLM-Pruner"></a>7 LLM-Pruner</h1><h2 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>LLM-Pruner: On the Structural Pruning of Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>NeurlPS</td></tr><tr><td>发表时间</td><td>2023.9</td></tr><tr><td>代码</td><td><a href="https://github.com/horseee/LLM-Pruner">https://github.com/horseee/LLM-Pruner</a></td></tr><tr><td>压缩技术</td><td>结构剪枝+LoRA微调</td></tr></tbody></table></div><h2 id="方法-6"><a href="#方法-6" class="headerlink" title="方法"></a>方法</h2><p>发现依赖结构-&gt;评估重要性(采用二阶导数)-&gt;微调恢复</p><h2 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h2><p>无baselines</p><p><img src="/images/summery1016_imgs/33.png" alt=""></p><h1 id="8-SHEARED-LLAMA"><a href="#8-SHEARED-LLAMA" class="headerlink" title="8 SHEARED LLAMA"></a>8 SHEARED LLAMA</h1><h2 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.10</td></tr><tr><td>代码</td><td><a href="https://github.com/horseee/LLM-Pruner">https://github.com/horseee/LLM-Pruner</a></td></tr><tr><td>压缩技术</td><td>结构剪枝+动态批量加载</td></tr></tbody></table></div><h2 id="方法-7"><a href="#方法-7" class="headerlink" title="方法"></a>方法</h2><p>微调对于结构剪枝至关重要</p><p>端到端剪枝</p><p><img src="/images/summery1016_imgs/34.png" alt=""></p><p>根据训练数据比例动态加载训练数据</p><h2 id="实验-7"><a href="#实验-7" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/35.png" alt=""></p><h1 id="9-ZipLM"><a href="#9-ZipLM" class="headerlink" title="9 ZipLM"></a>9 ZipLM</h1><h2 id="简介-8"><a href="#简介-8" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>ZipLM: Hardware-Aware Structured Pruning of Language Models</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023.2</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>结构剪枝+知识蒸馏、逐层剪枝</td></tr></tbody></table></div><h2 id="方法-8"><a href="#方法-8" class="headerlink" title="方法"></a>方法</h2><p>一次剪枝一个结构来解决相关性问题。</p><p>权重更新，二阶导数近似：</p><p><img src="/images/summery1016_imgs/36.png" alt=""></p><p>蒸馏：</p><p><img src="/images/summery1016_imgs/37.png" alt=""></p><h2 id="实验-8"><a href="#实验-8" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/38.png" alt=""></p><h1 id="10-Matrix-Decomposition"><a href="#10-Matrix-Decomposition" class="headerlink" title="10 Matrix Decomposition"></a>10 Matrix Decomposition</h1><h2 id="简介-9"><a href="#简介-9" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Compressing Pre-trained Language Models by Matrix Decomposition</th></tr></thead><tbody><tr><td>期刊</td><td>AACl</td></tr><tr><td>发表时间</td><td>2021</td></tr><tr><td>代码</td><td><a href="https://github.com/kene111/matrix-decomposition">https://github.com/kene111/matrix-decomposition</a></td></tr><tr><td>压缩技术</td><td>低秩分解+蒸馏</td></tr></tbody></table></div><h2 id="方法-9"><a href="#方法-9" class="headerlink" title="方法"></a>方法</h2><ol><li><p>通过SVD分解</p></li><li><p>知识蒸馏，训练损失由三部分构成：</p><script type="math/tex; mode=display">L=\alpha L_{CE}+(1-\alpha)L_{KD}+L_{FD}</script><p><img src="/images/summery1016_imgs/39.png" alt=""></p></li></ol><p><img src="/images/summery1016_imgs/40.png" alt=""></p><p><img src="/images/summery1016_imgs/41.png" alt=""></p><p><strong>基础模型(微调模型)用于分解和作为教师模型</strong></p><h2 id="实验-9"><a href="#实验-9" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/42.png" alt=""></p><h1 id="11-Movement-Pruning"><a href="#11-Movement-Pruning" class="headerlink" title="11 Movement Pruning"></a>11 Movement Pruning</h1><h2 id="简介-10"><a href="#简介-10" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Movement Pruning:Adaptive Sparsity by Fine-Tuning</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2020</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>移动剪枝</td></tr></tbody></table></div><h2 id="方法-10"><a href="#方法-10" class="headerlink" title="方法"></a>方法</h2><p>考虑到权重值在迁移学习中不是从头开始的，传统的基于幅度的剪枝可能无法充分适应新任务的需求，因为他们的修剪决策是基于原始模型的权重值。公式推导：</p><p><img src="/images/summery1016_imgs/45.png" alt=""></p><p><img src="/images/summery1016_imgs/46.png" alt=""></p><p><img src="/images/summery1016_imgs/112.png" alt=""></p><p><img src="/images/summery1016_imgs/113.png" alt=""></p><p>从公式可以看出，当W远离0点是，重要性S变大。于是修剪那些逐渐靠近0的权重而幅度修剪的方法修剪离0近的值。</p><p><img src="/images/summery1016_imgs/114.png" alt=""></p><h2 id="实验-10"><a href="#实验-10" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/115.png" alt=""></p><h1 id="12-ITP"><a href="#12-ITP" class="headerlink" title="12 ITP"></a>12 ITP</h1><h2 id="简介-11"><a href="#简介-11" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Importance Estimation for Neural Network Pruning</th></tr></thead><tbody><tr><td>期刊</td><td>CVPR</td></tr><tr><td>发表时间</td><td>2019</td></tr><tr><td>代码</td><td><a href="https://github.com/NVlabs/Taylor_pruning*.*">https://github.com/NVlabs/Taylor_pruning*.*</a></td></tr><tr><td>压缩技术</td><td>迭代剪枝、一阶二阶近似</td></tr></tbody></table></div><h2 id="方法-11"><a href="#方法-11" class="headerlink" title="方法"></a>方法</h2><p>计算精确的重要性对于大型网络来说是及其昂贵的，然后采用一阶或二阶近似的方法评估重要性。</p><p><img src="/images/summery1016_imgs/121.png" alt="image-20231026144556225"></p><p><img src="/images/summery1016_imgs/122.png" alt=""></p><p><img src="/images/summery1016_imgs/123.png" alt=""></p><p>以一个训练过的网络作为输入，并在一个具有较小学习率的迭代微调过程中对其进行剪枝。在每个时期内，都要重复以下步骤：</p><ol><li>对于每个小批处理，我们计算参数梯度，并通过梯度下降来更新网络权值。我们还计算了每个神经元（或滤波器）的重要性</li><li>在预定义的小批之后，我们将每个神经元（或过滤器）的重要性分数取为小批的平均值，并去除N个重要性分数最小的神经元</li></ol><h2 id="实验-11"><a href="#实验-11" class="headerlink" title="实验"></a>实验</h2><p>在LSTM上做的实验。</p><h1 id="13-HMD"><a href="#13-HMD" class="headerlink" title="13  HMD"></a>13  HMD</h1><h2 id="简介-12"><a href="#简介-12" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Rank and run-time aware compression of NLP Applications</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2020</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>混合低秩压缩</td></tr></tbody></table></div><h2 id="方法-12"><a href="#方法-12" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/131.png" alt=""></p><h2 id="实验-12"><a href="#实验-12" class="headerlink" title="实验"></a>实验</h2><h1 id="14-SPDF"><a href="#14-SPDF" class="headerlink" title="14 SPDF"></a>14 SPDF</h1><h2 id="简介-13"><a href="#简介-13" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models</th></tr></thead><tbody><tr><td>期刊</td><td>EMNLP</td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>剪枝之后恢复剪枝</td></tr></tbody></table></div><h2 id="方法-13"><a href="#方法-13" class="headerlink" title="方法"></a>方法</h2><p>首先，我们预先训练一个稀疏GPT模型，以减少计算训练的流量。然后，在微调阶段，我们强化GPT模型，允许零权值学习并增加建模能力，以更准确地学习下游任务。采用随机修剪，均匀稀疏的方法</p><p>三个假设：</p><ol><li>在llm的训练前阶段，可以使用高程度的权重稀疏度，同时通过密集的微调来保持下游的精度。</li><li>稀疏预训练模型的性能与下游任务中数据集的大小和难度相关。</li><li>当我们增加语言模型的规模时，更大的模型在训练前变得更容易接受更高水平的稀疏性。</li></ol><p><img src="/images/summery1016_imgs/141.png" alt=""></p><h2 id="实验-13"><a href="#实验-13" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/142.png" alt=""></p><h1 id="15-A-Fast-Post-Training-Pruning-Framework-forTransformers"><a href="#15-A-Fast-Post-Training-Pruning-Framework-forTransformers" class="headerlink" title="15 A Fast Post-Training Pruning Framework forTransformers"></a>15 A Fast Post-Training Pruning Framework forTransformers</h1><h2 id="简介-14"><a href="#简介-14" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>A Fast Post-Training Pruning Framework for Transformers</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/WoosukKwon/retraining-free-pruning">https://github.com/WoosukKwon/retraining-free-pruning</a></td></tr><tr><td>压缩技术</td><td>训练后采用20K数据三分钟剪枝</td></tr></tbody></table></div><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><ol><li>剪枝是降低变压器模型巨大推理成本的一种有效方法。然而，之前关于修剪变压器的工作需要重新训练模型</li><li>虽然结构化剪枝方法可以实现高压缩率和加速，但它们通常很难在实践中使用。其中一个原因是在修剪期间或修剪后额外训练的计算成本很高，另一个原因是剪枝管道的高度复杂性，其中每个剪枝阶段通常需要重写训练代码，并引入额外的超参数来进行调优。</li><li></li></ol><h2 id="方法-14"><a href="#方法-14" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/151.png" alt=""></p><p>定义问题：</p><p><img src="/images/summery1016_imgs/152.png" alt=""></p><p>二阶近似：</p><p><img src="/images/summery1016_imgs/153.png" alt=""></p><p>海森近似：</p><p><img src="/images/summery1016_imgs/154.png" alt=""></p><p><strong>掩码搜索：</strong></p><p><img src="/images/summery1016_imgs/155.png" alt=""></p><p><img src="/images/summery1016_imgs/156.png" alt=""></p><p><strong>掩码重排</strong>：</p><p>虽然它简化了问题，但仅使用对角线假设可能无法找到最佳的解决方案，因为它没有考虑到不同掩模变量之间的相互作用。例如，如果在一个图层中有两个注意头发挥着相似的作用，那么只修剪其中一个可能不会影响模型的准确性。然而，当它们两者都被修剪时，模型的精度可能会显著降低。这种交互作用被费雪信息矩阵的非对角元素捕获，在前一阶段被忽略。因此，我们可以通过使用对Fisher矩阵的块对角近似来更好地考虑剪枝问题中的相互作用，其中一个块对应于一个MHA层或一个FFN层，如下图所示。</p><p><img src="/images/summery1016_imgs/157.png" alt=""></p><p>用贪婪算法近似地解决这个问题。在将掩模初始化即热启动）后，为每一轮选择一个具有最高Fisher信息的修剪头（或过滤器），并与当前掩模中的一个未修剪头（或过滤器）交换：</p><p><img src="/images/summery1016_imgs/158.png" alt=""></p><p><strong>掩码微调：</strong></p><p>非零变量被调优到任何真实值，这样修剪后的模型就可以恢复其精度。通过线性最小二乘法进行分层重建。我们调整掩模变量以使最小化层重构误差，。从第一层到最后一层，我们用修剪模型中剩余的头/滤波器重建原始模型的输出激活。正式形式如下：</p><p><img src="/images/summery1016_imgs/159.png" alt=""></p><h2 id="实验-14"><a href="#实验-14" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/1510.png" alt=""></p><h1 id="16-随机森林预测剪枝架构"><a href="#16-随机森林预测剪枝架构" class="headerlink" title="16 随机森林预测剪枝架构"></a>16 随机森林预测剪枝架构</h1><h2 id="简介-15"><a href="#简介-15" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>PRUNING LARGE LANGUAGE MODELS VIA ACCURACY PREDICTOR</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>建立预测器预测剪枝体系结构</td></tr></tbody></table></div><h2 id="笔记-1"><a href="#笔记-1" class="headerlink" title="笔记"></a>笔记</h2><ol><li>目前，llm压缩的一些工作主要集中在模型量化上</li><li>注意层的重要性远高于MLP：</li></ol><p><img src="/images/summery1016_imgs/163.png" alt=""></p><h2 id="方法-15"><a href="#方法-15" class="headerlink" title="方法"></a>方法</h2><p>构建架构-精度对：</p><p><img src="/images/summery1016_imgs/161.png" alt=""></p><p>权重重要性评估：</p><p><img src="/images/summery1016_imgs/162.png" alt=""></p><p>建立随机森林模型。</p><p>QLoRA微调恢复。</p><h2 id="实验-15"><a href="#实验-15" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/164.png" alt=""></p><h1 id="17-Wanda"><a href="#17-Wanda" class="headerlink" title="17 Wanda"></a>17 Wanda</h1><h2 id="简介-16"><a href="#简介-16" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td><a href="https://github.com/locuslab/wanda">locuslab/wanda: A simple and effective LLM pruning approach. (github.com)</a></td></tr><tr><td>压缩技术</td><td>一次剪枝，不需要再训练或重量更新</td></tr></tbody></table></div><h2 id="笔记-2"><a href="#笔记-2" class="headerlink" title="笔记"></a>笔记</h2><ol><li>由于最近在llm中出现的大幅度特征的观察，我们的方法在每个输出的基础上，将最小幅度的权重乘以相应的输入激活。</li><li>到目前为止，许多显著的进展都集中在模型量化上，这是一个将参数被量化为更低的位级表示的过程</li><li>幅度修剪（Han et al.，2015），一种成熟的修剪方法，即使具有相对较低的稀疏性水平，在llm上也会显著失败</li><li>一旦llm达到一定规模（实际中约6B参数），一小组隐藏状态特征就会比其余特征大得多。这些离群值特征表现出几个有趣的特征。首先，它们有非常大的大小，大约是典型的隐藏状态值的100倍。其次，它们通常是稀疏的，并且存在于某些特定的特征维度中。最后，这些离群值特征对于llm的预测能力是至关重要的：在推理时消除这些特征将导致语言建模性能的显著下降</li><li>与SparseGPT不同，我们的方法不需要对修剪过的网络进行权值更新，这表明llm具有有效的精确的稀疏子网络，而不是它们仅仅存在于原始权值的邻域中</li><li>选择正确的比较组对于剪枝大型语言模型（LLMs）非常重要，即使在传统的Magnitude剪枝方法中也是如此</li></ol><h2 id="方法-16"><a href="#方法-16" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/171.png" alt=""></p><p><strong>我们在每个输出的基础上（图中的每一行）上比较和删除权重，其中权重重要性分数在每个输出神经元中进行局部比较</strong></p><p>对比SparseGPT：</p><p><img src="/images/summery1016_imgs/172.png" alt=""></p><p>令$\lambda=0$：</p><p><img src="/images/summery1016_imgs/173.png" alt=""></p><h2 id="实验-16"><a href="#实验-16" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/174.png" alt=""></p><p>校准数据量：</p><p><img src="/images/summery1016_imgs/175.png" alt=""></p><p>不同分组结果：</p><p><img src="/images/summery1016_imgs/176.png" alt=""></p><h1 id="18-SparseBERT"><a href="#18-SparseBERT" class="headerlink" title="18 SparseBERT"></a>18 SparseBERT</h1><h2 id="简介-17"><a href="#简介-17" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Rethinking Network Pruning— under the Pre-train and Fine-tune Paradigm</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2021</td></tr><tr><td>代码</td><td><a href="https://github.com/dongkuanx27/SparseBERT">https://github.com/dongkuanx27/SparseBERT</a></td></tr><tr><td>压缩技术</td><td>SparseBERT将在微调阶段执行。它在修剪的同时保留了通用的和特定于任务的语言知识、幅度剪枝</td></tr></tbody></table></div><h2 id="笔记-3"><a href="#笔记-3" class="headerlink" title="笔记"></a>笔记</h2><ol><li>通过研究知识在训练前、微调和修剪过程中如何传递和丢失来填补这一空白，并提出一个知识感知的稀疏剪枝过程</li><li>最近的研究结果表明，自我注意和前馈层是过度参数化的，是最多计算消耗的部分</li></ol><h2 id="方法-17"><a href="#方法-17" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/181.png" alt=""></p><p><img src="/images/summery1016_imgs/182.png" alt=""></p><p>SparseBERT使用没有微调的预先训练的BERT作为初始化模型，修剪自注意和前馈层的线性变换.</p><p>为了在剪枝过程中学习特定于任务的任务知识，同时保留通用知识，我们应用了知识蒸馏：采用特定任务的微调BERT作为教师网络，采用预先训练的BERT作为学生。我们将下游任务数据输入师生框架，以训练学生再现教师的行为。</p><p>蒸馏损失：</p><p><img src="/images/summery1016_imgs/183.png" alt=""></p><p>蒸馏与剪枝并行：</p><p><img src="/images/summery1016_imgs/184.png" alt=""></p><h2 id="实验-17"><a href="#实验-17" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/185.png" alt=""></p><h1 id="19-Compresso"><a href="#19-Compresso" class="headerlink" title="19 Compresso"></a>19 Compresso</h1><h2 id="简介-18"><a href="#简介-18" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th><strong>Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models.</strong></th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td>(<a href="https://github.com/microsoft/Moonlit/tree/main/Compresso">https://github.com/microsoft/Moonlit/tree/main/Compresso</a>)</td></tr><tr><td>压缩技术</td><td>使用指令调优数据集、提示词、端到端剪枝</td></tr></tbody></table></div><h2 id="笔记-4"><a href="#笔记-4" class="headerlink" title="笔记"></a>笔记</h2><ol><li>LLM培训由于其庞大的模型规模，资源非常密集。其次，llm的训练数据集是广泛的，而且由于法律限制，往往不可用。直接使用开源数据集可能会导致分布外的问题，因为剪枝数据的分布与预训练前的数据分布有很大的不同</li><li>大多数最先进的修剪方法都涉及到一个训练过程来更新梯度，并利用它们来估计权重的重要性，然而，由于两个主要原因，这些方法不能直接应用于llm。首先，它们是需要下游训练数据集。因此，修剪后的模型不能保留在不同任务之间的泛化能力。其次，llm的修剪过程需要大量的训练资源。</li><li>尽管它的速度很快，但一次性修剪也有其局限性。首先，它在很大程度上依赖于预先预定义的权重重要性度量来修剪决策，因此在所有层之间采用均匀稀疏比，而不考虑每个层的不同冗余。其次，与基于训练的剪枝相比，剩余模型参数的误差恢复很有限，这可能会影响最终的性能。我们的压缩机解决了所有这些限制。</li><li>使用指令对llm进行微调已被证明可以提高性能和对不可见任务的泛化</li></ol><h2 id="方法-18"><a href="#方法-18" class="headerlink" title="方法"></a>方法</h2><p>为了解决基于训练的剪枝中高训练成本和数据收集的挑战，我们将低秩适应（LoRA）纳入L0正则化，并使用指令调优数据集作为训练数据的替代方案。具体来说，我们利用可学习的二进制掩码来决定是否保留或修剪每个子模块（即，头、FFN中间维度和隐藏维度）。</p><p>然后，在指令调整过程中，采用L0正则化方法优化掩模值，同时通过LoRA更新模型参数。此外，与一次性LLM修剪方法相比，通常采用跨所有层的均匀稀疏比，压缩机自动学习改进的层级稀疏比。</p><p><img src="/images/summery1016_imgs/191.png" alt=""></p><p>数据集：建议使用指令调优数据集作为修剪数据。尽管它们的分布不同于训练前的数据集，但它们已经证明了在微调预训练和收敛的llm以符合人类意图方面的成功</p><p>高效的基于训练的结构化修剪：</p><p>基本思想是： (i)我们引入一组二进制掩模Z∈{0,1}来指示是删除（Z = 0）还是保留（Z = 1）每个掩模子模块，从而表示剩余的模型大小；（ii）我们冻结原始LLM，利用LoRA向LLM的秩分解矩阵注入LLM可训练的每一层。这大大减少了可训练参数的数量和所需的GPU内存；（iii）我们使用增强的L0正则化联合优化这些掩模值和LoRA模块（方法。这确保了修剪后的模型大小满足给定的约束条件。</p><p>剪枝形式（采用Cofi）：</p><p><img src="/images/summery1016_imgs/192.png" alt=""></p><p>引入Lora：</p><p><img src="/images/summery1016_imgs/193.png" alt=""></p><p>定义稀疏性函数(无需手动选择剪枝比例)：</p><p><img src="/images/summery1016_imgs/194.png" alt=""></p><p>$L_0$重参数化：</p><p><img src="/images/summery1016_imgs/195.png" alt=""></p><p>惩罚项：</p><p><img src="/images/summery1016_imgs/196.png" alt=""></p><p>训练目标是下一个令牌预测损失和$L_{0{reg}}$损失的组合。</p><p>剪枝过程中，我们将提示符放在输入文本之前。根据指令调优的实践（Taori et al.，2023），我们不计算提示部分的下一代令牌生成损失。协作提示在两个阶段的修剪和推理阶段都被使用:</p><p><img src="/images/summery1016_imgs/197.png" alt=""></p><h2 id="实验-18"><a href="#实验-18" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/198.png" alt="image-20231106160054281"></p><h1 id="20-Cofi"><a href="#20-Cofi" class="headerlink" title="20 Cofi"></a>20 Cofi</h1><h2 id="简介-19"><a href="#简介-19" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Structured Pruning Learns Compact and Accurate Models</th></tr></thead><tbody><tr><td>期刊</td><td>ACL</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/princeton-nlp/CoFiPruning">https://github.com/princeton-nlp/CoFiPruning</a></td></tr><tr><td>压缩技术</td><td>粗粒度和细粒度剪枝、分层精馏策略</td></tr></tbody></table></div><h2 id="笔记-5"><a href="#笔记-5" class="headerlink" title="笔记"></a>笔记</h2><ol><li>蒸馏方法需要大量的未标记数据，而且训练成本昂贵</li><li>经验证据表明，50%的层可以下降，而没有很大的精度下降，导致2×的加速。</li><li>FFN修剪的其他主要部分-前馈层（FFNs）-也被认为是过度参数化的</li><li>加速率是我们在整个论文中使用的一个主要度量方法，因为压缩率并不一定反映了推理延迟的实际改进</li><li>我们将这项工作的范围框架为针对特定任务的剪枝。我们希望未来的研究能够继续这一工作，因为与一般蒸馏相比，从大型预训练模型进行的修剪可能会导致更少的计算，并导致更灵活的模型结构</li></ol><h2 id="方法-19"><a href="#方法-19" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/201.png" alt=""></p><p>目标稀疏：</p><p><img src="/images/summery1016_imgs/202.png" alt=""></p><p>隐层蒸馏损失：</p><p><img src="/images/summery1016_imgs/203.png" alt=""></p><p>动态映射关系：</p><p><img src="/images/summery1016_imgs/204.png" alt=""></p><p>蒸馏损失：</p><p><img src="/images/summery1016_imgs/205.png" alt=""></p><h2 id="实验-19"><a href="#实验-19" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/206.png" alt=""></p><h1 id="21-BIP"><a href="#21-BIP" class="headerlink" title="21 BIP"></a>21 BIP</h1><h2 id="简介-20"><a href="#简介-20" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Advancing Model Pruning via Bi-level Optimization</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/OPTML-Group/BiP">https://github.com/OPTML-Group/BiP</a></td></tr><tr><td>压缩技术</td><td>剪枝-训练范式并行处理，双层优化</td></tr></tbody></table></div><h2 id="笔记-6"><a href="#笔记-6" class="headerlink" title="笔记"></a>笔记</h2><ol><li>正如彩票假说（LTH）所示，修剪也有提高其泛化能力的潜力</li><li>获得较高的剪枝模型精度（如IMP）和较高的计算效率（如一次性剪枝）</li><li>剪枝-再训练学习范式涵盖了两种任务：❶剪枝决定了模型权值的稀疏模式，以及❷训练保留非零权值来恢复模型的精度</li><li>底层和上层优化的数据批量选择：我们在实现（θ-step）和（m-step）时，采用不同的数据批量（具有相同的批量大小）。这是BLO公式的优点之一，它可以灵活地定制底层和上层问题</li><li>在m上的离散优化：我们遵循“凸松弛+硬阈值”机制。具体地说，我们将二进制掩蔽变量放宽为连续掩蔽分数m∈[0,1]。然后我们得到基于松弛m的后传递损失梯度</li></ol><h2 id="方法-20"><a href="#方法-20" class="headerlink" title="方法"></a>方法</h2><p>将剪枝任务（即❶）和模型再训练任务（即❷）解释为两个优化级别，其中前者被表述为上层优化问题，并依赖于低层次再训练任务的优化。因此，我们将模型剪枝问题转换为以下BLO问题（❷嵌套在❶中）：</p><p><img src="/images/summery1016_imgs/207.png" alt=""></p><p>BLO可以灵活地在上层和下层的优化级别上分别使用不匹配的修剪和再训练目标。这种灵活性允许我们在(1)中规范底层训练目标函数，并在两个级别上定制已实现的优化方法。更具体地说，我们可以使用一个数据批处理（称为B2）来更新上层修剪掩码m，而不是不同于用于获取底层解决方案θ∗(m)的数据批处理（称为B1）。由此产生的BLO过程可以模拟元学习的想法来改进模型泛化[98]，其中低级问题使用B1对θ进行微调，而上层问题使用B2验证稀疏感知精细模型（m⊙θ∗(m)）的泛化。</p><p>在梯度下降的情况下，公式中的目标函数的梯度产生：</p><p><img src="/images/summery1016_imgs/212.png" alt=""></p><p>IG：</p><p><img src="/images/summery1016_imgs/213.png" alt="image-20231106190027013"></p><p>由于矩阵反演和二阶偏导数的存在，精确的IG公式(3)仍然难以计算。为了简化它，我们施加了无黑森假设，∇2θℓ=0，它一般是温和的</p><p>近似：</p><p><img src="/images/summery1016_imgs/214.png" alt=""></p><p>使用一阶近似：</p><p><img src="/images/summery1016_imgs/215.png" alt=""></p><p>两个步骤：</p><ul><li><p>用于模型再训练的较低级别的SGD：</p><p><img src="/images/summery1016_imgs/216.png" alt="image-20231106190355644"></p></li><li><p>用于修剪的上层SPGD：</p><p><img src="/images/summery1016_imgs/217.png" alt=""></p></li></ul><p><img src="/images/summery1016_imgs/218.png" alt=""></p><h2 id="实验-20"><a href="#实验-20" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/219.png" alt=""></p><h1 id="22-Compress-Then-Prompt"><a href="#22-Compress-Then-Prompt" class="headerlink" title="22 Compress,Then Prompt"></a>22 Compress,Then Prompt</h1><h2 id="简介-21"><a href="#简介-21" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>提示剪枝</td></tr></tbody></table></div><h2 id="笔记-7"><a href="#笔记-7" class="headerlink" title="笔记"></a>笔记</h2><ol><li>引入了一个新的视角，通过提示压缩模型来优化这种权衡。具体来说，我们首先观察到，对于某些问题，通过添加精心设计的硬提示，可以显著提高压缩LLM的生成质量</li><li>最近的一项研究调查了OPT-175B模型的推理过程，发现(1)令牌生成是导致推理延迟的主要因素，(2)多层感知器（MLP）在令牌生成过程中比注意块产生更高的I/O和计算延迟</li><li>设计提示的成功意味着三个巨大的潜力：<ul><li>跨数据集可移植性。这个人为设计的提示只提供了模型重量不准确的信息。因此，直观地说，不管使用的特定数据集，我们假设llm可以在相同的提示下生成更相关的响应。</li><li>交叉压缩的可转移性。类似地，人工设计的提示符只提到权重不准确，而没有指定确切的压缩级别或方法。我们假设llm可以在不同的压缩级别和方法中以相同的提示生成更多相关的响应</li><li>跨任务可转移性。如果llm能够理解它们的压缩状态并相应地进行调整，那么这种适应性并不局限于特定的任务或问题领域。相反，它可以扩展到广泛的任务</li></ul></li></ol><h2 id="方法-21"><a href="#方法-21" class="headerlink" title="方法"></a>方法</h2><p>数据驱动的方法来学习软提示：</p><p>剪枝之后，目标函数：</p><p><img src="/images/summery1016_imgs/221.png" alt=""></p><p>不更新模型参数：需要注意的是，压缩后的模型参数是固定的，不会被更新。唯一可训练的参数是提示标记的嵌入，它们由矩阵E表示，大小为k×d.初始化提示标记嵌入：提示标记的嵌入矩阵E的初始化采用了一种方法，其中每行的向量都是从LLM的标记嵌入矩阵W中随机选择的。</p><h2 id="实验-21"><a href="#实验-21" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/222.png" alt=""></p><h1 id="23-SPARSEGPT"><a href="#23-SPARSEGPT" class="headerlink" title="23 SPARSEGPT"></a>23 SPARSEGPT</h1><h2 id="简介-22"><a href="#简介-22" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SPARSEGPT: MASSIVE LANGUAGE MODELS CAN BE ACCURATELY PRUNED IN ONE-SHOT</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td><a href="https://github.com/ist-daslab/sparsegpt">https://github.com/ist-daslab/sparsegpt</a></td></tr><tr><td>压缩技术</td><td>非结构剪枝</td></tr></tbody></table></div><h2 id="笔记-8"><a href="#笔记-8" class="headerlink" title="笔记"></a>笔记</h2><ol><li>大规模生成式预训练变压器（GPT）家族模型可以在一次射击中修剪到至少50%的稀疏性，而不需要任何再训练，以最小的精度损失</li><li>我们的方法完全是局部的，在某种意义上，它仅仅依赖于权重更新，旨在保持每一层的输入-输出关系，这些计算没有任何全局梯度信息。因此，值得注意的是，人们可以在密集预训练模型的“邻域”中直接识别这种稀疏模型，其输出与密集模型的输出非常密切相关</li><li>较大的模型更容易稀疏：具体地说，我们发现，对于一个固定的稀疏水平，相对精度差距的密集和稀疏模型变体缩小我们增加模型大小，诱导50%稀疏性导致几乎没有精度减少最大的模型</li><li>一种特别流行的方法是将该问题划分为掩模选择和权重重建[20,27,22]。具体地说，这意味着首先根据一些显著性准则选择一个修剪掩模M，如权值大小[50]，然后在保持掩模不变的同时优化剩余的未修剪权值</li><li>在每个剪枝步骤之后，它执行权重更新，旨在保留每个层的输入-输出关系。这些更新的计算没有任何全局梯度信息。因此，大规模GPT模型的高度参数化使得我们的方法能够直接识别密集预训练模型的“近邻”中的稀疏精确模型。值得注意的是，由于我们的主要精度度量（困惑度）是非常敏感的，因此生成的稀疏模型的输出似乎与密集模型的输出非常密切相关。我们的第二个主要发现是，较大的模型更容易稀疏：在一个固定的稀疏水平，稀疏模型的相对精度下降，相对于密集的，缩小我们增加模型大小，诱导50%稀疏导致几乎没有精度减少最大的模型。这一发现应该被视为对未来压缩如此大规模的模型的工作非常令人鼓舞。</li></ol><h2 id="方法-22"><a href="#方法-22" class="headerlink" title="方法"></a>方法</h2><p>掩码的选择和权重的重建：</p><p>种特别流行的方法是将该问题划分为掩模选择和权重重建[20,27,22]。具体地说，这意味着首先根据一些显著性准则选择一个剪枝掩模M，如权值大小[50]，然后在保持掩模不变的同时优化剩余的未剪枝权值。重要的是，一旦掩模固定，(2)就变成一个线性平方误差问题，它是凸的，因此很容易优化。甚至可以通过对每个矩阵行应用标准的线性回归公式，以封闭的形式求解：</p><p><img src="/images/summery1016_imgs/231.png" alt=""></p><p>标准线性回归的方程通常表示为：</p><p>Y = X * β + ε</p><p>其中：</p><ul><li>Y 代表因变量（目标）。</li><li>X 代表自变量（输入特征）。</li><li>β 代表回归系数，它是一个向量，包含了每个输入特征的权重。</li><li>ε 代表误差项，表示模型无法完美拟合数据的部分。</li></ul><p>标准线性回归的目标是找到最佳的回归系数β，以使误差项ε的平方和最小化，通常通过最小二乘法（Least Squares）来实现。</p><p>封闭形式解是指可以通过数学公式直接求解得到的解。对于标准线性回归，封闭形式解是：</p><p>β = (X^T <em> X)^(-1) </em> X^T * Y</p><p><strong>更新权重：</strong></p><p><img src="/images/summery1016_imgs/232.png" alt=""></p><p><strong>每列迭代更新：</strong></p><p><img src="/images/summery1016_imgs/233.png" alt=""></p><p>根据误差自适应选择掩码：</p><p><img src="/images/summery1016_imgs/234.png" alt=""></p><p><img src="/images/summery1016_imgs/235.png" alt="image-20231107195956622"></p><h2 id="实验-22"><a href="#实验-22" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/236.png" alt=""></p><h1 id="24-OCB"><a href="#24-OCB" class="headerlink" title="24 OCB"></a>24 OCB</h1><h2 id="简介-23"><a href="#简介-23" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/ist-daslab/obc">https://github.com/ist-daslab/obc</a></td></tr><tr><td>压缩技术</td><td>非结构剪枝、局部剪枝</td></tr></tbody></table></div><h2 id="笔记-9"><a href="#笔记-9" class="headerlink" title="笔记"></a>笔记</h2><ol><li>我们给定了一个精确的训练模型，并且必须在没有任何再训练的情况下压缩它，仅基于少量的校准输入数据</li><li>OBS可以在DNN尺度上导致最先进的压缩，通过引入数值方法，可以近似OBS在现代模型的大量参数计数上所需的二阶信息。然而，这些方法并不适用于训练后的设置，因为它们需要逐步修剪，以及显著的再训练，以恢复良好的准确性。</li><li>AdaRound、AdaQuant和BRECQ的一个关键步骤是按顺序逐步量化层，这样在早期层中积累的误差就可以通过在后期层中的权重调整来补偿</li></ol><h2 id="方法-23"><a href="#方法-23" class="headerlink" title="方法"></a>方法</h2><p>剪枝目标：</p><p><img src="/images/summery1016_imgs/241.png" alt=""></p><p>OBS：</p><p><img src="/images/summery1016_imgs/242.png" alt=""></p><p>处理单行：</p><p>使用高斯分解，迭代海森逆矩阵：</p><p><img src="/images/summery1016_imgs/243.png" alt=""></p><p>根据剪枝目标函数快速求出权重的海森矩阵：</p><p>在标准的线性回归问题中，目标函数为：</p><p><img src="/images/summery1016_imgs/244.png" alt=""></p><p><img src="/images/summery1016_imgs/245.png" alt=""></p><p>所有行处理：</p><p>依次计算每个loss，从而计算全局掩码，之后再进行更新。两种方案：</p><p><img src="/images/summery1016_imgs/246.png" alt=""></p><h2 id="实验-23"><a href="#实验-23" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/247.png" alt=""></p><h1 id="25-AdaPrune"><a href="#25-AdaPrune" class="headerlink" title="25 AdaPrune"></a>25 AdaPrune</h1><h2 id="简介-24"><a href="#简介-24" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks NeurIPS</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2021</td></tr><tr><td>代码</td><td><a href="https://github.com/papers-submission/structured_transposable_masks">https://github.com/papers-submission/structured_transposable_masks</a></td></tr><tr><td>压缩技术</td><td>幅度剪枝、半结构化</td></tr></tbody></table></div><h2 id="笔记-10"><a href="#笔记-10" class="headerlink" title="笔记"></a>笔记</h2><ol><li>由于弗兰克尔和Carbin [12]通过应用密集训练发现了最佳面具（中奖彩票），所以如何在没有训练的情况下找到最佳面具的问题仍然悬而未决</li></ol><h2 id="方法-24"><a href="#方法-24" class="headerlink" title="方法"></a>方法</h2><p>MD：（越大越好）</p><p><img src="/images/summery1016_imgs/252.png" alt=""></p><p><img src="/images/summery1016_imgs/251.png" alt=""></p><p>转置掩码目标函数:</p><p><img src="/images/summery1016_imgs/253.png" alt=""></p><p>转变为最小流问题:</p><p><img src="/images/summery1016_imgs/254.png" alt=""></p><p>进一步近似算法：</p><p><img src="/images/summery1016_imgs/255.png" alt=""></p><p>4:8转到2:4，全量微调，1000K校正数据</p><p><img src="/images/summery1016_imgs/256.png" alt=""></p><h2 id="实验-24"><a href="#实验-24" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/257.png" alt=""></p><h1 id="26-Post-training-4-bit-quantization"><a href="#26-Post-training-4-bit-quantization" class="headerlink" title="26 Post training 4-bit quantization"></a>26 Post training 4-bit quantization</h1><h2 id="简介-25"><a href="#简介-25" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Post training 4-bit quantization of convolutional networks for rapid-deployment</th></tr></thead><tbody><tr><td>期刊</td><td>NeurIPS</td></tr><tr><td>发表时间</td><td>2019</td></tr><tr><td>代码</td><td><a href="https://github.com/submission2019/cnn-quantization">https://github.com/submission2019/cnn-quantization</a></td></tr><tr><td>压缩技术</td><td>四位量化</td></tr></tbody></table></div><h2 id="笔记-11"><a href="#笔记-11" class="headerlink" title="笔记"></a>笔记</h2><ol><li>本文介绍了第一种实用的4位训练后量化方法：它不涉及量化模型的训练（微调），也不需要完整数据集的可用性。</li><li>训练是补偿量化导致的模型精度损失的有效方法。然而，它并不总是适用于现实世界的场景，因为它需要全尺寸的数据集，而由于隐私、专有或使用现成的预先训练的模型，数据通常无法访问。培训也很耗时，需要很长时间的优化，以及熟练的人力和计算资源。</li></ol><h2 id="方法-25"><a href="#方法-25" class="headerlink" title="方法"></a>方法</h2><p><strong>ACIQ: Analytical Clipping for Integer Quantization</strong>:</p><p>通常，整数张量在张量的最大值和最小值之间被均匀地量化。在下面，我们证明了这是次优的:</p><p><img src="/images/summery1016_imgs/261.png" alt=""></p><p>位数为M，量化值为：</p><p><img src="/images/summery1016_imgs/262.png" alt=""></p><p>X与其量化版本Q (X)之间的期望均方误差：</p><p><img src="/images/summery1016_imgs/263.png" alt=""></p><p><img src="/images/summery1016_imgs/264.png" alt=""></p><p>量化噪声：</p><p><img src="/images/summery1016_imgs/265.png" alt=""></p><p>切片噪声：</p><p><img src="/images/summery1016_imgs/266.png" alt=""></p><p>总误差：</p><p><img src="/images/summery1016_imgs/267.png" alt=""></p><p>给定M情况下，求出ab关系：</p><p><img src="/images/summery1016_imgs/268.png" alt=""></p><p><strong>Per-channel bit-allocation</strong>：我们不是限制所有通道值具有相同的4位表示，而是允许一些通道具有更高的位宽，而限制其他通道具有更低的位宽。我们唯一的要求是，写入内存或从内存中读取的位的总数保持不变。</p><p><img src="/images/summery1016_imgs/269.png" alt=""></p><p>拉格朗日求解：</p><p><img src="/images/summery1016_imgs/2610.png" alt=""></p><p><img src="/images/summery1016_imgs/2611.png" alt=""></p><p>每个通道的最优解：</p><p><img src="/images/summery1016_imgs/2612.png" alt=""></p><p> <strong>Bias-Correction</strong>：量化之后具有固有偏差，在平均值和方差存在误差：</p><p><img src="/images/summery1016_imgs/2613.png" alt=""></p><p>于是提出修正：</p><p><img src="/images/summery1016_imgs/2614.png" alt=""></p><p><img src="/images/summery1016_imgs/2615.png" alt=""></p><p><strong>整合</strong></p><p>Weights and Activations: 对于权重和激活值，作者采用了逐通道的位分配策略。这意味着每个通道（例如，卷积层的不同滤波器）都可以有不同的位宽，而不是整个层都使用相同的位宽。这允许网络根据每个通道的需求来分配位数，从而更灵活地进行量化。</p><p>Reasoning: 作者在权重上没有采用任何形式的剪切操作。这是因为先前的研究（如Migacz, 2017; Zhao et al., 2019）已经指出，在较大的位宽情况下，即使用较多的位数表示权重时，采用权重剪切并没有明显的优势。</p><p>Weights vs. Activations: 作者在量化激活值时采用了ACIQ（Analytical Clipping for Integer Quantization）方法，但没有在权重上使用它。这可能是因为先前的研究或实验证据表明，在某些情况下，对激活值应用剪切可以更有效地减小量化引入的误差，而在权重上使用可能没有相同的优势</p><p>仅对权重进行偏差校正（Bias Correction for Weights Only）：</p><p>Offline vs. Online Bias Correction: 作者提到偏差校正（Bias Correction）的方法可以应用于权重和激活，但由于权重的偏差可以在离线情况下进行，而激活的偏差需要通过运行输入图像进行估计，因此只对权重进行了偏差校正。在线估计激活偏差可能会在运行时引入较大的计算开销。</p><h2 id="实验-25"><a href="#实验-25" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/2616.png" alt=""></p><h1 id="27-GPTQ"><a href="#27-GPTQ" class="headerlink" title="27 GPTQ"></a>27 GPTQ</h1><h2 id="简介-26"><a href="#简介-26" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS</th></tr></thead><tbody><tr><td>期刊</td><td>ICLR</td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td><a href="https://github.com/IST-DASLab/gptq">https://github.com/IST-DASLab/gptq</a></td></tr><tr><td>压缩技术</td><td>二阶近似量化</td></tr></tbody></table></div><h2 id="笔记-12"><a href="#笔记-12" class="headerlink" title="笔记"></a>笔记</h2><ol><li>OBQ以贪婪的顺序量化权值，即它总是选择当前导致的附加量化误差最小的权值。有趣的是，我们发现，虽然这种相当自然的策略似乎确实表现得很好，但它比以任意顺序量化权重的改进通常很小，特别是在大的、高度参数化的层上。最有可能的情况是，这是因为具有较小的量化权值数量被过程结束时被量化的权值所平衡，此时只剩下少数其他可以为补偿调整的未量化权值</li></ol><h2 id="方法-26"><a href="#方法-26" class="headerlink" title="方法"></a>方法</h2><p><strong>Step 1: Arbitrary Order Insight</strong>：以相同的顺序量化所有行的权值</p><p><img src="/images/summery1016_imgs/271.png" alt=""></p><p><strong>Step 2: Lazy Batch-Updates</strong>：只有一次块已经完全处理，我们执行全局更新整个H−1和W矩阵使用多权重版本的方程(2)和(3)如下：</p><p><img src="/images/summery1016_imgs/272.png" alt=""></p><p><strong>Step 3: Cholesky Reformulation</strong></p><p>整体算法：</p><p><img src="/images/summery1016_imgs/273.png" alt=""></p><h2 id="实验-26"><a href="#实验-26" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/274.png" alt=""></p><h1 id="28"><a href="#28" class="headerlink" title="28"></a>28</h1><h2 id="简介-27"><a href="#简介-27" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Improving Neural Network Quantization without Retraining using Outlier Channel Splitting</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2019</td></tr><tr><td>代码</td><td><a href="https://github.com/cornell-zhang/dnn-quant-ocs">https://github.com/cornell-zhang/dnn-quant-ocs</a></td></tr><tr><td>压缩技术</td><td>识别少量包含异常值的通道，复制它们，然后将这些通道中的值减半</td></tr></tbody></table></div><h2 id="笔记-13"><a href="#笔记-13" class="headerlink" title="笔记"></a>笔记</h2><ol><li>在训练深度神经网络（DNN）之后，网络的权重和激活值通常会呈现出钟形分布的特征而实际硬件在进行量化时使用线性量化网格，这意味着硬件会将连续的范围划分成等间隔的线性步长，将浮点数值映射到离散的量化级别。这样的线性量化网格可以简化硬件实现，但也带来了一个问题，即分布的形状与线性量化网格之间可能存在不匹配，导致一些数值落在量化级别之外，称为异常值。</li></ol><h2 id="方法-27"><a href="#方法-27" class="headerlink" title="方法"></a>方法</h2><p>分割通道，将值变小之后再量化：</p><p><img src="/images/summery1016_imgs/281.png" alt=""></p><p>分裂量化：</p><p><img src="/images/summery1016_imgs/282.png" alt=""></p><p>实现了无差量化：</p><p><img src="/images/summery1016_imgs/283.png" alt=""></p><h2 id="实验-27"><a href="#实验-27" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/285.png" alt=""></p><h1 id="29AWQ"><a href="#29AWQ" class="headerlink" title="29AWQ"></a>29AWQ</h1><h2 id="简介-28"><a href="#简介-28" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</th></tr></thead><tbody><tr><td>期刊</td><td></td></tr><tr><td>发表时间</td><td>2023</td></tr><tr><td>代码</td><td><a href="https://github.com/mit-han-lab/llm-awq">https://github.com/mit-han-lab/llm-awq</a></td></tr><tr><td>压缩技术</td><td>训练后量化，激活异常值感知</td></tr></tbody></table></div><h2 id="笔记-14"><a href="#笔记-14" class="headerlink" title="笔记"></a>笔记</h2><ol><li>GPTQ可能会在重建过程中过度拟合校准集，使分布外域上的学习特征发生扭曲，这可能会有问题，因为llm是多面体模型。</li><li>为了找到显著的权重通道，我们应该参考激活分布而不是权重分布，尽管我们只做权重量化</li><li>为了避免硬件效率低下的混合精度实现，我们分析了权重量化的误差，并推导出放大显著通道可以减少它们的相对量化误差。</li><li>局限性：尽管在FP16中保留0.1%的权重可以提高量化性能，而不显著增加模型大小（以总位测量），这种混合精度数据类型将使系统实现变得困难。我们需要提出一种方法来保护重要的重量，而不实际保留它们作为FP16。</li></ol><h2 id="方法-28"><a href="#方法-28" class="headerlink" title="方法"></a>方法</h2><p><img src="/images/summery1016_imgs/291.png" alt=""></p><p>原量化：</p><p><img src="/images/summery1016_imgs/292.png" alt=""></p><p>缩放：</p><p><img src="/images/summery1016_imgs/293.png" alt=""></p><p>目标函数：</p><p><img src="/images/summery1016_imgs/295.png" alt=""></p><p>网格搜索：</p><p><img src="/images/summery1016_imgs/294.png" alt=""></p><h2 id="实验-28"><a href="#实验-28" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/296.png" alt=""></p><h1 id="30AdaRound"><a href="#30AdaRound" class="headerlink" title="30AdaRound"></a>30AdaRound</h1><h2 id="简介-29"><a href="#简介-29" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>Up or Down? Adaptive Rounding for Post-Training Quantization</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2020</td></tr><tr><td>代码</td><td></td></tr><tr><td>压缩技术</td><td>训练后量化，非四舍五入近似</td></tr></tbody></table></div><h2 id="笔记-15"><a href="#笔记-15" class="headerlink" title="笔记"></a>笔记</h2><ol><li>四舍五入量化次优</li></ol><h2 id="方法-29"><a href="#方法-29" class="headerlink" title="方法"></a>方法</h2><p>约束量化范围：</p><p><img src="/images/summery1016_imgs/301.png" alt=""></p><p>目标函数：</p><p><img src="/images/summery1016_imgs/302.png" alt=""></p><p>二阶近似：</p><p><img src="/images/summery1016_imgs/303.png" alt=""></p><p>对角假设和常数假设：</p><p><img src="/images/summery1016_imgs/304.png" alt=""></p><p><img src="/images/summery1016_imgs/305.png" alt=""></p><p>松弛：</p><p><img src="/images/summery1016_imgs/306.png" alt=""></p><p><img src="/images/summery1016_imgs/307.png" alt=""></p><p>为了避免更深层次网络的量化误差的积累，并考虑到激活函数，我们使用了以下非对称重构公式：</p><p><img src="/images/summery1016_imgs/308.png" alt=""></p><h2 id="实验-29"><a href="#实验-29" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/309.png" alt=""></p><h1 id="31SPDY"><a href="#31SPDY" class="headerlink" title="31SPDY"></a>31SPDY</h1><h2 id="简介-30"><a href="#简介-30" class="headerlink" title="简介"></a>简介</h2><div class="table-container"><table><thead><tr><th>名称</th><th>SPDY: Accurate Pruning with Speedup Guarantees</th></tr></thead><tbody><tr><td>期刊</td><td>ICML</td></tr><tr><td>发表时间</td><td>2022</td></tr><tr><td>代码</td><td><a href="https://github.com/IST-DASLab/spdy">https://github.com/IST-DASLab/spdy</a></td></tr><tr><td>压缩技术</td><td>非结构加速，针对推理速度</td></tr></tbody></table></div><h2 id="笔记-16"><a href="#笔记-16" class="headerlink" title="笔记"></a>笔记</h2><ol><li>大多数现有的修剪方法只最小化剩余权值的数量，即模型的大小，而不是对推理时间进行优化。</li><li>我们首先观察到已知的度量，例如（标准化）权重大小，与得到的非结构化稀疏模型的优越精度不一致相关。</li><li>简单地用随机掩模施加相应的稀疏性，准确地估计这种稀疏性轮廓的运行时间</li><li>针对神经网络剪枝所导致的层次稀疏性的问题，无结构的加速技术并不依赖于在神经网络层中出现的特定模式。具体而言，即使在同一层次和相同稀疏度水平的情况下，无结构的加速技术也能够表现出相似的性能水平。这表明，该加速技术对于不同层次和不同剪枝策略都能够保持一致的效果，而不受到层次稀疏性中特定模式的影响。这种特性使得无结构的加速技术更具灵活性，能够适用于多种剪枝方法，而不受到这些方法所产生的特定稀疏模式的制约。</li></ol><h2 id="方法-30"><a href="#方法-30" class="headerlink" title="方法"></a>方法</h2><p>约束优化公式：</p><p><img src="/images/summery1016_imgs/3101.png" alt=""></p><p>递归公式：</p><p><img src="/images/summery1016_imgs/312.png" alt=""></p><p>计算误差：</p><p><img src="/images/summery1016_imgs/313.png" alt=""></p><p>步骤i:随机选100个$c$向量，找到最好的$c^*$.评估配置的质量：</p><p>该数据库为每个层ℓ和每个稀疏度s存储稀疏度s通过AdaPrune施加后剩余权重的“重建”。</p><p>首先，我们在数据库中查询每一层对应的重构权值，每一层在其目标稀疏度处。其次，我们将重建权重的结果模型“缝合”在一起，并在给定的小验证集上对其进行评估。</p><p><img src="/images/summery1016_imgs/314.png" alt=""></p><h2 id="实验-30"><a href="#实验-30" class="headerlink" title="实验"></a>实验</h2><p><img src="/images/summery1016_imgs/315.png" alt=""></p><p><img src="/images/summery1016_imgs/316.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PruneOnceforAll</title>
      <link href="/2023/10/09/PruneOnceforAll/"/>
      <url>/2023/10/09/PruneOnceforAll/</url>
      
        <content type="html"><![CDATA[<h1 id="Prune-Once-for-All"><a href="#Prune-Once-for-All" class="headerlink" title="Prune Once for All"></a>Prune Once for All</h1><p>Prune Once for All: Sparse Pre-Trained Language Models：</p><p>通过整合权重剪枝和模型蒸馏来训练稀疏的预训练Transformer语言模型。这些稀疏的预训练模型可以用于迁移学习，适用于各种任务，同时保持它们的稀疏模式。</p><h1 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h1><p>就准确性而言，BERT是在训练前阶段还是在迁移学习阶段被修剪并不重要。这表明，LM可以在训练前修剪一次，然后微调到任何下游任务，而不需要特定于任务的调整。</p><p>在本文中，提出了一种新的方法，为(Prune OFA），利用<strong>权重剪枝</strong>和<strong>模型蒸馏</strong>来产生预先训练的基于变压器的语言模型与高稀疏比。将方法应用于BERT-Base、BERT-Large和DistilBERT ，为这些模型架构生成稀疏的预训练模型。然后，展示了如何对这些稀疏模型进行微调，以生成四个特定任务的稀疏模型。文章还表明，可以使用量化感知训练进一步压缩模型，以在压缩-精度比方面实现最先进的结果。</p><p>本文方法不需要对每个任务的特殊剪枝超参数进行调整，因为本文对所有任务的模型进行一次修剪。</p><h1 id="Weight-pruning"><a href="#Weight-pruning" class="headerlink" title="Weight pruning"></a>Weight pruning</h1><p>在本文中，主要关注非结构化的权值剪枝。2018年提出了一种渐进幅度修剪（GMP）的方法，在训练过程中逐步以低幅度修剪权重。在训练过程中，每一个f步修剪最小幅度的权值，直到达到时间步$t$的时间稀疏比$s_t$，定义为：</p><p><img src="../images/PruneOFA_imgs/1.png" alt=""></p><p>其中$s_i$和$s_f$是初始和最终的稀疏度比，$t_s$和$t_e$是剪枝的开始和结束的时间步长。</p><p>在最近的一篇论文中，提出了一种基于IMP（迭代幅度剪枝）和学习率退卷（LRR）的剪枝算法。IMP包括两个步骤：修剪模型的一部分，并继续对其进行微调，以从诱导的剪枝错误中恢复。重复这两个步骤，直到达到期望的稀疏度比。在LRR中，学习速率调度器被恢复到它在微调步骤开始时的剪枝步骤之前的状态。文中建议将学习速率重绕原理合并到GMP中，即每f步在时间$t_s$时重绕到其状态。在测试之后，调度程序继续执行其原始设置，直到训练结束。</p><h1 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h1><p>在本文中，作者提出了利用模型蒸馏的方法来进行剪枝过程。作者关注的方法是，教师和学生共享相同的架构，但他们的稀疏比不同。在这种情况下，教师是一个在目标任务上进行训练的密集模型，而学生是一个具有固定稀疏性或正在进行修剪的模型。在训练前和微调阶段，可以在蒸馏阶段应用于语言模型。在训练前阶段，教师是一个预先训练过的语言模型，而在微调阶段，教师是一个针对目标任务进行微调的语言模型。</p><h1 id="Prune-Once-for-All-1"><a href="#Prune-Once-for-All-1" class="headerlink" title="Prune Once for All"></a>Prune Once for All</h1><p>该方法由两个步骤组成，即教师模型准备和学生模型剪枝。我们训练的稀疏预训练模型是我们用于迁移学习的模型，同时保持其稀疏模式。我们将这个方法称为“一次剪枝，用于所有”，因为我们展示了如何仅对预训练模型进行一次剪枝，然后对稀疏的预训练模型进行多个语言任务的微调。</p><p><img src="../images/PruneOFA_imgs/2.png" alt="Prune OFA method"></p><p><strong>Teacher preparation</strong></p><p>Prune OFA的第一步是获得一个在预训练任务中针对目标$L_{PT}$经过优化的模型，如图所示。相同的数据集将用于下一步对学生模型进行剪枝。这个模型将在学生模型剪枝步骤中初始化学生模型和教师模型。</p><p><strong>Student pruning</strong></p><p>学生模型是从在教师准备步骤中准备的教师模型中初始化的。然后，在来自教师准备步骤的预训练任务和知识蒸馏目标$L_{kd}$的线性组合上对学生模型进行微调：</p><p><img src="../images/PruneOFA_imgs/3.png" alt=""></p><p>同时使用$GMP + LRR$方法进行修剪。该过程的输出模型是一个稀疏预训练的LM，可以在不需要额外修剪的情况下用于迁移学习，从而为特定的下游任务生成稀疏模型。</p><p>在学生模型的微调过程中，使用了两个不同的目标来进行训练，这两个目标是从教师准备步骤中获得的：</p><ol><li>第一个目标是来自教师准备步骤的预训练任务，也就是在准备教师模型时使用的任务。这个任务的知识和经验被用来指导学生模型的微调。</li><li>第二个目标是知识蒸馏目标，通常表示为$L_{kd}$。知识蒸馏是一种训练方法，其中学生模型试图模拟教师模型的输出，以获取其知识和泛化能力。这个目标也被用于微调学生模型。</li></ol><p>这两个目标的线性组合意味着它们以某种权重的加权方式结合在一起，用来微调学生模型。这个组合可以帮助学生模型在微调过程中综合利用来自不同目标的信息，以提高性能。</p><p><strong>Pattern-lock</strong></p><p>作者希望在微调过程中保持由Prune OFA创建的稀疏预训练模型的稀疏性模式。提出了一种称为模式锁的方法，它可以防止在训练模型时改变模型中发现的零。</p><p>方法：在训练之前，Pattern-lock方法会为每个稀疏层$l$初始化一个掩码$M^l$，该掩码代表了该层的稀疏模式，其权重为$W^l$:</p><p><img src="../images/PruneOFA_imgs/4.png" alt=""></p><p>然后，在训练时，损失$L$梯度$w.r.t$修改为:</p><p><img src="../images/PruneOFA_imgs/5.png" alt=""></p><p>确保最初为0的权重将保持在0到整个微调.</p><h1 id="Experimental"><a href="#Experimental" class="headerlink" title="Experimental"></a>Experimental</h1><p>作者通过在三种不同大小的架构上应用Prune OFA来展示他们的方法；BERT-Base，BERT-Barge和DistilBERT。由于没有用于训练BERT-Base、BERT-Large和DistilBERT的原始处理训练数据，作者运行一个额外的步骤，使用作者准备的处理训练数据来微调预先训练过的模型。接下来，作者执行学生修剪步骤来获得他们的稀疏预训练模型。将BERT-Base和DistilBERT修剪到{85%，90%}，将BERT-Large修剪到90%。修剪应用于变压器编码器中的所有线性层，包括池器层如果存在的话。</p><p><strong>idea</strong></p><p>给预训练模型进行剪枝，然后针对特定任务进行微调。如果微调的时候继续剪枝？</p><p>如果给预训练模型进行低秩近似剪枝，然后微调？</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 蒸馏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>oBert</title>
      <link href="/2023/10/08/oBert/"/>
      <url>/2023/10/08/oBert/</url>
      
        <content type="html"><![CDATA[<h1 id="obert"><a href="#obert" class="headerlink" title="obert"></a>obert</h1><p><a href="https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT">github.com</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>直接看结果：</p><p><img src="../images/obert_imgs/1.png" alt=""></p><p>贡献：</p><ul><li>调研了lottery-ticket, movement pruning,magnitude and second-order pruning.</li><li>介绍了一种通用的二阶剪枝方法，称为最优BERT外科医生（oBERT），支持非结构化和块剪枝，是第一种既高精度又可扩展到BERT模型维数的二阶方法</li><li><strong>二阶剪枝方法需要逆黑森的近似，这对于LLM参数计数的存储和计算是昂贵的→未来如何近似逆海森矩阵？</strong></li></ul><h1 id="The-Optimal-BERT-Surgeon-oBERT"><a href="#The-Optimal-BERT-Surgeon-oBERT" class="headerlink" title="The Optimal BERT Surgeon (oBERT)"></a>The Optimal BERT Surgeon (oBERT)</h1><h2 id="Generalized-Second-Order-Block-Pruning"><a href="#Generalized-Second-Order-Block-Pruning" class="headerlink" title="Generalized Second-Order Block Pruning"></a>Generalized Second-Order Block Pruning</h2><p>令$W_M=M\odot W^<em>$其中$W^</em>\in R^d$是一个密集模型的权重，$d$是全部权重，$M\in \{0,1\}^d$表示掩码，即剪枝，于是使用泰勒展开式得到：</p><p><img src="../images/obert_imgs/2.png" alt=""></p><p>考虑到$W^<em>$优化良好，于是假定$\nabla L(W^</em>)\approx0$,通过修剪权值子集所引起的损失的变化可以表示为:</p><p><img src="../images/obert_imgs/3.png" alt=""></p><p><strong>→→→→如果不近似呢?如何推导？</strong></p><p>其中：</p><p><img src="../images/obert_imgs/4.png" alt=""></p><script type="math/tex; mode=display">\delta W :=W_M-W^*</script><p>在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵：</p><p><img src="../images/obert_imgs/5.png" alt=""></p><p>$m$是用于近似黑森的梯度外积的数量</p><p><strong>推导：</strong></p><p>对于一个将输入向量$in\in n_{in}$映射到输出向量$o\in n_0$的网络：</p><p><img src="../images/obert_imgs/6.png" alt=""></p><p>与训练集对应的均方误差定义为($P$是样本数，$t^{[k]}$是期望输出，$o^{[k]}$是实际输出)：</p><p><img src="../images/obert_imgs/7.png" alt=""></p><p>关于$W$的一阶导数是($\frac{\partial E}{\partial W}=\frac{\partial E}{\partial o}  \frac{\partial o}{\partial W}$)：</p><p><img src="../images/obert_imgs/8.png" alt=""></p><p>海森矩阵是：</p><p><img src="../images/obert_imgs/9.png" alt=""></p><p>考虑一个完全训练到$W*$的局部最小误差的网络,可以忽略$t^{[k]}-o^{[k]}$：</p><p><img src="../images/obert_imgs/10.png" alt=""></p><p>如果输出网络只有一个输出，我们可以将导数的n维数据向量$X^{[k]}$定义为：</p><p><img src="../images/obert_imgs/11.png" alt=""></p><p>于是海森矩阵可以写成:</p><p><img src="../images/obert_imgs/12.png" alt=""></p><p>如果网络是多元输出，则$X\in n×n_o$:</p><p><img src="../images/obert_imgs/13.png" alt=""></p><p>于是海森矩阵可以写成：</p><p><img src="../images/obert_imgs/14.png" alt=""></p><p>以上表明，$H$是与梯度变量$X$相关的样本协方差矩阵。对于单个输出情况，可以通过依次添加连续的“分量”计算完整的海森矩阵:</p><p><img src="../images/obert_imgs/15.png" alt=""></p><p>然后可以得到在$W^*$处近似海森矩阵的方法是通过一个衰减的经验fisher信息矩阵。</p><p>回到剪枝问题，识别一个给定形状的权重Q块，通过零掩蔽去除将导致最小的损失增加。这将导致以下约束优化问题：</p><p><img src="../images/obert_imgs/16.png" alt=""></p><p>一次剪枝一组权重Q，对于这个组中的权重，如果对其进行剪枝，则必须保证增量和原始值相同，如果不对其剪枝，则不用管它。</p><p>由拉格朗日乘数法得到权重更新：</p><p><img src="../images/obert_imgs/17.png" alt=""></p><p>Q权重的重要性：</p><p><img src="../images/obert_imgs/18.png" alt=""></p><h2 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h2><p>由于$\hat F^{-1}(W)$太难计算，文中使用了近似的方法。</p><h3 id="修剪最优的权重集"><a href="#修剪最优的权重集" class="headerlink" title="修剪最优的权重集"></a>修剪最优的权重集</h3><p>在实践中，评估每组Q的显著性得分$\rho_Q$，并修剪得分最低的$\frac{s×d}{|Q|}$组权重，其中$s\in (0,1]$表示稀疏度，$d$是全部权重。</p><h3 id="经验逆fisher矩阵计算"><a href="#经验逆fisher矩阵计算" class="headerlink" title="经验逆fisher矩阵计算"></a>经验逆fisher矩阵计算</h3><p>采用Woodbury/Sherman-Morrison(WSM) inversion formula：</p><script type="math/tex; mode=display">(A+uv^T)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}</script><p>得到：</p><p><img src="../images/obert_imgs/19.png" alt=""></p><p><img src="../images/obert_imgs/20.png" alt=""></p><h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>$N_B=\frac{d}{B}$表示总的块数，第一步计算：</p><p><img src="../images/obert_imgs/21.png" alt=""></p><p>第二步计算($\in R^{N_B}$)：</p><p><img src="../images/obert_imgs/22.png" alt=""></p><p>第三步计算：</p><p><img src="../images/obert_imgs/23.png" alt=""></p><h1 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h1><p>更新每组权重得分$\rho_Q$：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">scores<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">*</span> finv<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>更新$W^*$</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">obs_updates<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    <span class="token punctuation">.</span>mul<span class="token punctuation">(</span>        <span class="token punctuation">(</span>param<span class="token punctuation">.</span>data <span class="token operator">*</span> <span class="token punctuation">(</span>mask_diffs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_devices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_finvs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_eps<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>计算逆经验$Fisher$矩阵</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> g<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Updates empirical Fisher inverse with a new gradient    :param g: a collected gradient    """</span>    <span class="token comment"># if 'd / B' is not integer, pad with zeros for batch calculations</span>    <span class="token keyword">if</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">:</span>        g <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>            <span class="token punctuation">[</span>g<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>B <span class="token operator">-</span> g<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>g<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>    <span class="token comment"># prepare grad for batch calculations</span>    g <span class="token operator">=</span> g<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks<span class="token punctuation">,</span> self<span class="token punctuation">.</span>B<span class="token punctuation">)</span>    <span class="token comment"># batched f_inv x g: (batch, B, B) x (batch, B) -> (batch, B)</span>    finv_g <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bij,bj->bi"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>f_inv<span class="token punctuation">,</span> g<span class="token punctuation">)</span>    <span class="token comment"># scalar denominator for each batch: (batch)</span>    alpha <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>m <span class="token operator">+</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bi,bi->b"</span><span class="token punctuation">,</span> g<span class="token punctuation">,</span> finv_g<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    finv_g <span class="token operator">/=</span> alpha    <span class="token comment"># update f_inv with new outer product: (batch, B) x (batch, B) -> (batch, B, B)</span>    self<span class="token punctuation">.</span>f_inv<span class="token punctuation">.</span>baddbmm_<span class="token punctuation">(</span>finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> finv_g<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 二阶近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵导数</title>
      <link href="/2023/10/08/Hessian/"/>
      <url>/2023/10/08/Hessian/</url>
      
        <content type="html"><![CDATA[<h1 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h1><ul><li>一元函数：$f:R \rightarrow R$</li><li>多元函数：$f:R^n \rightarrow R$</li><li>向量函数：$f:R^n \rightarrow R^m$</li></ul><h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><p>导数针对一元函数$f:R \rightarrow R$，$f(x) \approx  f(x_0)+f^1(x_0)(x-x_0)$</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度针对多元函数$f:R^n \rightarrow R$，梯度是一个向量：$\nabla f=\begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \frac{\partial f}{\partial x_3}\end{bmatrix}$,也可以写作：函数相对于$\vec {x}$的梯度算子为$\nabla_x$。</p><script type="math/tex; mode=display">f{(\vec x)}\approx f(\vec{x_0})+\nabla f(\vec{x_0})\cdot (\vec x- \vec{x_0})</script><h2 id="Jacobian-矩阵"><a href="#Jacobian-矩阵" class="headerlink" title="$Jacobian$矩阵"></a>$Jacobian$矩阵</h2><p>针对向量函数：$f:R^n \rightarrow R^m$</p><p><img src="../images/hessian_imgs/1.png" alt=""></p><p>矩阵分量：</p><script type="math/tex; mode=display">\textbf{J}_{ij}=\frac{\partial f_i}{\partial x_j}</script><p>其他常用的符号：</p><script type="math/tex; mode=display">Df、\textbf{Df}、\textbf{J}_f(x_1,...,x_n),\frac{\partial(f_1,...,f_m)}{\partial (x_1,...,x_n)}</script><p>近似：</p><script type="math/tex; mode=display">f{(\vec x)}\approx f(\vec{x_k})+J (\vec{x_k})\cdot (\vec x- \vec{x_k})</script><h1 id="Hessian-矩阵"><a href="#Hessian-矩阵" class="headerlink" title="$Hessian$矩阵"></a>$Hessian$矩阵</h1><p>使用于：$f:R^n \rightarrow R$，是函数的二阶矩阵：</p><p><img src="../images/hessian_imgs/2.png" alt=""></p><p>这是一个$n×n$的方阵，可以写成：</p><script type="math/tex; mode=display">\textbf{H}_{ij}=\frac{\partial ^2f}{\partial x_i \partial x_j}</script><p>近似：</p><p><img src="../images/hessian_imgs/3.png" alt=""></p><p><img src="../images/hessian_imgs/4.png" alt=""></p><p><img src="../images/hessian_imgs/5.png" alt=""></p><h1 id="Fisher-矩阵"><a href="#Fisher-矩阵" class="headerlink" title="$Fisher$矩阵"></a>$Fisher$矩阵</h1><p>$Fisher \ information$:假设观察到的数据$X_1,X2,.,X_n$服从一个概率分布$f(X;\theta)$,$\theta$是目标参数，那么似然函数$likelihood$：</p><p><img src="../images/hessian_imgs/6.png" alt=""></p><p>为了解开方程，需要$\log(likelihood)$的一阶导数为0，其一阶导数$Score\ function$：</p><p><img src="../images/hessian_imgs/7.png" alt=""></p><p>那么$Fisher\ information$用$I(\theta)$表示，定义即$Score\ function$的二阶矩：</p><p><img src="../images/hessian_imgs/8.png" alt=""></p><p>现证明$E[S(X;\theta)]=0$：</p><p><img src="../images/hessian_imgs/9.png" alt=""></p><p>从而得到：</p><p><img src="../images/hessian_imgs/10.png" alt=""></p><p>于是得到$Fisher \ information$的第一条数学意义：用来估计$Maximum\ Likelihood \ Estimate$方程的方差。即收集到的数据越多，象征着得到的信息越多。</p><p>对于$\theta$有多大把握，可以围绕估计值的期望，根据模型评分的协方差定义一个不确定性度量：</p><p><img src="../images/hessian_imgs/11.png" alt=""></p><p>上面评分函数的协方差即$Fisher $信息的定义，一般$\theta $ 是一个向量，即$Fisher $信息是以矩阵形式存在，称为$Fisher$信息矩阵$FIM$：</p><p><img src="../images/hessian_imgs/12.png" alt=""></p><p>一般情况下似然函数是复杂的，很难计算期望值，因此可以使用经验分布来近似$F$中的期望值。它由训练数据$X={X_1,X_2,.,X_N}$给出，即：</p><p><img src="../images/hessian_imgs/13.png" alt=""></p><h1 id="Fisher-和-Hessian"><a href="#Fisher-和-Hessian" class="headerlink" title="$Fisher$和$Hessian$"></a>$Fisher$和$Hessian$</h1><p>对数似然的负期望$Hessian$，等于$Fisher$信息矩阵。</p><p>对数似然的$Hessian$为：</p><p><img src="../images/hessian_imgs/14.png" alt=""></p><p>期望：</p><p><img src="../images/hessian_imgs/15.png" alt=""></p><p>因此：</p><p><img src="../images/hessian_imgs/16.png" alt=""></p><p>费舍尔信息矩阵被定义为评分函数的协方差，它是一个曲率矩阵，可以理解为对数似然函数的黑森负期望。因此，F的直接应用，是在二阶优化方法中替换H </p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/question/26561604">(4 封私信) 费雪信息 (Fisher information) 的直观意义是什么？ - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/228099600">费舍尔信息矩阵及自然梯度法 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bilibili.com/video/BV17B4y1y7wX/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=50e29aacf23dfe7179edd1b5d8ece200">【TRPO系列讲解】（二）Hessian矩阵、Fisher信息矩阵、KL散度_哔哩哔哩_bilibili</a></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线代 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoraPrune</title>
      <link href="/2023/09/26/LoraPrune/"/>
      <url>/2023/09/26/LoraPrune/</url>
      
        <content type="html"><![CDATA[<h1 id="文章-LoRAPrune"><a href="#文章-LoRAPrune" class="headerlink" title="文章: $LoRAPrune$"></a>文章: $LoRAPrune$</h1><p>文章链接：<br>代码链接：</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>神经网络剪枝可以压缩模型，目前大多数方法依赖于计算参数的梯度，但是计算参数梯度开销很大，文章提出$LoRAPrune$方法，首先设计一个$PEFT$感知的剪枝准则，它利用低等级自适应（$LoRA$）的值和梯度，而不是预先训练的参数的梯度来进行重要性估计。然后，提出了一个迭代剪枝方法，以去除冗余参数，同时最大化$PEFT$的优势。</p><p>在各种任务上的实验结果表明，该方法取得了最先进的结果。例如，在$VTAB-1k$基准中，$LoRAPrune$仅利用了0.76%的可训练参数，显著优于幅度和运动剪枝方法，分别高出5.7%和4.3%。此外，该方法实现了与$PEFT$方法相当的性能，突出了其在提供高质量结果方面的有效性，同时受益于剪枝的优势。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>神经网络剪枝是一种流行的模型压缩技术，通过去除冗余参数，可以显著降低这些大型模型的规模和复杂性。大多数最先进的评估参数重要性的方法都需要参数的梯度。</p><p>莫尔查诺夫引入了一种技术，该技术使用泰勒展开来近似由剪枝引起的损失波动，并使用一阶项来评估参数的重要性。同样，Yu 开发了一种基于梯度显著性评分的方法来评估参数的重要性，Zhang 提出了灵敏度平滑作为计算参数重要性的方法。此外，剪枝过程经常被纳入作为迭代剪枝-再训练循环的一部分，以恢复模型的精度。但是微调和计算梯度代价是昂贵的。</p><p>参数高效调优方法：LoRA插入一组可训练的并行或串行的低秩矩阵。插入的低秩矩阵中的参数数仅为模型参数的1%左右。在下游任务的微调期间，原始参数被冻结（即不更新，不计算梯度），只有插入的低秩矩阵被更新以近似参数更新。由于LoRA只更新了少量的参数，与全参数微调方法相比，它的优化难度和计算需求显著降低。然而，PEFT通常需要冻结的预训练参数，而不计算它们的梯度，依赖于预训练参数的梯度的剪枝方法不能直接应用于这些大语言模型。</p><p>一个想法：是否可以利用LoRA的低秩矩阵的梯度来评估预训练参数的重要性。因此提出LoRAPrune：只使用LoRA的梯度。与下图中描述的梯度引导剪枝方法相比，LoRAPrune利用LoRA的梯度作为预先训练的参数梯度的近似值。因此，$LoRAPrune$实现了对lpm中的冻结参数进行修剪的目标。</p><p><img src="../images/1.png" alt="两种剪枝方案"></p><p>上图将LoRAPrune（左）与现有的梯度引导剪枝方法（右）进行比较： (a) LoRAPrune通过在整个过程中只计算低秩矩阵，可以对大规模模型进行高效的调优和剪枝。(b)传统的剪枝方法需要从大尺度参数中获得梯度。颜色（红色）表示可训练参数，颜色（蓝色）表示冻结参数，颜色（黄色）表示渐变。</p><p><strong>本文贡献：</strong></p><ul><li>利用从模型的低秩分解中导出的梯度来近似估算预训练参数的重要性。通过这样做，它避免了直接计算所有参数的梯度的需求。</li><li>提出一种将PEFT与剪枝相结合的方法。与其他PEFT方法相比，LoRAPrune能够部署具有相似数量训练参数的轻量级预训练模型。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p><strong>Parameter-efficient tuning</strong>：参数高效微调</p><p><strong>Neural network pruning</strong>：神经网络剪枝。如何确定参数的重要性仍然需要进行研究</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h3 id="初步"><a href="#初步" class="headerlink" title="初步"></a>初步</h3><p>首先重新讨论了具有结构重参数化的参数高效自适应方法。为了有效地微调神经网络的参数，目标模块（如全连接层）可以以并行或顺序的方式插入一个LoRA到预先训练好的参数中。在训练过程中，预先训练的参数被冻结，不计算其梯度，而插入的LoRA是可训练的。</p><p><strong>Parallel low-rank adaption</strong>：给定两个低秩矩阵$A\in R^{r×k}$和$B\in R^{d×r}$，通过并行低秩自适应微调的目标模块的正向过程可以写为：</p><p><img src="../images/2.png" alt=""></p><p>$W_0,z\in R^{n×k},x\in R^{n×d}$表示原始目标模块的权重、输出和输入。自适应后，新的权重$W$可以重新参数化为$W = W_0 + BA$。</p><p><strong>Sequential low-rank adaption</strong>:目标模块在顺序低秩自适应中的前向过程可以写为:</p><p><img src="../images/3.png" alt=""></p><p>自适应后，新的权重$W$可以重新参数化为$W = (BA+E)W_0$。</p><h3 id="低秩梯度判据"><a href="#低秩梯度判据" class="headerlink" title="低秩梯度判据"></a>低秩梯度判据</h3><p>一个参数$w_{ij}∈W_0$的重要性可以通过去除它所引起的损失来量化。对于输入x和相应的标签y，$w_ij$的诱导误差可以被测量为有无参数的预测误差的平方差：</p><p><img src="../images/4.png" alt=""></p><p>为每个参数计算成本很高。使用一阶泰勒展开式来近似重要性：</p><p><img src="../images/5.png" alt=""></p><p>在LPM中获取$W_0$的梯度是困难的，因为它需要大量的计算能力和存储空间。在本文中，作者讨论了如何通过在下游任务自适应中插入可学习矩阵$A$和$B$来修剪预先训练好的参数$w0$。如上所述，$A$和$B$可以以平行或顺序的方式插入到预先训练好的模型中。因此，文章分别讨论了这两种情况下相应的剪枝方法。</p><p><strong>Pruning for parallel adapter</strong>：在并行的情况下，如果删除元素$w_{ij}∈W$，可以设置元素$（BA）_{ij} =−w_{ij}$。等式（3）中每个参数的重要性可以重新表述如下：</p><p><img src="../images/6.png" alt=""></p><p>利用$（BA）_{ij} =−w_{ij}$的一阶泰勒展开式来近似等式(5)，参数$w_{ij}$的估计重要性可以表示为:<br><img src="../images/7.png" alt=""></p><p>接下来只保存并使用两个低秩矩阵A和B的梯度来近似：</p><p><img src="../images/8.png" alt=""></p><p><img src="../images/9.png" alt=""></p><p><strong>在(8)中，是否可以改变系数？</strong></p><p>下所示，LoRA梯度准则只需要计算A和B的梯度，与预训练的总权值W0的梯度相比，节省了内存和计算量。</p><p><img src="../images/10.png" alt=""></p><h3 id="LORA剪枝"><a href="#LORA剪枝" class="headerlink" title="LORA剪枝"></a>LORA剪枝</h3><p>使用移动平均值来评估参数的重要性。具体来说，第t次迭代时的参数重要性计算如下（它允许在模型训练的早期阶段就开始剪枝操作，而不必等待整个模型完全收敛，从而节省了时间和计算资源）：</p><p><img src="../images/11.png" alt=""></p><p>插入一个二进制掩模$\beta∈\{0,1\}^{d×k}$作为参数，然后使用<strong>修剪-微调-修剪</strong>方法进行修剪。考虑到每个参数的重要性，在每次剪枝迭代中通过设置相应的掩码为1，并将其余参数设置为0来保留Top-k个重要参数。形式上，每一个修剪层的正向过程可以写成:</p><p><img src="../images/12.png" alt=""></p><p>算法如下：</p><p><img src="../images/13.png" alt=""></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在3090GPU上运行实验，采用修剪-微调-修剪操作。证明了将微调和剪枝过程相结合具有一定的时间效率，且不会影响plm的性能。</p><h1 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h1>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2023/09/20/SVD/"/>
      <url>/2023/09/20/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h1><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>奇异值分解$(Value Decomposition$，简称$SVD)$是一种在线性代数和矩阵分析中非常重要的数学技术，它可以将一个矩阵分解为三个矩阵的乘积，具体来说，将一个矩阵$A$分解为以下形式：</p><script type="math/tex; mode=display">A = UΣV^T</script><p>$SVD$的关键性质和应用包括：</p><ol><li>数据降维：$SVD$可用于将高维数据降维到低维，通过保留最重要的奇异值和对应的奇异向量，可以实现数据压缩和特征选择。</li><li>矩阵逆：$SVD$可用于计算矩阵的伪逆，对于非方阵或奇异矩阵尤其有用。</li><li>奇异值阈值截断：通过保留前k个最大的奇异值和相应的奇异向量，可以实现矩阵的低秩近似，用于图像压缩、推荐系统等。</li><li>主成分分析$(PCA)$：$SVD$可以用于$PCA$，通过对数据协方差矩阵进行$SVD$分解，可以找到数据的主成分。</li><li>推荐系统：$SVD$在协同过滤中有广泛应用，用于推荐用户可能感兴趣的物品。</li><li>图像压缩：$SVD$可用于图像压缩和去噪，通过保留奇异值较大的部分，可以减小图像尺寸并去除一些噪声。</li></ol><h3 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2 矩阵"></a>2 矩阵</h3><p>矩阵的意义：<a href="https://www.cnblogs.com/marsggbo/p/10144060.html">【转载】理解矩阵（三） - marsggbo - 博客园 (cnblogs.com)</a></p><p>以$Ma=b$为例介绍矩阵$M$的含义：</p><ul><li>从变换的角度来说，矩阵$M$可以理解为对向量$ a$做变换得到了 $b$。</li><li>坐标系的角度来说，$M$可以理解成是一个坐标系（常用的坐标是笛卡尔坐标系，即 $I$），向量$a$就是在$M$这个坐标系下的坐标，$a$对应到$I$坐标系下的坐标是向量 $b$。</li></ul><h3 id="2-特征值分解"><a href="#2-特征值分解" class="headerlink" title="2 特征值分解"></a>2 特征值分解</h3><p><a href="https://blog.csdn.net/zpalyq110/article/details/86751064">奇异值分解(SVD)原理及实例分析_Freeman_zxp的博客-CSDN博客</a></p><h3 id="2-1-特征值和特征向量"><a href="#2-1-特征值和特征向量" class="headerlink" title="2.1 特征值和特征向量"></a>2.1 特征值和特征向量</h3><h5 id="特征值（Eigenvalues）："><a href="#特征值（Eigenvalues）：" class="headerlink" title="特征值（Eigenvalues）："></a>特征值（Eigenvalues）：</h5><ol><li><p>特征值是一个矩阵的标量性质，通常用λ表示。</p></li><li><p>特征值告诉我们矩阵在某个方向上的缩放因子或拉伸因子。</p></li><li><p>特征值的数目等于矩阵的维度。</p></li><li><p>特征值可以是实数或复数。</p></li></ol><p><strong>对于2：</strong>考虑一个二维平面上的线性变换，由一个矩阵A表示。我们有一个单位向量v（长度为1），它表示一个在该平面上的方向。当我们将这个向量v乘以矩阵A时，我们得到另一个向量Av。如果Av与v的方向相同（可能只是相反方向），那么这意味着矩阵A并没有改变该方向，只是对向量进行了缩放或拉伸。特征值就是用来表示这个缩放或拉伸的因子。具体来说，如果λ是矩阵A的一个特征值，而v是对应的特征向量，那么当我们将向量v乘以矩阵A时，结果是λv。这意味着矩阵A对向量v的作用只是将它缩放为原来的长度的λ倍。如果λ大于1，那么矩阵A在v的方向上对向量进行了拉伸；如果0 &lt; λ &lt; 1，那么矩阵A在v的方向上对向量进行了压缩；如果λ为负数，那么矩阵A对向量进行了反转，并改变了它的方向。所以，特征值λ告诉我们在特定方向v上矩阵A的作用是如何改变向量的大小或方向的。</p><h5 id="特征向量（Eigenvectors）："><a href="#特征向量（Eigenvectors）：" class="headerlink" title="特征向量（Eigenvectors）："></a>特征向量（Eigenvectors）：</h5><ol><li><p>特征向量是与特征值相关联的向量，通常用v表示。</p></li><li><p>特征向量表示在矩阵变换下不改变方向的向量。</p></li><li><p>特征向量描述了矩阵的变换性质，即它们定义了矩阵的主要方向。</p></li><li><p>特征向量通常标准化为单位向量。</p></li></ol><h3 id="2-2-特征值分解"><a href="#2-2-特征值分解" class="headerlink" title="2.2 特征值分解"></a>2.2 特征值分解</h3><p>特征值分解的实质是求解给定矩阵的特征值和 特征向盘，提取出矩阵最重要的特征。<br>既然我们知道一个矩阵是可以通过特征值和特征向量来表示，那假设存在一个$n×n$的满秩对称矩阵$A$，我们便可以通过特征值将$A$分解。首先求出$A$的$n$个特征值：$\lambda_1,\lambda_2,…,\lambda_n$,以及对应的特征向量(标准化处理后的):$x_1,x_2,…,x_n$。于是：</p><script type="math/tex; mode=display">Ax_1=\lambda_1x_1 \\\\Ax_2=\lambda_2x_2 \\\\……\\\\Ax_n=\lambda_nx_n</script><p>令$U=[x_1,x_2,…,x_n]$,$\Lambda=\begin{bmatrix} \lambda_1 &amp; 0 &amp; 0 \\\\ 0 &amp; \lambda_2 &amp; 0 \\\\ 0 &amp; 0 &amp; \lambda_n \end{bmatrix}$化简公式为：$AU=U\Lambda$。$U$是正交阵，有$U^T=U^{-1}$。最终：</p><script type="math/tex; mode=display">A=U\Lambda U^{-1}=U\Lambda U^T</script><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3 奇异值分解"></a>3 奇异值分解</h3><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p>对于满秩对称矩阵，可以简单的通过计算特征值进行分解，对于$m×n$的一般矩阵，需要使用奇异值分解$SVD$。已知对于任意矩阵都满足$A^TA,AA^T$，为对称矩阵，因此可以对$A^TA,AA^T$进行分解。</p><p>定义矩阵$A$的$SVD$为：</p><script type="math/tex; mode=display">A=U\Sigma V^T</script><p>其中$U$是$m×m$的矩阵，$\Sigma$是$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值。$V$是$m×m$的矩阵,$U$和$V$都是酉矩阵，满足$U^TU=I,V^TV=I$。</p><p><strong>计算：</strong></p><p>首先得到$n×n$的方阵$A^TA$。然后进行特征值分解，得到的特征值和特征向量满足下式：</p><script type="math/tex; mode=display">(A^TA)v_i=\lambda_iv_i</script><p>这样我们就可以得到矩阵 $A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将  $A^TA$ 的所有特征向量张成一个$n×n$的矩阵$V$,即$SVD$公式中的矩阵$V$,一般我们将$V$中的每个特征向量叫做A的右奇异向量。</p><p>然后得到$m×m$的方阵$AA^T$。然后进行特征值分解，得到的特征值和特征向量满足下式：</p><script type="math/tex; mode=display">(AA^T)u_i=\lambda_iu_i</script><p>这样我们就可以得到矩阵 $AA^T$的$n$个特征值和对应的$n$个特征向量$u$了。将  $AA^T$ 的所有特征向量张成一个$m×m$的矩阵$U$,即$SVD$公式中的矩阵$U$,一般我们将$U$中的每个特征向量叫做A的左奇异向量。</p><p>对于$\Sigma$,除了对角线上是奇异值其他位置都是0,因此只需要求出每个奇异值$\sigma$即可。注意到：</p><script type="math/tex; mode=display">A=U\Sigma V^T\Rightarrow AV=U\Sigma V^TV\Rightarrow AV=U\Sigma\Rightarrow Av_i=\sigma_iu_i\Rightarrow \sigma_i=Av_i/u_i</script><p>可以求出奇异值矩阵$\Sigma$。</p><p>证明$A^TA$的特征向量组成的就是$SVD$中的$V$矩阵：</p><script type="math/tex; mode=display">A=U\Sigma V^T\Rightarrow A^T=V\Sigma U^T\Rightarrow A^TA=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T\ (U^TU=I)</script><p>进一步可以看到特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：</p><script type="math/tex; mode=display">\sigma_i=\sqrt{\lambda_i}</script>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoSparse</title>
      <link href="/2023/09/19/LoSparse/"/>
      <url>/2023/09/19/LoSparse/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1950343972&auto=1&height=66"></iframe><h1 id="文章题目：-LoSparse"><a href="#文章题目：-LoSparse" class="headerlink" title="文章题目：$LoSparse$"></a>文章题目：$LoSparse$</h1><p><strong>文章链接：</strong><a href="https://arxiv.org/pdf/2306.11222.pdf">https://arxiv.org/pdf/2306.11222.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/yxli2123/LoSparse">https://github.com/yxli2123/LoSparse</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>$transform$模型需要大量的计算资源，出于模型压缩的目的，作者提出了$LoSparse(Low-Rank and Sparse approximation)$模型，它通过一个<strong>低秩矩阵</strong>和一个<strong>稀疏矩阵</strong>的和来逼近一个权重矩阵。该方法结合了低秩近似和剪枝的优点，同时避免了它们的限制。剪枝增强了低秩近似的多样性，低秩近似可以防止剪枝损失太多的表达神经元</p><ul><li>低秩近似：压缩了神经元中的一致和表达丰富的部分</li><li>剪枝：去除了神经元中的不一致和非表达丰富的部分</li></ul><p>低秩可以压缩权重中相关性大的部分，但是完全忽略了相关性较小的部分（这些相关性小的部分在神经元之间可能是独特的，具有表达能力，并且对于模型的性能来说可能非常关键），结合剪枝，可以继续减少相关性小的那部分但是不至于全部去除导致网络多样性下降。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>目前模型压缩常用的是剪枝,分为结构性剪枝和非结构性剪枝。结构性剪枝一种方式为ITP，即迭代剪枝，可以同时训练和剪枝，本文用到的剪枝方法就是迭代剪枝。</p><p>为什么只用剪枝不行：</p><p><img src="../images/image-20230918230903444.png" alt="神经元重要性得分直方图"></p><p>(b)为理想情况，大多数神经元是冗余的，对于网络来说不重要，只有小部分神经元是重要的，但是现实情况为(a)，很大一部分神经元是可表达的，因此大量剪枝可能导致重要的神经元会被减去。</p><p>低秩近似可以提取相关性大的权重的公共基。但是$transformer$模型的秩很高，它们包含许多参数，这使得简单地应用低秩近似来压缩这些矩阵可能会损害模型的性能。这是因为这种情况下忽略了神经元的多样性。</p><p>因此提出了低秩近似和稀疏近似的结合：低秩近似防止了修剪过度去除表达神经元，而稀疏近似增强了低秩近似的多样性。</p><p>同时该方法与知识蒸馏是正交关系，很容易再集成知识蒸馏手段，提高模型性能。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>介绍了transform模型和权重敏感性评分公式：</p><p><img src="../images/image-20230918232543247.png" alt=""></p><p>为了减少随机抽样导致的可变性，本文使用平滑公式:</p><p><img src="../images/image-20230918232355166.png" alt=""></p><p>结构性剪枝神经元的重要性评分公式：</p><p><img src="../images/image-20230918232819344.png" alt=""></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>通过一个低秩矩阵和一个稀疏矩阵的和来近似一个权矩阵：</p><p><img src="../images/image-20230918233005983.png" alt="LoSparse 在单个线性投影矩阵的示意图"></p><h3 id="低秩矩阵和稀疏矩阵的近似"><a href="#低秩矩阵和稀疏矩阵的近似" class="headerlink" title="低秩矩阵和稀疏矩阵的近似"></a>低秩矩阵和稀疏矩阵的近似</h3><p>给定一个权重矩阵$W\in R^{d_1×d_2}$,通常采用结构化剪枝稀疏矩阵$S\in R^{d_1×d_2}$来近似$W$以进行压缩。然而稀疏矩阵近似导致性能不佳，尤其是当压缩比率较高时。因此，本文引入了一个低秩矩阵来改进近似。具体来说，权重矩阵可以表示为：</p><p><img src="../images/image-20230918233726382.png" alt=""></p><p>其中$U\in R^{d_1×d_2}$和$R\in R^{d_1×d_2}$的乘积表示秩为$r$的低秩矩阵。</p><p><strong>为什么需要低秩矩阵？</strong></p><p><img src="../images/image-20230918234337880.png" alt="语言模型的奇异值"></p><p>首先，<strong>它可以有效地逼近神经元的相干部分</strong>，我们可以看到语言模型中权重矩阵的频谱在开始时迅速下降。<strong>频谱下降反映模型中特征之间的相关性。较大的特征值通常对应于具有更高相关性的特征。因此，快速下降的频谱表示模型中存在一些高度相关的特征，即一致部分</strong>，因此可以通过低秩来压缩公共部分。</p><p>其次<strong>低秩矩阵和稀疏矩阵的解耦使得剪枝变得容易</strong>。<strong>频谱的趋于平稳表示模型的权重矩阵中的特征或模式已经相对稳定地学习和表示。这也可能表明在训练过程中，神经元之间的相关性逐渐减弱，即神经元对不同的特征或模式变得更加独立</strong>而低秩矩阵无法捕获这些信息，但是低秩矩阵能够将相干部分与神经元的非相干部分解耦。因此可以添加一个矩阵$S$来近似剩余的不连贯部分，然后修剪非表达不连贯的部分。</p><p><img src="../images/image-20230918230903444.png" alt="线性投影的神经元的重要性得分分布情况"></p><p>上图表示，大多数不连贯的部分在解耦之后具有较低的重要性分数，因此可用剪枝删除冗余参数。LoSparse算法成功地分离了神经元中不连贯的部分，并简化了非表达成分的修剪。(和ideal的图逼近)。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>给定一个预训练的权重矩阵$W^{(0)}$，首先基于$W^{(0)}$的奇异值分解（SVD）初始化秩 $r$的低秩矩阵。具体来说，本文选择：</p><p><img src="../images/image-20230919000424683.png" alt=""></p><p>$u_1,u_2,.,u_r\in R^{d_1}$是左奇异向量，$v_1,v_2,.,v_r\in R^{d_1}$是左奇异向量，关于上面的$r$个奇异值$\sigma_1 \geq \sigma_2 \geq .\geq\sigma_3$在$W^{(0)}$的SVD中。</p><p>因此初始化$S^{(0)}$为：</p><p><img src="../images/image-20230919001321431.png" alt=""></p><p>因此，原始的前向传递（$Y=XW$）可替换为更高效的形式:</p><p><img src="../images/image-20230919001529751.png" alt=""></p><p>作者将上面这种分解应用到了每一个权重矩阵并且将$S=\{S_m\}_{m=1}^{M}$表示为所有稀疏矩阵的集合。初始化$S$之后，对所有$S$进行迭代剪枝。</p><p><strong>具体方法：</strong></p><p>在第t次迭代时，首先采取随机梯度下降步骤来更新$U^{(t)}$,$V^{(t)}$和$S^{(t)}$。对于$S^{(t)}$，</p><p><img src="../images/1695089652536.jpg" alt=""></p><p>这是损失函数 $L$ 对于变量 $S^{(t)}$的梯度。梯度是一个向量，它包含了损失函数相对于每个分量（或维度）的偏导数。</p><p>然后在公式(4)的基础上对$S^{(t)}$进行重要性评分，$\widetilde{S}^{(t)}$剪枝公式如下：</p><p><img src="../images/1111.png" alt=""></p><p>$\tau( {\widetilde{S}^{(t)}},\Gamma({S}^{(t)}))$的第$i$列定义如下：</p><p><img src="../images/1695090856116.jpg" alt=""></p><p>在每个迭代中，选择性地保留了权重矩阵 $S^{(t)}$中具有高重要性分数的一部分神经元，而剔除了贡献较低的神经元。这个策略通过逐渐减小 $p_t$ 的值来控制保留的神经元数量，以达到对模型进行剪枝$（pruning）$或精简的目的,$p_t$减小公式如下：</p><p><img src="../images/1695091453574.jpg" alt=""></p><p>具体算法如下：</p><p><img src="../images/1695091607438.jpg" alt="算法"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在自然语言理解、问答任务、自然语言生成任务与其他模型做了对比，显示出模型的优越性。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h5 id="稀疏逼近的有效性"><a href="#稀疏逼近的有效性" class="headerlink" title="稀疏逼近的有效性"></a>稀疏逼近的有效性</h5><p>作者将LoSparse与两种变体进行比较:(1)丢弃稀疏矩阵$S$，只微调低秩矩阵$UV$$(Low-rank \ I)$；(2)遵循(8)的初始化，但逐渐将初始化的$S$修剪为零$(Low-rank  \ II)$。</p><p><img src="../images/1695092292904.jpg" alt="不同任务比较结果"></p><p>从图中可见，$(Low-rank  \ II)$的性能要比$(Low-rank \ I)$好得多。也就是说，修剪掉所有的稀疏矩阵比微调一个由奇异值阈值得到的低秩矩阵更有效，因此$LoSeparse$#可以增强低秩近似。</p><p><strong>解释：</strong>这是因为$Low-rank  \ I$的初始化与预训练的权重不同，因此它可能会从预训练的权重中丢失太多的知识。因此，下游任务的性能会严重下降。另一方面，$LoSeparse$弥补了低秩初始化与预训练权值之间的差距，从而保留了存储在预训练权值中的原始知识。这表明，尽管单独的低秩近似更有效和简洁，但我们应该利用稀疏近似来指导其训练过程。</p><h5 id="稀疏分配"><a href="#稀疏分配" class="headerlink" title="稀疏分配"></a>稀疏分配</h5><p>主要探究低秩近似和稀疏近似如何互相分配，主要方法是给定一个固定的剩余比例，改变低秩矩阵的比例，并相应改变稀疏矩阵的比例。不同分配条件下的结果如下图，可以看到低秩近似和稀疏近似对NLU任务的性能贡献几乎相等，因为当改变分配时性能保持稳定。</p><p><img src="../images/786cac11a7d27a4dadbb82d0082e95a.png" alt="关于稀疏分配的结果"></p><h3 id="结合知识蒸馏"><a href="#结合知识蒸馏" class="headerlink" title="结合知识蒸馏"></a>结合知识蒸馏</h3><p>本文选择了一个针对特定任务进行微调的$DeBertaV3-base$模型作为教师模型，一个压缩的$DeBertaV3-base$模型作为学生模型。结果发现知识蒸馏可以进一步提高$LoSeparse$的性能,</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>文章提出了一种结合低秩近似和结构化稀疏近似的$transfomer$模型压缩方法$LoSparse$。在自然语言理解、问题回答和自然语言生成方面的实验表明，作者的方法明显优于以前的压缩方法。此外，其在自然语言生成任务和极高稀疏度的设置中特别有效。实验证明了$Losparse$是通用的，与其他流行的压缩方法是互补的。实验表明，$LoSparse$算法可以提高$CoFi$算法和传统的知识蒸馏迭代剪枝算法的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剪枝 </tag>
            
            <tag> 低秩近似 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
